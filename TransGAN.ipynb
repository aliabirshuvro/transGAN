{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PgMyUiglnMda",
        "hfK0ek1XoW1J",
        "5qxJ8iQxnUtB",
        "1Cl2GVh_nxiK",
        "xzT84La4n4bC",
        "YNI1QMw3p7qS",
        "SlLMDSWyoIJS",
        "xDpUS9h0qBTK",
        "IgrahiFAqgcy",
        "XK35fF6squoi",
        "cihNMpuQq52i",
        "NecW75Klv6dM",
        "eK7OH1kNwBjz",
        "2LYKbqk4wImT",
        "iwTdRk4uwPNs",
        "XqSyN_bswbBs",
        "-BM1Y8bAwkyU",
        "bJF0QKRMw6o8",
        "XV7CvH5vxAKc",
        "s_PM6cxyxL3c",
        "GB-y6GqHxUqt",
        "AjEhp13vxgXk",
        "u6weNos1xkm8",
        "_KAvZfIqxpaM",
        "YnRoaSV3xuZs",
        "PBBSKBp5xydP",
        "O7T_aB3Dx64N",
        "d2plPLWpx88W"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je26LmJlz1n2"
      },
      "source": [
        "##Paper Name- TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up\n",
        "\n",
        "##Paper URL- https://arxiv.org/pdf/2102.07074.pdf\n",
        "##GitHub URL - https://github.com/VITA-Group/TransGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgMyUiglnMda"
      },
      "source": [
        "#Util.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIM30Me1hPgt"
      },
      "source": [
        "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
        "# and proprietary rights in and to this software, related documentation\n",
        "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
        "# distribution of this software and related documentation without an express\n",
        "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
        "\n",
        "\"\"\"Miscellaneous utility classes and functions.\"\"\"\n",
        "\n",
        "import ctypes\n",
        "import fnmatch\n",
        "import importlib\n",
        "import inspect\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import types\n",
        "import io\n",
        "import pickle\n",
        "import re\n",
        "import requests\n",
        "import html\n",
        "import hashlib\n",
        "import glob\n",
        "import tempfile\n",
        "import urllib\n",
        "import urllib.request\n",
        "import uuid\n",
        "\n",
        "from distutils.util import strtobool\n",
        "from typing import Any, List, Tuple, Union\n",
        "\n",
        "\n",
        "# Util classes\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class EasyDict(dict):\n",
        "    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n",
        "\n",
        "    def __getattr__(self, name: str) -> Any:\n",
        "        try:\n",
        "            return self[name]\n",
        "        except KeyError:\n",
        "            raise AttributeError(name)\n",
        "\n",
        "    def __setattr__(self, name: str, value: Any) -> None:\n",
        "        self[name] = value\n",
        "\n",
        "    def __delattr__(self, name: str) -> None:\n",
        "        del self[name]\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n",
        "\n",
        "    def __init__(self, file_name: str = None, file_mode: str = \"w\", should_flush: bool = True):\n",
        "        self.file = None\n",
        "\n",
        "        if file_name is not None:\n",
        "            self.file = open(file_name, file_mode)\n",
        "\n",
        "        self.should_flush = should_flush\n",
        "        self.stdout = sys.stdout\n",
        "        self.stderr = sys.stderr\n",
        "\n",
        "        sys.stdout = self\n",
        "        sys.stderr = self\n",
        "\n",
        "    def __enter__(self) -> \"Logger\":\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
        "        self.close()\n",
        "\n",
        "    def write(self, text: Union[str, bytes]) -> None:\n",
        "        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n",
        "        if isinstance(text, bytes):\n",
        "            text = text.decode()\n",
        "        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n",
        "            return\n",
        "\n",
        "        if self.file is not None:\n",
        "            self.file.write(text)\n",
        "\n",
        "        self.stdout.write(text)\n",
        "\n",
        "        if self.should_flush:\n",
        "            self.flush()\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n",
        "        if self.file is not None:\n",
        "            self.file.flush()\n",
        "\n",
        "        self.stdout.flush()\n",
        "\n",
        "    def close(self) -> None:\n",
        "        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n",
        "        self.flush()\n",
        "\n",
        "        # if using multiple loggers, prevent closing in wrong order\n",
        "        if sys.stdout is self:\n",
        "            sys.stdout = self.stdout\n",
        "        if sys.stderr is self:\n",
        "            sys.stderr = self.stderr\n",
        "\n",
        "        if self.file is not None:\n",
        "            self.file.close()\n",
        "            self.file = None\n",
        "\n",
        "\n",
        "# Cache directories\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "_dnnlib_cache_dir = None\n",
        "\n",
        "def set_cache_dir(path: str) -> None:\n",
        "    global _dnnlib_cache_dir\n",
        "    _dnnlib_cache_dir = path\n",
        "\n",
        "def make_cache_dir_path(*paths: str) -> str:\n",
        "    if _dnnlib_cache_dir is not None:\n",
        "        return os.path.join(_dnnlib_cache_dir, *paths)\n",
        "    if 'DNNLIB_CACHE_DIR' in os.environ:\n",
        "        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n",
        "    if 'HOME' in os.environ:\n",
        "        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n",
        "    if 'USERPROFILE' in os.environ:\n",
        "        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n",
        "    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)\n",
        "\n",
        "# Small util functions\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def format_time(seconds: Union[int, float]) -> str:\n",
        "    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n",
        "    s = int(np.rint(seconds))\n",
        "\n",
        "    if s < 60:\n",
        "        return \"{0}s\".format(s)\n",
        "    elif s < 60 * 60:\n",
        "        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n",
        "    elif s < 24 * 60 * 60:\n",
        "        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n",
        "    else:\n",
        "        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n",
        "\n",
        "\n",
        "def ask_yes_no(question: str) -> bool:\n",
        "    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"{0} [y/n]\".format(question))\n",
        "            return strtobool(input().lower())\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "\n",
        "def tuple_product(t: Tuple) -> Any:\n",
        "    \"\"\"Calculate the product of the tuple elements.\"\"\"\n",
        "    result = 1\n",
        "\n",
        "    for v in t:\n",
        "        result *= v\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "_str_to_ctype = {\n",
        "    \"uint8\": ctypes.c_ubyte,\n",
        "    \"uint16\": ctypes.c_uint16,\n",
        "    \"uint32\": ctypes.c_uint32,\n",
        "    \"uint64\": ctypes.c_uint64,\n",
        "    \"int8\": ctypes.c_byte,\n",
        "    \"int16\": ctypes.c_int16,\n",
        "    \"int32\": ctypes.c_int32,\n",
        "    \"int64\": ctypes.c_int64,\n",
        "    \"float32\": ctypes.c_float,\n",
        "    \"float64\": ctypes.c_double\n",
        "}\n",
        "\n",
        "\n",
        "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n",
        "    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n",
        "    type_str = None\n",
        "\n",
        "    if isinstance(type_obj, str):\n",
        "        type_str = type_obj\n",
        "    elif hasattr(type_obj, \"__name__\"):\n",
        "        type_str = type_obj.__name__\n",
        "    elif hasattr(type_obj, \"name\"):\n",
        "        type_str = type_obj.name\n",
        "    else:\n",
        "        raise RuntimeError(\"Cannot infer type name from input\")\n",
        "\n",
        "    assert type_str in _str_to_ctype.keys()\n",
        "\n",
        "    my_dtype = np.dtype(type_str)\n",
        "    my_ctype = _str_to_ctype[type_str]\n",
        "\n",
        "    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n",
        "\n",
        "    return my_dtype, my_ctype\n",
        "\n",
        "\n",
        "def is_pickleable(obj: Any) -> bool:\n",
        "    try:\n",
        "        with io.BytesIO() as stream:\n",
        "            pickle.dump(obj, stream)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "# Functionality to import modules/objects by name, and call functions by name\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n",
        "    \"\"\"Searches for the underlying module behind the name to some python object.\n",
        "    Returns the module and the object name (original name with module part removed).\"\"\"\n",
        "\n",
        "    # allow convenience shorthands, substitute them by full names\n",
        "    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n",
        "    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n",
        "\n",
        "    # list alternatives for (module_name, local_obj_name)\n",
        "    parts = obj_name.split(\".\")\n",
        "    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n",
        "\n",
        "    # try each alternative in turn\n",
        "    for module_name, local_obj_name in name_pairs:\n",
        "        try:\n",
        "            module = importlib.import_module(module_name) # may raise ImportError\n",
        "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
        "            return module, local_obj_name\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # maybe some of the modules themselves contain errors?\n",
        "    for module_name, _local_obj_name in name_pairs:\n",
        "        try:\n",
        "            importlib.import_module(module_name) # may raise ImportError\n",
        "        except ImportError:\n",
        "            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n",
        "                raise\n",
        "\n",
        "    # maybe the requested attribute is missing?\n",
        "    for module_name, local_obj_name in name_pairs:\n",
        "        try:\n",
        "            module = importlib.import_module(module_name) # may raise ImportError\n",
        "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
        "        except ImportError:\n",
        "            pass\n",
        "\n",
        "    # we are out of luck, but we have no idea why\n",
        "    raise ImportError(obj_name)\n",
        "\n",
        "\n",
        "def get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n",
        "    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n",
        "    if obj_name == '':\n",
        "        return module\n",
        "    obj = module\n",
        "    for part in obj_name.split(\".\"):\n",
        "        obj = getattr(obj, part)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def get_obj_by_name(name: str) -> Any:\n",
        "    \"\"\"Finds the python object with the given name.\"\"\"\n",
        "    module, obj_name = get_module_from_obj_name(name)\n",
        "    return get_obj_from_module(module, obj_name)\n",
        "\n",
        "\n",
        "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n",
        "    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n",
        "    assert func_name is not None\n",
        "    func_obj = get_obj_by_name(func_name)\n",
        "    assert callable(func_obj)\n",
        "    return func_obj(*args, **kwargs)\n",
        "\n",
        "\n",
        "def construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n",
        "    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n",
        "    return call_func_by_name(*args, func_name=class_name, **kwargs)\n",
        "\n",
        "\n",
        "def get_module_dir_by_obj_name(obj_name: str) -> str:\n",
        "    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n",
        "    module, _ = get_module_from_obj_name(obj_name)\n",
        "    return os.path.dirname(inspect.getfile(module))\n",
        "\n",
        "\n",
        "def is_top_level_function(obj: Any) -> bool:\n",
        "    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n",
        "    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n",
        "\n",
        "\n",
        "def get_top_level_function_name(obj: Any) -> str:\n",
        "    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n",
        "    assert is_top_level_function(obj)\n",
        "    module = obj.__module__\n",
        "    if module == '__main__':\n",
        "        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n",
        "    return module + \".\" + obj.__name__\n",
        "\n",
        "\n",
        "# File system helpers\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n",
        "    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n",
        "    Returns list of tuples containing both absolute and relative paths.\"\"\"\n",
        "    assert os.path.isdir(dir_path)\n",
        "    base_name = os.path.basename(os.path.normpath(dir_path))\n",
        "\n",
        "    if ignores is None:\n",
        "        ignores = []\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for root, dirs, files in os.walk(dir_path, topdown=True):\n",
        "        for ignore_ in ignores:\n",
        "            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n",
        "\n",
        "            # dirs need to be edited in-place\n",
        "            for d in dirs_to_remove:\n",
        "                dirs.remove(d)\n",
        "\n",
        "            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n",
        "\n",
        "        absolute_paths = [os.path.join(root, f) for f in files]\n",
        "        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n",
        "\n",
        "        if add_base_to_relative:\n",
        "            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n",
        "\n",
        "        assert len(absolute_paths) == len(relative_paths)\n",
        "        result += zip(absolute_paths, relative_paths)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n",
        "    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n",
        "    Will create all necessary directories.\"\"\"\n",
        "    for file in files:\n",
        "        target_dir_name = os.path.dirname(file[1])\n",
        "\n",
        "        # will create all intermediate-level directories\n",
        "        if not os.path.exists(target_dir_name):\n",
        "            os.makedirs(target_dir_name)\n",
        "\n",
        "        shutil.copyfile(file[0], file[1])\n",
        "\n",
        "\n",
        "# URL helpers\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n",
        "    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n",
        "    if not isinstance(obj, str) or not \"://\" in obj:\n",
        "        return False\n",
        "    if allow_file_urls and obj.startswith('file://'):\n",
        "        return True\n",
        "    try:\n",
        "        res = requests.compat.urlparse(obj)\n",
        "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
        "            return False\n",
        "        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n",
        "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n",
        "    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n",
        "    assert num_attempts >= 1\n",
        "    assert not (return_filename and (not cache))\n",
        "\n",
        "    # Doesn't look like an URL scheme so interpret it as a local filename.\n",
        "    if not re.match('^[a-z]+://', url):\n",
        "        return url if return_filename else open(url, \"rb\")\n",
        "\n",
        "    # Handle file URLs.  This code handles unusual file:// patterns that\n",
        "    # arise on Windows:\n",
        "    #\n",
        "    # file:///c:/foo.txt\n",
        "    #\n",
        "    # which would translate to a local '/c:/foo.txt' filename that's\n",
        "    # invalid.  Drop the forward slash for such pathnames.\n",
        "    #\n",
        "    # If you touch this code path, you should test it on both Linux and\n",
        "    # Windows.\n",
        "    #\n",
        "    # Some internet resources suggest using urllib.request.url2pathname() but\n",
        "    # but that converts forward slashes to backslashes and this causes\n",
        "    # its own set of problems.\n",
        "    if url.startswith('file://'):\n",
        "        filename = urllib.parse.urlparse(url).path\n",
        "        if re.match(r'^/[a-zA-Z]:', filename):\n",
        "            filename = filename[1:]\n",
        "        return filename if return_filename else open(filename, \"rb\")\n",
        "\n",
        "    assert is_url(url)\n",
        "\n",
        "    # Lookup from cache.\n",
        "    if cache_dir is None:\n",
        "        cache_dir = make_cache_dir_path('downloads')\n",
        "\n",
        "    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
        "    if cache:\n",
        "        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n",
        "        if len(cache_files) == 1:\n",
        "            filename = cache_files[0]\n",
        "            return filename if return_filename else open(filename, \"rb\")\n",
        "\n",
        "    # Download.\n",
        "    url_name = None\n",
        "    url_data = None\n",
        "    with requests.Session() as session:\n",
        "        if verbose:\n",
        "            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n",
        "        for attempts_left in reversed(range(num_attempts)):\n",
        "            try:\n",
        "                with session.get(url) as res:\n",
        "                    res.raise_for_status()\n",
        "                    if len(res.content) == 0:\n",
        "                        raise IOError(\"No data received\")\n",
        "\n",
        "                    if len(res.content) < 8192:\n",
        "                        content_str = res.content.decode(\"utf-8\")\n",
        "                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n",
        "                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n",
        "                            if len(links) == 1:\n",
        "                                url = requests.compat.urljoin(url, links[0])\n",
        "                                raise IOError(\"Google Drive virus checker nag\")\n",
        "                        if \"Google Drive - Quota exceeded\" in content_str:\n",
        "                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n",
        "\n",
        "                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n",
        "                    url_name = match[1] if match else url\n",
        "                    url_data = res.content\n",
        "                    if verbose:\n",
        "                        print(\" done\")\n",
        "                    break\n",
        "            except KeyboardInterrupt:\n",
        "                raise\n",
        "            except:\n",
        "                if not attempts_left:\n",
        "                    if verbose:\n",
        "                        print(\" failed\")\n",
        "                    raise\n",
        "                if verbose:\n",
        "                    print(\".\", end=\"\", flush=True)\n",
        "\n",
        "    # Save to cache.\n",
        "    if cache:\n",
        "        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n",
        "        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n",
        "        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        with open(temp_file, \"wb\") as f:\n",
        "            f.write(url_data)\n",
        "        os.replace(temp_file, cache_file) # atomic\n",
        "        if return_filename:\n",
        "            return cache_file\n",
        "\n",
        "    # Return data as file object.\n",
        "    assert not return_filename\n",
        "    return io.BytesIO(url_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfK0ek1XoW1J"
      },
      "source": [
        "#exps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qxJ8iQxnUtB"
      },
      "source": [
        "##celeba_hq_256_train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nS0Rgk-njHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a3302bef-21db-48ce-8fbd-527c803fe62c"
      },
      "source": [
        "!/usr/bin/env bash\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--rank', type=str, default=\"0\")\n",
        "    parser.add_argument('--node', type=str, default=\"0015\")\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    return opt\n",
        "args = parse_args()\n",
        "\n",
        "os.system(f\"CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train_derived.py \\\n",
        "-gen_bs 32 \\\n",
        "-dis_bs 16 \\\n",
        "--accumulated_times 4 \\\n",
        "--g_accumulated_times 4 \\\n",
        "--dist-url 'tcp://localhost:10641' \\\n",
        "--dist-backend 'nccl' \\\n",
        "--multiprocessing-distributed \\\n",
        "--world-size 1 \\\n",
        "--rank {args.rank} \\\n",
        "--dataset celeba \\\n",
        "--data_path ./celeba_hq \\\n",
        "--bottom_width 8 \\\n",
        "--img_size 256 \\\n",
        "--max_iter 500000 \\\n",
        "--gen_model ViT_custom_local544444_256_rp \\\n",
        "--dis_model ViT_scale3_local_new_rp \\\n",
        "--g_window_size 16 \\\n",
        "--d_window_size 8 \\\n",
        "--g_norm pn \\\n",
        "--df_dim 384 \\\n",
        "--d_depth 3 \\\n",
        "--g_depth 5,4,4,4,4,4 \\\n",
        "--latent_dim 512 \\\n",
        "--gf_dim 1024 \\\n",
        "--num_workers 32 \\\n",
        "--g_lr 0.0001 \\\n",
        "--d_lr 0.0001 \\\n",
        "--optimizer adam \\\n",
        "--loss wgangp-eps \\\n",
        "--wd 1e-3 \\\n",
        "--beta1 0 \\\n",
        "--beta2 0.99 \\\n",
        "--phi 1 \\\n",
        "--eval_batch_size 10 \\\n",
        "--num_eval_imgs 50000 \\\n",
        "--init_type xavier_uniform \\\n",
        "--n_critic 4 \\\n",
        "--val_freq 10 \\\n",
        "--print_freq 50 \\\n",
        "--grow_steps 0 0 \\\n",
        "--fade_in 0 \\\n",
        "--patch_size 4 \\\n",
        "--diff_aug filter,translation,erase_ratio,color,hue \\\n",
        "--fid_stat fid_stat/fid_stats_celeba_hq_256.npz \\\n",
        "--ema 0.995 \\\n",
        "--exp_name celeba_hq_128\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: cannot set terminal process group (64): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            "\u001b[01;34m/content\u001b[00m# \n",
            "\u001b[01;34m/content\u001b[00m# \n",
            "\u001b[01;34m/content\u001b[00m# \n",
            "\u001b[01;34m/content\u001b[00m# ^C\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--rank RANK] [--node NODE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-843a1585-738b-4ca6-8acb-f8c893b1900a.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cl2GVh_nxiK"
      },
      "source": [
        "##celeba_256_train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiuZeH-An1sY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "72f18446-0853-418f-fd46-56348cf66a6b"
      },
      "source": [
        "#!/usr/bin/env bash\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--rank', type=str, default=\"0\")\n",
        "    parser.add_argument('--node', type=str, default=\"0015\")\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    return opt\n",
        "args = parse_args()\n",
        "\n",
        "os.system(f\"CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python train_derived.py \\\n",
        "-gen_bs 16 \\\n",
        "-dis_bs 16 \\\n",
        "--accumulated_times 4 \\\n",
        "--g_accumulated_times 8 \\\n",
        "--dist-url 'tcp://localhost:10641' \\\n",
        "--dist-backend 'nccl' \\\n",
        "--multiprocessing-distributed \\\n",
        "--world-size 1 \\\n",
        "--rank {args.rank} \\\n",
        "--dataset church \\\n",
        "--data_path ./lsun \\\n",
        "--bottom_width 8 \\\n",
        "--img_size 256 \\\n",
        "--max_iter 500000 \\\n",
        "--gen_model ViT_custom_local544444_256_rp_noise \\\n",
        "--dis_model ViT_scale3_local_new_rp \\\n",
        "--g_window_size 16 \\\n",
        "--d_window_size 16 \\\n",
        "--g_norm pn \\\n",
        "--df_dim 384 \\\n",
        "--d_depth 3 \\\n",
        "--g_depth 5,4,4,4,4,4 \\\n",
        "--latent_dim 512 \\\n",
        "--gf_dim 1024 \\\n",
        "--num_workers 0 \\\n",
        "--g_lr 0.0001 \\\n",
        "--d_lr 0.0001 \\\n",
        "--optimizer adam \\\n",
        "--loss wgangp-eps \\\n",
        "--wd 1e-3 \\\n",
        "--beta1 0 \\\n",
        "--beta2 0.99 \\\n",
        "--phi 1 \\\n",
        "--eval_batch_size 10 \\\n",
        "--num_eval_imgs 50000 \\\n",
        "--init_type xavier_uniform \\\n",
        "--n_critic 4 \\\n",
        "--val_freq 5000 \\\n",
        "--print_freq 50 \\\n",
        "--grow_steps 0 0 \\\n",
        "--fade_in 0 \\\n",
        "--patch_size 4 \\\n",
        "--diff_aug translation,erase_ratio,color \\\n",
        "--fid_stat fid_stat/fid_stats_church_256.npz \\\n",
        "--ema 0.995 \\\n",
        "--exp_name church_256\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--rank RANK] [--node NODE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-a7604558-b01b-4d64-beee-119598d36c92.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzT84La4n4bC"
      },
      "source": [
        "##cifar_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO_1qBUcoDKp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "8fe50050-045c-4f07-ceff-34af57f01cda"
      },
      "source": [
        "#!/usr/bin/env bash\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--rank', type=str, default=\"0\")\n",
        "    parser.add_argument('--node', type=str, default=\"0015\")\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    return opt\n",
        "args = parse_args()\n",
        "\n",
        "os.system(f\"CUDA_VISIBLE_DEVICES=0,1 python train_derived.py \\\n",
        "-gen_bs 128 \\\n",
        "-dis_bs 64 \\\n",
        "--dist-url 'tcp://localhost:14256' \\\n",
        "--dist-backend 'nccl' \\\n",
        "--multiprocessing-distributed \\\n",
        "--world-size 1 \\\n",
        "--rank {args.rank} \\\n",
        "--dataset cifar10 \\\n",
        "--bottom_width 8 \\\n",
        "--img_size 32 \\\n",
        "--max_iter 500000 \\\n",
        "--gen_model ViT_custom_rp \\\n",
        "--dis_model ViT_custom_scale2_rp_noise \\\n",
        "--df_dim 384 \\\n",
        "--d_heads 4 \\\n",
        "--d_depth 3 \\\n",
        "--g_depth 5,4,2 \\\n",
        "--dropout 0 \\\n",
        "--latent_dim 256 \\\n",
        "--gf_dim 1024 \\\n",
        "--num_workers 16 \\\n",
        "--g_lr 0.0001 \\\n",
        "--d_lr 0.0001 \\\n",
        "--optimizer adam \\\n",
        "--loss wgangp-eps \\\n",
        "--wd 1e-3 \\\n",
        "--beta1 0 \\\n",
        "--beta2 0.99 \\\n",
        "--phi 1 \\\n",
        "--eval_batch_size 8 \\\n",
        "--num_eval_imgs 50000 \\\n",
        "--init_type xavier_uniform \\\n",
        "--n_critic 4 \\\n",
        "--val_freq 20 \\\n",
        "--print_freq 50 \\\n",
        "--grow_steps 0 0 \\\n",
        "--fade_in 0 \\\n",
        "--patch_size 2 \\\n",
        "--ema_kimg 500 \\\n",
        "--ema_warmup 0.1 \\\n",
        "--ema 0.9999 \\\n",
        "--diff_aug translation,cutout,color \\\n",
        "--exp_name cifar_train\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--rank RANK] [--node NODE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-843a1585-738b-4ca6-8acb-f8c893b1900a.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTuqBirFoFQC"
      },
      "source": [
        "#model_search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNI1QMw3p7qS"
      },
      "source": [
        "##ada.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNfb4uSyoExB"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "_constant_cache = dict()\n",
        "\n",
        "def constant(value, shape=None, dtype=None, device=None, memory_format=None):\n",
        "    value = np.asarray(value)\n",
        "    if shape is not None:\n",
        "        shape = tuple(shape)\n",
        "    if dtype is None:\n",
        "        dtype = torch.get_default_dtype()\n",
        "    if device is None:\n",
        "        device = torch.device('cpu')\n",
        "    if memory_format is None:\n",
        "        memory_format = torch.contiguous_format\n",
        "\n",
        "    key = (value.shape, value.dtype, value.tobytes(), shape, dtype, device, memory_format)\n",
        "    tensor = _constant_cache.get(key, None)\n",
        "    if tensor is None:\n",
        "        tensor = torch.as_tensor(value.copy(), dtype=dtype, device=device)\n",
        "        if shape is not None:\n",
        "            tensor, _ = torch.broadcast_tensors(tensor, torch.empty(shape))\n",
        "        tensor = tensor.contiguous(memory_format=memory_format)\n",
        "        _constant_cache[key] = tensor\n",
        "    return tensor\n",
        "\n",
        "def matrix(*rows, device=None):\n",
        "    assert all(len(row) == len(rows[0]) for row in rows)\n",
        "    elems = [x for row in rows for x in row]\n",
        "    ref = [x for x in elems if isinstance(x, torch.Tensor)]\n",
        "    if len(ref) == 0:\n",
        "        return constant(np.asarray(rows), device=device)\n",
        "    assert device is None or device == ref[0].device\n",
        "    elems = [x if isinstance(x, torch.Tensor) else constant(x, shape=ref[0].shape, device=ref[0].device) for x in elems]\n",
        "    return torch.stack(elems, dim=-1).reshape(ref[0].shape + (len(rows), -1))\n",
        "\n",
        "def translate2d(tx, ty, **kwargs):\n",
        "    return matrix(\n",
        "        [1, 0, tx],\n",
        "        [0, 1, ty],\n",
        "        [0, 0, 1],\n",
        "        **kwargs)\n",
        "\n",
        "def translate3d(tx, ty, tz, **kwargs):\n",
        "    return matrix(\n",
        "        [1, 0, 0, tx],\n",
        "        [0, 1, 0, ty],\n",
        "        [0, 0, 1, tz],\n",
        "        [0, 0, 0, 1],\n",
        "        **kwargs)\n",
        "\n",
        "def scale2d(sx, sy, **kwargs):\n",
        "    return matrix(\n",
        "        [sx, 0,  0],\n",
        "        [0,  sy, 0],\n",
        "        [0,  0,  1],\n",
        "        **kwargs)\n",
        "\n",
        "def scale3d(sx, sy, sz, **kwargs):\n",
        "    return matrix(\n",
        "        [sx, 0,  0,  0],\n",
        "        [0,  sy, 0,  0],\n",
        "        [0,  0,  sz, 0],\n",
        "        [0,  0,  0,  1],\n",
        "        **kwargs)\n",
        "\n",
        "def rotate2d(theta, **kwargs):\n",
        "    return matrix(\n",
        "        [torch.cos(theta), torch.sin(-theta), 0],\n",
        "        [torch.sin(theta), torch.cos(theta),  0],\n",
        "        [0,                0,                 1],\n",
        "        **kwargs)\n",
        "\n",
        "def rotate3d(v, theta, **kwargs):\n",
        "    vx = v[..., 0]; vy = v[..., 1]; vz = v[..., 2]\n",
        "    s = torch.sin(theta); c = torch.cos(theta); cc = 1 - c\n",
        "    return matrix(\n",
        "        [vx*vx*cc+c,    vx*vy*cc-vz*s, vx*vz*cc+vy*s, 0],\n",
        "        [vy*vx*cc+vz*s, vy*vy*cc+c,    vy*vz*cc-vx*s, 0],\n",
        "        [vz*vx*cc-vy*s, vz*vy*cc+vx*s, vz*vz*cc+c,    0],\n",
        "        [0,             0,             0,             1],\n",
        "        **kwargs)\n",
        "\n",
        "def translate2d_inv(tx, ty, **kwargs):\n",
        "    return translate2d(-tx, -ty, **kwargs)\n",
        "\n",
        "def scale2d_inv(sx, sy, **kwargs):\n",
        "    return scale2d(1 / sx, 1 / sy, **kwargs)\n",
        "\n",
        "def rotate2d_inv(theta, **kwargs):\n",
        "    return rotate2d(-theta, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlLMDSWyoIJS"
      },
      "source": [
        "##diff_aug.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEOXA7hGqF0L"
      },
      "source": [
        "# Code Heavily borrowed from Differentiable Augmentation and StyleGAN-ADA\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.signal\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "\"\"\"\n",
        "from models_search.ada import *\n",
        "# from models_search import conv2d_gradfix\n",
        "from torch_utils import persistence\n",
        "from torch_utils import misc\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from torch_utils.ops import grid_sample_gradfix\n",
        "from torch_utils.ops import conv2d_gradfix\n",
        "\"\"\"    \n",
        "wavelets = {\n",
        "    'haar': [0.7071067811865476, 0.7071067811865476],\n",
        "    'db1':  [0.7071067811865476, 0.7071067811865476],\n",
        "    'db2':  [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],\n",
        "    'db5':  [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125],\n",
        "    'db6':  [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017],\n",
        "    'db7':  [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236],\n",
        "    'db8':  [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161],\n",
        "    'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n",
        "    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],\n",
        "    'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148],\n",
        "    'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255],\n",
        "    'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609],\n",
        "}\n",
        "\n",
        "# global Hz_fbank\n",
        "# Hz_lo = np.asarray(wavelets['sym2'])            # H(z)\n",
        "# Hz_hi = Hz_lo * ((-1) ** np.arange(Hz_lo.size)) # H(-z)\n",
        "# Hz_lo2 = np.convolve(Hz_lo, Hz_lo[::-1]) / 2    # H(z) * H(z^-1) / 2\n",
        "# Hz_hi2 = np.convolve(Hz_hi, Hz_hi[::-1]) / 2    # H(-z) * H(-z^-1) / 2\n",
        "# Hz_fbank = np.eye(4, 1)                         # Bandpass(H(z), b_i)\n",
        "# for i in range(1, Hz_fbank.shape[0]):\n",
        "#     Hz_fbank = np.dstack([Hz_fbank, np.zeros_like(Hz_fbank)]).reshape(Hz_fbank.shape[0], -1)[:, :-1]\n",
        "#     Hz_fbank = scipy.signal.convolve(Hz_fbank, [Hz_lo2])\n",
        "#     Hz_fbank[i, (Hz_fbank.shape[1] - Hz_hi2.size) // 2 : (Hz_fbank.shape[1] + Hz_hi2.size) // 2] += Hz_hi2\n",
        "# Hz_fbank = torch.as_tensor(Hz_fbank, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True, affine=None):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x, affine=affine)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "def rand_crop(x, affine=None):\n",
        "    b, _, h, w = x.shape\n",
        "    x_large = torch.nn.functional.interpolate(x, scale_factor=1.2, mode='bicubic')\n",
        "    _, _, h_large, w_large = x_large.size()\n",
        "    h_start, w_start = random.randint(0, (h_large - h)), random.randint(0, (w_large - w))\n",
        "    x_crop = x_large[:, :, h_start:h_start+h, w_start:w_start+w]\n",
        "    assert x_crop.size() == x.size()\n",
        "    output = torch.where(torch.rand([b, 1, 1, 1], device=x.device) < 0.2, x_crop, x)\n",
        "    return output\n",
        "    \n",
        "\n",
        "def rand_filter(images, affine=None):\n",
        "    ratio = 0.25\n",
        "   \n",
        "    \n",
        "    _, Hz_fbank = affine\n",
        "    Hz_fbank = Hz_fbank.to(images.device)\n",
        "    imgfilter_bands = [1,1,1,1]\n",
        "    batch_size, num_channels, height, width = images.shape\n",
        "    device = images.device\n",
        "    num_bands = Hz_fbank.shape[0]\n",
        "    assert len([1,1,1,1]) == num_bands\n",
        "    expected_power = constant(np.array([10, 1, 1, 1]) / 13, device=device) # Expected power spectrum (1/f).\n",
        "\n",
        "    # Apply amplification for each band with probability (imgfilter * strength * band_strength).\n",
        "    g = torch.ones([batch_size, num_bands], device=device) # Global gain vector (identity).\n",
        "    for i, band_strength in enumerate(imgfilter_bands):\n",
        "        t_i = torch.exp2(torch.randn([batch_size], device=device) * 1)\n",
        "        t_i = torch.where(torch.rand([batch_size], device=device) < ratio * band_strength, t_i, torch.ones_like(t_i))\n",
        "#         if debug_percentile is not None:\n",
        "#             t_i = torch.full_like(t_i, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * 1)) if band_strength > 0 else torch.ones_like(t_i)\n",
        "        t = torch.ones([batch_size, num_bands], device=device)                  # Temporary gain vector.\n",
        "        t[:, i] = t_i                                                           # Replace i'th element.\n",
        "        t = t / (expected_power * t.square()).sum(dim=-1, keepdims=True).sqrt() # Normalize power.\n",
        "        g = g * t                                                               # Accumulate into global gain.\n",
        "\n",
        "    # Construct combined amplification filter.\n",
        "    Hz_prime = g @ Hz_fbank                                    # [batch, tap]\n",
        "    Hz_prime = Hz_prime.unsqueeze(1).repeat([1, num_channels, 1])   # [batch, channels, tap]\n",
        "    Hz_prime = Hz_prime.reshape([batch_size * num_channels, 1, -1]) # [batch * channels, 1, tap]\n",
        "\n",
        "    # Apply filter.\n",
        "    p = Hz_fbank.shape[1] // 2\n",
        "    images = images.reshape([1, batch_size * num_channels, height, width])\n",
        "    images = torch.nn.functional.pad(input=images, pad=[p,p,p,p], mode='reflect')\n",
        "    images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(2), groups=batch_size*num_channels)\n",
        "    images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(3), groups=batch_size*num_channels)\n",
        "    images = images.reshape([batch_size, num_channels, height, width])\n",
        "    return images\n",
        "            \n",
        "def rand_hue(images, affine=None):\n",
        "    batch_size, num_channels, height, width = images.shape\n",
        "    device = images.device\n",
        "    I_4 = torch.eye(4, device=device)\n",
        "    C = I_4\n",
        "    v = constant(np.asarray([1, 1, 1, 0]) / np.sqrt(3), device=device) # Luma axis.\n",
        "\n",
        "    # Apply hue rotation with probability (hue * strength).\n",
        "    if num_channels > 1:\n",
        "        theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * 1\n",
        "        theta = torch.where(torch.rand([batch_size], device=device) < 0.5, theta, torch.zeros_like(theta))\n",
        "#         if debug_percentile is not None:\n",
        "#             theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * 1)\n",
        "        C = rotate3d(v, theta) @ C # Rotate around v.\n",
        "\n",
        "    # Apply saturation with probability (saturation * strength).\n",
        "#     if self.saturation > 0 and num_channels > 1:\n",
        "#         s = torch.exp2(torch.randn([batch_size, 1, 1], device=device) * self.saturation_std)\n",
        "#         s = torch.where(torch.rand([batch_size, 1, 1], device=device) < self.saturation * self.p, s, torch.ones_like(s))\n",
        "#         if debug_percentile is not None:\n",
        "#             s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.saturation_std))\n",
        "#         C = (v.ger(v) + (I_4 - v.ger(v)) * s) @ C\n",
        "\n",
        "    # ------------------------------\n",
        "    # Execute color transformations.\n",
        "    # ------------------------------\n",
        "\n",
        "    # Execute if the transform is not identity.\n",
        "    if C is not I_4:\n",
        "        images = images.reshape([batch_size, num_channels, height * width])\n",
        "        if num_channels == 3:\n",
        "            images = C[:, :3, :3] @ images + C[:, :3, 3:]\n",
        "        elif num_channels == 1:\n",
        "            C = C[:, :3, :].mean(dim=1, keepdims=True)\n",
        "            images = images * C[:, :, :3].sum(dim=2, keepdims=True) + C[:, :, 3:]\n",
        "        else:\n",
        "            raise ValueError('Image must be RGB (3 channels) or L (1 channel)')\n",
        "        images = images.reshape([batch_size, num_channels, height, width])\n",
        "    return images\n",
        "\n",
        "def rand_geo(images, affine=None):\n",
        "    batch_size, num_channels, height, width = images.shape\n",
        "    device = images.device\n",
        "    \n",
        "    Hz_geom, _ = affine\n",
        "    Hz_geom = Hz_geom.to(images.device)\n",
        "    \n",
        "    I_3 = torch.eye(3, device=device)\n",
        "    G_inv = I_3\n",
        "\n",
        "    # Apply x-flip with probability (xflip * strength).\n",
        "    if 1:\n",
        "        i = torch.floor(torch.rand([batch_size], device=device) * 2)\n",
        "        i = torch.where(torch.rand([batch_size], device=device) < 1, i, torch.zeros_like(i))\n",
        "#         if debug_percentile is not None:\n",
        "#             i = torch.full_like(i, torch.floor(debug_percentile * 2))\n",
        "        G_inv = G_inv @ scale2d_inv(1 - 2 * i, 1)\n",
        "\n",
        "#     # Apply 90 degree rotations with probability (rotate90 * strength).\n",
        "#     if self.rotate90 > 0:\n",
        "#         i = torch.floor(torch.rand([batch_size], device=device) * 4)\n",
        "#         i = torch.where(torch.rand([batch_size], device=device) < self.rotate90 * P, i, torch.zeros_like(i))\n",
        "#         if debug_percentile is not None:\n",
        "#             i = torch.full_like(i, torch.floor(debug_percentile * 4))\n",
        "#         G_inv = G_inv @ rotate2d_inv(-np.pi / 2 * i)\n",
        "\n",
        "    # Apply integer translation with probability (xint * strength).\n",
        "#     if self.xint > 0:\n",
        "#         t = (torch.rand([batch_size, 2], device=device) * 2 - 1) * self.xint_max\n",
        "#         t = torch.where(torch.rand([batch_size, 1], device=device) < self.xint * P, t, torch.zeros_like(t))\n",
        "#         if debug_percentile is not None:\n",
        "#             t = torch.full_like(t, (debug_percentile * 2 - 1) * self.xint_max)\n",
        "#         G_inv = G_inv @ translate2d_inv(torch.round(t[:,0] * width), torch.round(t[:,1] * height))\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Select parameters for general geometric transformations.\n",
        "    # --------------------------------------------------------\n",
        "\n",
        "    # Apply isotropic scaling with probability (scale * strength).\n",
        "    if 1:\n",
        "        s = torch.exp2(torch.randn([batch_size], device=device) * 0.2)\n",
        "        s = torch.where(torch.rand([batch_size], device=device) < 0.3, s, torch.ones_like(s))\n",
        "#         if debug_percentile is not None:\n",
        "#             s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.scale_std))\n",
        "        G_inv = G_inv @ scale2d_inv(s, s)\n",
        "\n",
        "#     # Apply pre-rotation with probability p_rot.\n",
        "#     p_rot = 1 - torch.sqrt((1 - self.rotate * self.p).clamp(0, 1)) # P(pre OR post) = p\n",
        "#     if self.rotate > 0:\n",
        "#         theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max\n",
        "#         theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))\n",
        "#         if debug_percentile is not None:\n",
        "#             theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * self.rotate_max)\n",
        "#         G_inv = G_inv @ rotate2d_inv(-theta) # Before anisotropic scaling.\n",
        "\n",
        "#     Apply anisotropic scaling with probability (aniso * strength).\n",
        "    if 1:\n",
        "        s = torch.exp2(torch.randn([batch_size], device=device) * 0.2)\n",
        "        s = torch.where(torch.rand([batch_size], device=device) < 0.3, s, torch.ones_like(s))\n",
        "#         if debug_percentile is not None:\n",
        "#             s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.aniso_std))\n",
        "        G_inv = G_inv @ scale2d_inv(s, 1 / s)\n",
        "\n",
        "#     # Apply post-rotation with probability p_rot.\n",
        "#     if self.rotate > 0:\n",
        "#         theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max\n",
        "#         theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))\n",
        "#         if debug_percentile is not None:\n",
        "#             theta = torch.zeros_like(theta)\n",
        "#         G_inv = G_inv @ rotate2d_inv(-theta) # After anisotropic scaling.\n",
        "\n",
        "    # Apply fractional translation with probability (xfrac * strength).\n",
        "    if 1:\n",
        "        t = torch.randn([batch_size, 2], device=device) * 0.125\n",
        "        t = torch.where(torch.rand([batch_size, 1], device=device) < 0.3, t, torch.zeros_like(t))\n",
        "#         if debug_percentile is not None:\n",
        "#             t = torch.full_like(t, torch.erfinv(debug_percentile * 2 - 1) * 0.125)\n",
        "        G_inv = G_inv @ translate2d_inv(t[:,0] * width, t[:,1] * height)\n",
        "\n",
        "    # ----------------------------------\n",
        "    # Execute geometric transformations.\n",
        "    # ----------------------------------\n",
        "\n",
        "    # Execute if the transform is not identity.\n",
        "    if G_inv is not I_3:\n",
        "\n",
        "        # Calculate padding.\n",
        "        cx = (width - 1) / 2\n",
        "        cy = (height - 1) / 2\n",
        "        cp = matrix([-cx, -cy, 1], [cx, -cy, 1], [cx, cy, 1], [-cx, cy, 1], device=device) # [idx, xyz]\n",
        "        cp = G_inv @ cp.t() # [batch, xyz, idx]\n",
        "        Hz_pad = Hz_geom.shape[0] // 4\n",
        "        margin = cp[:, :2, :].permute(1, 0, 2).flatten(1) # [xy, batch * idx]\n",
        "        margin = torch.cat([-margin, margin]).max(dim=1).values # [x0, y0, x1, y1]\n",
        "        margin = margin + constant([Hz_pad * 2 - cx, Hz_pad * 2 - cy] * 2, device=device)\n",
        "        margin = margin.max(constant([0, 0] * 2, device=device))\n",
        "        margin = margin.min(constant([width-1, height-1] * 2, device=device))\n",
        "        mx0, my0, mx1, my1 = margin.ceil().to(torch.int32)\n",
        "\n",
        "        # Pad image and adjust origin.\n",
        "        images = torch.nn.functional.pad(input=images, pad=[mx0,mx1,my0,my1], mode='reflect')\n",
        "        G_inv = translate2d((mx0 - mx1) / 2, (my0 - my1) / 2) @ G_inv\n",
        "\n",
        "        # Upsample.\n",
        "        images = upfirdn2d.upsample2d(x=images, f=Hz_geom, up=2)\n",
        "        G_inv = scale2d(2, 2, device=device) @ G_inv @ scale2d_inv(2, 2, device=device)\n",
        "        G_inv = translate2d(-0.5, -0.5, device=device) @ G_inv @ translate2d_inv(-0.5, -0.5, device=device)\n",
        "\n",
        "        # Execute transformation.\n",
        "        shape = [batch_size, num_channels, (height + Hz_pad * 2) * 2, (width + Hz_pad * 2) * 2]\n",
        "        G_inv = scale2d(2 / images.shape[3], 2 / images.shape[2], device=device) @ G_inv @ scale2d_inv(2 / shape[3], 2 / shape[2], device=device)\n",
        "        grid = torch.nn.functional.affine_grid(theta=G_inv[:,:2,:], size=shape, align_corners=False)\n",
        "        images = grid_sample_gradfix.grid_sample(images, grid)\n",
        "\n",
        "        # Downsample and crop.\n",
        "        images = upfirdn2d.downsample2d(x=images, f=Hz_geom, down=2, padding=-Hz_pad*2, flip_filter=True)\n",
        "    return images\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rand_brightness(x, affine=None):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x, affine=None):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x, affine=None):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.2, affine=None):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "def rand_translation_1(x, ratio=0.1, affine=None):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_x = translation_x*2 - 1\n",
        "    translation_y = translation_y*2 - 1\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "def rand_strong_translation(x, ratio=0.125, affine=None):\n",
        "    ratio = 0.125\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5, affine=None):\n",
        "    if random.random() < 0.3:\n",
        "        cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "        )\n",
        "        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "        del offset_x\n",
        "        del offset_y\n",
        "        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "        mask[grid_batch, grid_x, grid_y] = 0\n",
        "        x = x * mask.unsqueeze(1)\n",
        "        del mask\n",
        "        del grid_x\n",
        "        del grid_y\n",
        "        del grid_batch\n",
        "    return x\n",
        "\n",
        "def rand_erase(x, ratio=0.5, affine=None):\n",
        "    ratio_x = random.randint(20, x.size(2)//2 + 20)\n",
        "    ratio_y = random.randint(20, x.size(3)//2 + 20)\n",
        "    if random.random() < 0.3:\n",
        "#         cutout_size = int(x.size(2) * ratio_x + 0.5), int(x.size(3) * ratio_y + 0.5)\n",
        "        cutout_size = ratio_x, ratio_y\n",
        "        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "        )\n",
        "        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "        del offset_x\n",
        "        del offset_y\n",
        "        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "        mask[grid_batch, grid_x, grid_y] = 0\n",
        "        x = x * mask.unsqueeze(1)\n",
        "        del mask\n",
        "        del grid_x\n",
        "        del grid_y\n",
        "        del grid_batch\n",
        "    return x\n",
        "\n",
        "def rand_erase_ratio(x, ratio=0.5, affine=None):\n",
        "    ratio_x = random.randint(int(x.size(2)*0.2), int(x.size(2)*0.7))\n",
        "    ratio_y = random.randint(int(x.size(3)*0.2), int(x.size(3)*0.7))\n",
        "    if random.random() < 0.3:\n",
        "#         cutout_size = int(x.size(2) * ratio_x + 0.5), int(x.size(3) * ratio_y + 0.5)\n",
        "        cutout_size = ratio_x, ratio_y\n",
        "        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "        )\n",
        "        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "        del offset_x\n",
        "        del offset_y\n",
        "        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "        mask[grid_batch, grid_x, grid_y] = 0\n",
        "        x = x * mask.unsqueeze(1)\n",
        "        del mask\n",
        "        del grid_x\n",
        "        del grid_y\n",
        "        del grid_batch\n",
        "    return x\n",
        "\n",
        "def rand_erase2_ratio(x, ratio=0.5, affine=None):\n",
        "    ratio_x = random.randint(int(x.size(2)*0.2), int(x.size(2)*0.7))\n",
        "    ratio_y = random.randint(int(x.size(3)*0.2), int(x.size(3)*0.7))\n",
        "    if random.random() < 0.3:\n",
        "#         cutout_size = int(x.size(2) * ratio_x + 0.5), int(x.size(3) * ratio_y + 0.5)\n",
        "        cutout_size = ratio_x, ratio_y\n",
        "        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "        )\n",
        "        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "        del offset_x\n",
        "        del offset_y\n",
        "        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "        mask[grid_batch, grid_x, grid_y] = 0\n",
        "        x = x * mask.unsqueeze(1)\n",
        "        del mask\n",
        "        del grid_x\n",
        "        del grid_y\n",
        "        del grid_batch\n",
        "        \n",
        "        cutout_size = ratio_x, ratio_y\n",
        "        offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "        grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "            torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "            torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "        )\n",
        "        grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "        grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "        del offset_x\n",
        "        del offset_y\n",
        "        mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "        mask[grid_batch, grid_x, grid_y] = 0\n",
        "        x = x * mask.unsqueeze(1)\n",
        "        del mask\n",
        "        del grid_x\n",
        "        del grid_y\n",
        "        del grid_batch\n",
        "    return x\n",
        "\n",
        "def rand_rand_erase_ratio(x, ratio=0.5, affine=None):\n",
        "    ratio_x = random.randint(int(x.size(2)*0.2), int(x.size(2)*0.7))\n",
        "    ratio_y = random.randint(int(x.size(3)*0.2), int(x.size(3)*0.7))\n",
        "#     if random.random() < 0.3:\n",
        "#         cutout_size = int(x.size(2) * ratio_x + 0.5), int(x.size(3) * ratio_y + 0.5)\n",
        "    cutout_size = ratio_x, ratio_y\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x[:int(x.size(0)*0.3)] = x[:int(x.size(0)*0.3)] * mask[:int(x.size(0)*0.3)].unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "def rand_cutmix(x, affine=None):\n",
        "    def rand_bbox(size, lam):\n",
        "        W = size[2]\n",
        "        H = size[3]\n",
        "        cut_rat = lam\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "\n",
        "        # uniform\n",
        "#         if random.random()<0.5:\n",
        "#             cx = 0\n",
        "#         else:\n",
        "#             cx = int(W*0.6)\n",
        "#         if random.random()<0.5:\n",
        "#             cy = 0\n",
        "#         else:\n",
        "#             cy = int(H*0.6)\n",
        "            \n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "        bbx1 = np.clip(cx, 0, W)\n",
        "        bby1 = np.clip(cy, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h, 0, H)\n",
        "\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "    \n",
        "    lam = 0.45 + 0.1*random.random()\n",
        "    rand_index = torch.randperm(x.size()[0]).cuda()\n",
        "#     for i in range(10000):\n",
        "#         if rand_index[0].item() == 0:\n",
        "#             rand_index = torch.randperm(x.size()[0]).cuda()\n",
        "#         else:\n",
        "#             break\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "    return x\n",
        "\n",
        "# def rand_erase(x, ratio=0.5):\n",
        "#     ratio_x = random.randint(20, x.size(2)//2 + 20)\n",
        "#     ratio_y = random.randint(20, x.size(3)//2 + 20)\n",
        "#     cutout_size = int(x.size(2) * ratio_x + 0.5), int(x.size(3) * ratio_y + 0.5)\n",
        "#     offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "#     offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    \n",
        "    \n",
        "#     if random.random() < 0.3:\n",
        "#         cutout_size = int(x.size(2) * ratio_x + 0.5), int(x.size(3) * ratio_y + 0.5)\n",
        "#         offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "#         offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "#         grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "#             torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "#             torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "#             torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "#         )\n",
        "#         grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "#         grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "#         del offset_x\n",
        "#         del offset_y\n",
        "#         mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "#         mask[grid_batch, grid_x, grid_y] = 0\n",
        "#         x = x * mask.unsqueeze(1)\n",
        "#         del mask\n",
        "#         del grid_x\n",
        "#         del grid_y\n",
        "#         del grid_batch\n",
        "#     return x\n",
        "\n",
        "def rand_rotate(x, ratio=0.5, affine=None):\n",
        "    k = random.randint(1,3)\n",
        "    if random.random() < ratio:\n",
        "        x = torch.rot90(x, k, [2,3])\n",
        "    return x\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'translation_1': [rand_translation_1],\n",
        "    'strong_translation': [rand_strong_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "    'erase': [rand_erase],\n",
        "    'erase_ratio': [rand_erase_ratio],\n",
        "    'erase2_ratio': [rand_erase2_ratio],\n",
        "    'rand_erase_ratio': [rand_rand_erase_ratio],\n",
        "    'rotate': [rand_rotate],\n",
        "    'cutmix': [rand_cutmix],\n",
        "    'hue': [rand_hue],\n",
        "    'filter': [rand_filter],\n",
        "    'geo': [rand_geo],\n",
        "    'crop': [rand_crop],\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDpUS9h0qBTK"
      },
      "source": [
        "##ViT_custom_local544444_256_rp_noise.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx3D8HqMqO3h"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "#This portion is not needed as everything is in one place!\n",
        "\"\"\"\n",
        "from models_search.ViT_helper import DropPath, to_2tuple, trunc_normal_\n",
        "from models_search.diff_aug import DiffAugment\n",
        "\"\"\"\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "\n",
        "\n",
        "class matmul(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "    \n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=2, keepdim=True) + 1e-8)\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "def leakyrelu(x):\n",
        "    return nn.functional.leaky_relu_(x, 0.2)\n",
        "class CustomAct(nn.Module):\n",
        "    def __init__(self, act_layer):\n",
        "        super().__init__()\n",
        "        if act_layer == \"gelu\":\n",
        "            self.act_layer = gelu\n",
        "        elif act_layer == \"leakyrelu\":\n",
        "            self.act_layer = leakyrelu\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act_layer(x)\n",
        "        \n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = CustomAct(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "#         self.noise_strength_1 = torch.nn.Parameter(torch.zeros([]))\n",
        "#         self.noise_strength_2 = torch.nn.Parameter(torch.zeros([]))\n",
        "    def forward(self, x):\n",
        "#         x = x + torch.randn([x.size(0), x.size(1), 1], device=x.device) * self.noise_strength_1\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "#         x = x + torch.randn([x.size(0), x.size(1), 1], device=x.device) * self.noise_strength_2\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    \n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "        self.window_size = window_size\n",
        "        \n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(window_size)\n",
        "        coords_w = torch.arange(window_size)\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += window_size - 1\n",
        "        relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "        \n",
        "        self.noise_strength_1 = torch.nn.Parameter(torch.zeros([]))\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + torch.randn([x.size(0), x.size(1), 1], device=x.device) * self.noise_strength_1\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    \n",
        "class CustomNorm(nn.Module):\n",
        "    def __init__(self, norm_layer, dim):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_layer\n",
        "        if norm_layer == \"ln\":\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        elif norm_layer == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(dim)\n",
        "        elif norm_layer == \"in\":\n",
        "            self.norm = nn.InstanceNorm1d(dim)\n",
        "        elif norm_layer == \"pn\":\n",
        "            self.norm = PixelNorm(dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.norm_type == \"bn\" or self.norm_type == \"in\":\n",
        "            x = self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
        "            return x\n",
        "        elif self.norm_type == \"none\":\n",
        "            return x\n",
        "        else:\n",
        "            return self.norm(x)\n",
        "        \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "    \n",
        "class StageBlock(nn.Module):\n",
        "    def __init__(self, depth, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        models = [Block(\n",
        "                        dim=dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop, \n",
        "                        attn_drop=attn_drop, \n",
        "                        drop_path=drop_path, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=window_size\n",
        "                        ) for i in range(depth)]\n",
        "        self.block = nn.Sequential(*models)\n",
        "    def forward(self, x):\n",
        "#         for blk in self.block:\n",
        "#             # x = blk(x)\n",
        "#             checkpoint.checkpoint(blk, x)\n",
        "#         x = checkpoint.checkpoint(self.block, x)\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "def bicubic_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.functional.interpolate(x, scale_factor=2, mode='bicubic')\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, args, img_size=224, patch_size=16, in_chans=3, num_classes=10, embed_dim=384, depth=5,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super(Generator, self).__init__()\n",
        "        self.args = args\n",
        "        self.ch = embed_dim\n",
        "        self.bottom_width = args.bottom_width\n",
        "        self.embed_dim = embed_dim = args.gf_dim\n",
        "        self.window_size = args.g_window_size\n",
        "        norm_layer = args.g_norm\n",
        "        mlp_ratio = args.g_mlp\n",
        "        depth = [int(i) for i in args.g_depth.split(\",\")]\n",
        "        act_layer = args.g_act\n",
        "        self.l2_size = 0\n",
        "        \n",
        "        if self.l2_size == 0:\n",
        "            self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.embed_dim)\n",
        "        elif self.l2_size > 1000:\n",
        "            self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.l2_size//16)\n",
        "            self.l2 = nn.Sequential(\n",
        "                        nn.Linear(self.l2_size//16, self.l2_size),\n",
        "                        nn.Linear(self.l2_size, self.embed_dim)\n",
        "                      )\n",
        "        else:\n",
        "            self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.l2_size)\n",
        "            self.l2 = nn.Linear(self.l2_size, self.embed_dim)\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, self.bottom_width**2, embed_dim))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, (self.bottom_width*2)**2, embed_dim))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, (self.bottom_width*4)**2, embed_dim))\n",
        "        self.pos_embed_4 = nn.Parameter(torch.zeros(1, (self.bottom_width*8)**2, embed_dim//4))\n",
        "        self.pos_embed_5 = nn.Parameter(torch.zeros(1, (self.bottom_width*16)**2, embed_dim//16))\n",
        "        self.pos_embed_6 = nn.Parameter(torch.zeros(1, (self.bottom_width*32)**2, embed_dim//64))\n",
        "                                        \n",
        "        self.pos_embed = [\n",
        "            self.pos_embed_1,\n",
        "            self.pos_embed_2,\n",
        "            self.pos_embed_3,\n",
        "            self.pos_embed_4,\n",
        "            self.pos_embed_5,\n",
        "            self.pos_embed_6\n",
        "        ]\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  # stochastic depth decay rule\n",
        "        self.blocks_1 = StageBlock(\n",
        "                        depth=depth[0],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=8\n",
        "                        )\n",
        "        self.blocks_2 = StageBlock(\n",
        "                        depth=depth[1],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=16\n",
        "                        )\n",
        "        self.blocks_3 = StageBlock(\n",
        "                        depth=depth[2],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=32\n",
        "                        )\n",
        "        self.blocks_4 = StageBlock(\n",
        "                        depth=depth[3],\n",
        "                        dim=embed_dim//4, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=self.window_size\n",
        "                        )\n",
        "        self.blocks_5 = StageBlock(\n",
        "                        depth=depth[4],\n",
        "                        dim=embed_dim//16, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=self.window_size\n",
        "                        )\n",
        "        self.blocks_6 = StageBlock(\n",
        "                        depth=depth[5],\n",
        "                        dim=embed_dim//64, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=self.window_size\n",
        "                        )\n",
        "                                        \n",
        "        for i in range(len(self.pos_embed)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//64, 3, 1, 1, 0)\n",
        "        )\n",
        "#         self.apply(self._init_weights)\n",
        "    \n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.BatchNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.InstanceNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "    def forward(self, z, epoch):\n",
        "        if self.args.latent_norm:\n",
        "            latent_size = z.size(-1)\n",
        "            z = (z/z.norm(dim=-1, keepdim=True) * (latent_size ** 0.5))\n",
        "        if self.args.latent_norm:\n",
        "            latent_size = z.size(-1)\n",
        "            z = (z/z.norm(dim=-1, keepdim=True) * (latent_size ** 0.5))\n",
        "        if self.l2_size == 0:\n",
        "            x = self.l1(z).view(-1, self.bottom_width ** 2, self.embed_dim)\n",
        "        elif self.l2_size > 1000:\n",
        "            x = self.l1(z).view(-1, self.bottom_width ** 2, self.l2_size//16)\n",
        "            x = self.l2(x)\n",
        "        else:\n",
        "            x = self.l1(z).view(-1, self.bottom_width ** 2, self.l2_size)\n",
        "            x = self.l2(x)\n",
        "            \n",
        "        x = x + self.pos_embed[0]\n",
        "        B = x.size()\n",
        "        H, W = self.bottom_width, self.bottom_width\n",
        "        x = self.blocks_1(x)\n",
        "        \n",
        "        x, H, W = bicubic_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[1]\n",
        "        B, _, C = x.size()\n",
        "        x = self.blocks_2(x)\n",
        "        \n",
        "        x, H, W = bicubic_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[2]\n",
        "        B, _, C = x.size()\n",
        "        x = self.blocks_3(x)\n",
        "        \n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[3]\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        x = self.blocks_4(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H*W,C)\n",
        "            \n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[4]\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        x = self.blocks_5(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H*W,C)\n",
        "                                        \n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[5]\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        x = self.blocks_6(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H,W,C).permute(0,3,1,2)\n",
        "        \n",
        "        \n",
        "        output = self.deconv(x)\n",
        "        return output\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "class DisBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=leakyrelu, norm_layer=nn.LayerNorm, separate=0):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gain = np.sqrt(0.5) if norm_layer == \"none\" else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x*self.gain + self.drop_path(self.attn(self.norm1(x)))*self.gain\n",
        "        x = x*self.gain + self.drop_path(self.mlp(self.norm2(x)))*self.gain\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, img_size=32, patch_size=None, in_chans=3, num_classes=1, embed_dim=None, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = args.df_dim  \n",
        "        \n",
        "        depth = args.d_depth\n",
        "        self.args = args\n",
        "        self.patch_size = patch_size = args.patch_size\n",
        "        norm_layer = args.d_norm\n",
        "        act_layer = args.d_act\n",
        "        self.window_size = args.d_window_size\n",
        "        \n",
        "        self.fRGB_1 = nn.Conv2d(3, embed_dim//8, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_2 = nn.Conv2d(3, embed_dim//8, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_3 = nn.Conv2d(3, embed_dim//4, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_4 = nn.Conv2d(3, embed_dim//2, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        \n",
        "        num_patches_1 = (args.img_size // patch_size)**2\n",
        "        num_patches_2 = ((args.img_size//2) // patch_size)**2\n",
        "        num_patches_3 = ((args.img_size//4) // patch_size)**2\n",
        "        num_patches_4 = ((args.img_size//8) // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, num_patches_1, embed_dim//8))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, num_patches_2, embed_dim//4))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, num_patches_3, embed_dim//2))\n",
        "        self.pos_embed_4 = nn.Parameter(torch.zeros(1, num_patches_4, embed_dim))\n",
        "        \n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks_1 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//8, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth+1)])\n",
        "        self.blocks_2 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//4, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.blocks_3 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//2, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.blocks_4 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.last_block = nn.Sequential(\n",
        "#             Block(\n",
        "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer),\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], act_layer=act_layer, norm_layer=norm_layer)\n",
        "            )\n",
        "        \n",
        "        self.norm = CustomNorm(norm_layer, embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed_1, std=.02)\n",
        "        trunc_normal_(self.pos_embed_2, std=.02)\n",
        "        trunc_normal_(self.pos_embed_3, std=.02)\n",
        "        trunc_normal_(self.pos_embed_4, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "            \n",
        "    def forward_features(self, x):\n",
        "        if \"None\" not in self.args.diff_aug:\n",
        "            x = DiffAugment(x, self.args.diff_aug, True)\n",
        "        \n",
        "        x_1 = self.fRGB_1(x).flatten(2).permute(0,2,1)\n",
        "        x_2 = self.fRGB_2(nn.AvgPool2d(2)(x)).flatten(2).permute(0,2,1)\n",
        "        x_3 = self.fRGB_3(nn.AvgPool2d(4)(x)).flatten(2).permute(0,2,1)\n",
        "        x_4 = self.fRGB_4(nn.AvgPool2d(8)(x)).flatten(2).permute(0,2,1)\n",
        "        B = x.shape[0]\n",
        "        \n",
        "        x = x_1 + self.pos_embed_1\n",
        "        x = self.pos_drop(x)\n",
        "        H = W = self.args.img_size // self.patch_size\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        for blk in self.blocks_1:\n",
        "            x = blk(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H*W,C)\n",
        "            \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_2], dim=-1)\n",
        "        x = x + self.pos_embed_2\n",
        "        \n",
        "        for blk in self.blocks_2:\n",
        "            x = blk(x)\n",
        "        \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_3], dim=-1)\n",
        "        x = x + self.pos_embed_3\n",
        "        \n",
        "        for blk in self.blocks_3:\n",
        "            x = blk(x)\n",
        "            \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_4], dim=-1)\n",
        "        x = x + self.pos_embed_4\n",
        "        \n",
        "        for blk in self.blocks_4:\n",
        "            x = blk(x)\n",
        "            \n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.last_block(x)\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgrahiFAqgcy"
      },
      "source": [
        "##ViT_custom_local544444_256_rp.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XmmaE2NqkNL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "#This portion is not needed as everything is in one place!\n",
        "\"\"\"\n",
        "from models_search.ViT_helper import DropPath, to_2tuple, trunc_normal_\n",
        "from models_search.diff_aug import DiffAugment\n",
        "\"\"\"\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "\n",
        "\n",
        "class matmul(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "    \n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=2, keepdim=True) + 1e-8)\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "def leakyrelu(x):\n",
        "    return nn.functional.leaky_relu_(x, 0.2)\n",
        "class CustomAct(nn.Module):\n",
        "    def __init__(self, act_layer):\n",
        "        super().__init__()\n",
        "        if act_layer == \"gelu\":\n",
        "            self.act_layer = gelu\n",
        "        elif act_layer == \"leakyrelu\":\n",
        "            self.act_layer = leakyrelu\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act_layer(x)\n",
        "        \n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = CustomAct(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    \n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "        self.window_size = window_size\n",
        "        \n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(window_size)\n",
        "        coords_w = torch.arange(window_size)\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += window_size - 1\n",
        "        relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    \n",
        "class CustomNorm(nn.Module):\n",
        "    def __init__(self, norm_layer, dim):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_layer\n",
        "        if norm_layer == \"ln\":\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        elif norm_layer == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(dim)\n",
        "        elif norm_layer == \"in\":\n",
        "            self.norm = nn.InstanceNorm1d(dim)\n",
        "        elif norm_layer == \"pn\":\n",
        "            self.norm = PixelNorm(dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.norm_type == \"bn\" or self.norm_type == \"in\":\n",
        "            x = self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
        "            return x\n",
        "        elif self.norm_type == \"none\":\n",
        "            return x\n",
        "        else:\n",
        "            return self.norm(x)\n",
        "        \n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "    \n",
        "class StageBlock(nn.Module):\n",
        "    def __init__(self, depth, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        models = [Block(\n",
        "                        dim=dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop, \n",
        "                        attn_drop=attn_drop, \n",
        "                        drop_path=drop_path, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=window_size\n",
        "                        ) for i in range(depth)]\n",
        "        self.block = nn.Sequential(*models)\n",
        "    def forward(self, x):\n",
        "#         for blk in self.block:\n",
        "#             # x = blk(x)\n",
        "#             checkpoint.checkpoint(blk, x)\n",
        "#         x = checkpoint.checkpoint(self.block, x)\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "def bicubic_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.functional.interpolate(x, scale_factor=2, mode='bicubic')\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, args, img_size=224, patch_size=16, in_chans=3, num_classes=10, embed_dim=384, depth=5,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super(Generator, self).__init__()\n",
        "        self.args = args\n",
        "        self.ch = embed_dim\n",
        "        self.bottom_width = args.bottom_width\n",
        "        self.embed_dim = embed_dim = args.gf_dim\n",
        "        self.window_size = args.g_window_size\n",
        "        norm_layer = args.g_norm\n",
        "        mlp_ratio = args.g_mlp\n",
        "        depth = [int(i) for i in args.g_depth.split(\",\")]\n",
        "        act_layer = args.g_act\n",
        "        self.l2_size = 0\n",
        "        \n",
        "        if self.l2_size == 0:\n",
        "            self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.embed_dim)\n",
        "        elif self.l2_size > 1000:\n",
        "            self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.l2_size//16)\n",
        "            self.l2 = nn.Sequential(\n",
        "                        nn.Linear(self.l2_size//16, self.l2_size),\n",
        "                        nn.Linear(self.l2_size, self.embed_dim)\n",
        "                      )\n",
        "        else:\n",
        "            self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.l2_size)\n",
        "            self.l2 = nn.Linear(self.l2_size, self.embed_dim)\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, self.bottom_width**2, embed_dim))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, (self.bottom_width*2)**2, embed_dim))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, (self.bottom_width*4)**2, embed_dim))\n",
        "        self.pos_embed_4 = nn.Parameter(torch.zeros(1, (self.bottom_width*8)**2, embed_dim//4))\n",
        "        self.pos_embed_5 = nn.Parameter(torch.zeros(1, (self.bottom_width*16)**2, embed_dim//16))\n",
        "        self.pos_embed_6 = nn.Parameter(torch.zeros(1, (self.bottom_width*32)**2, embed_dim//64))\n",
        "                                        \n",
        "        self.pos_embed = [\n",
        "            self.pos_embed_1,\n",
        "            self.pos_embed_2,\n",
        "            self.pos_embed_3,\n",
        "            self.pos_embed_4,\n",
        "            self.pos_embed_5,\n",
        "            self.pos_embed_6\n",
        "        ]\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  # stochastic depth decay rule\n",
        "        self.blocks_1 = StageBlock(\n",
        "                        depth=depth[0],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=8\n",
        "                        )\n",
        "        self.blocks_2 = StageBlock(\n",
        "                        depth=depth[1],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=16\n",
        "                        )\n",
        "        self.blocks_3 = StageBlock(\n",
        "                        depth=depth[2],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=32\n",
        "                        )\n",
        "        self.blocks_4 = StageBlock(\n",
        "                        depth=depth[3],\n",
        "                        dim=embed_dim//4, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=self.window_size\n",
        "                        )\n",
        "        self.blocks_5 = StageBlock(\n",
        "                        depth=depth[4],\n",
        "                        dim=embed_dim//16, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=self.window_size\n",
        "                        )\n",
        "        self.blocks_6 = StageBlock(\n",
        "                        depth=depth[5],\n",
        "                        dim=embed_dim//64, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=self.window_size\n",
        "                        )\n",
        "                                        \n",
        "        for i in range(len(self.pos_embed)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//64, 3, 1, 1, 0)\n",
        "        )\n",
        "#         self.apply(self._init_weights)\n",
        "    \n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.BatchNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.InstanceNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "    def forward(self, z, epoch):\n",
        "        if self.args.latent_norm:\n",
        "            latent_size = z.size(-1)\n",
        "            z = (z/z.norm(dim=-1, keepdim=True) * (latent_size ** 0.5))\n",
        "        if self.args.latent_norm:\n",
        "            latent_size = z.size(-1)\n",
        "            z = (z/z.norm(dim=-1, keepdim=True) * (latent_size ** 0.5))\n",
        "        if self.l2_size == 0:\n",
        "            x = self.l1(z).view(-1, self.bottom_width ** 2, self.embed_dim)\n",
        "        elif self.l2_size > 1000:\n",
        "            x = self.l1(z).view(-1, self.bottom_width ** 2, self.l2_size//16)\n",
        "            x = self.l2(x)\n",
        "        else:\n",
        "            x = self.l1(z).view(-1, self.bottom_width ** 2, self.l2_size)\n",
        "            x = self.l2(x)\n",
        "            \n",
        "        x = x + self.pos_embed[0]\n",
        "        B = x.size()\n",
        "        H, W = self.bottom_width, self.bottom_width\n",
        "        x = self.blocks_1(x)\n",
        "        \n",
        "        x, H, W = bicubic_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[1]\n",
        "        B, _, C = x.size()\n",
        "        x = self.blocks_2(x)\n",
        "        \n",
        "        x, H, W = bicubic_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[2]\n",
        "        B, _, C = x.size()\n",
        "        x = self.blocks_3(x)\n",
        "        \n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[3]\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        x = self.blocks_4(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H*W,C)\n",
        "            \n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[4]\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        x = self.blocks_5(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H*W,C)\n",
        "                                        \n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[5]\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        x = self.blocks_6(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H,W,C).permute(0,3,1,2)\n",
        "        \n",
        "        \n",
        "        output = self.deconv(x)\n",
        "        return output\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "class DisBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=leakyrelu, norm_layer=nn.LayerNorm, separate=0):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gain = np.sqrt(0.5) if norm_layer == \"none\" else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x*self.gain + self.drop_path(self.attn(self.norm1(x)))*self.gain\n",
        "        x = x*self.gain + self.drop_path(self.mlp(self.norm2(x)))*self.gain\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, img_size=32, patch_size=None, in_chans=3, num_classes=1, embed_dim=None, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = args.df_dim  \n",
        "        \n",
        "        depth = args.d_depth\n",
        "        self.args = args\n",
        "        self.patch_size = patch_size = args.patch_size\n",
        "        norm_layer = args.d_norm\n",
        "        act_layer = args.d_act\n",
        "        self.window_size = args.d_window_size\n",
        "        \n",
        "        self.fRGB_1 = nn.Conv2d(3, embed_dim//8, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_2 = nn.Conv2d(3, embed_dim//8, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_3 = nn.Conv2d(3, embed_dim//4, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_4 = nn.Conv2d(3, embed_dim//2, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        \n",
        "        num_patches_1 = (args.img_size // patch_size)**2\n",
        "        num_patches_2 = ((args.img_size//2) // patch_size)**2\n",
        "        num_patches_3 = ((args.img_size//4) // patch_size)**2\n",
        "        num_patches_4 = ((args.img_size//8) // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, num_patches_1, embed_dim//8))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, num_patches_2, embed_dim//4))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, num_patches_3, embed_dim//2))\n",
        "        self.pos_embed_4 = nn.Parameter(torch.zeros(1, num_patches_4, embed_dim))\n",
        "        \n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks_1 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//8, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth+1)])\n",
        "        self.blocks_2 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//4, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.blocks_3 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//2, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.blocks_4 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.last_block = nn.Sequential(\n",
        "#             Block(\n",
        "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer),\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], act_layer=act_layer, norm_layer=norm_layer)\n",
        "            )\n",
        "        \n",
        "        self.norm = CustomNorm(norm_layer, embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed_1, std=.02)\n",
        "        trunc_normal_(self.pos_embed_2, std=.02)\n",
        "        trunc_normal_(self.pos_embed_3, std=.02)\n",
        "        trunc_normal_(self.pos_embed_4, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "            \n",
        "    def forward_features(self, x):\n",
        "        if \"None\" not in self.args.diff_aug:\n",
        "            x = DiffAugment(x, self.args.diff_aug, True)\n",
        "        \n",
        "        x_1 = self.fRGB_1(x).flatten(2).permute(0,2,1)\n",
        "        x_2 = self.fRGB_2(nn.AvgPool2d(2)(x)).flatten(2).permute(0,2,1)\n",
        "        x_3 = self.fRGB_3(nn.AvgPool2d(4)(x)).flatten(2).permute(0,2,1)\n",
        "        x_4 = self.fRGB_4(nn.AvgPool2d(8)(x)).flatten(2).permute(0,2,1)\n",
        "        B = x.shape[0]\n",
        "        \n",
        "        x = x_1 + self.pos_embed_1\n",
        "        x = self.pos_drop(x)\n",
        "        H = W = self.args.img_size // self.patch_size\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        for blk in self.blocks_1:\n",
        "            x = blk(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H*W,C)\n",
        "            \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_2], dim=-1)\n",
        "        x = x + self.pos_embed_2\n",
        "        \n",
        "        for blk in self.blocks_2:\n",
        "            x = blk(x)\n",
        "        \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_3], dim=-1)\n",
        "        x = x + self.pos_embed_3\n",
        "        \n",
        "        for blk in self.blocks_3:\n",
        "            x = blk(x)\n",
        "            \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_4], dim=-1)\n",
        "        x = x + self.pos_embed_4\n",
        "        \n",
        "        for blk in self.blocks_4:\n",
        "            x = blk(x)\n",
        "            \n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.last_block(x)\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK35fF6squoi"
      },
      "source": [
        "##ViT_custom_rp.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc5-OX1pq096"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "\"\"\"\n",
        "from models_search.ViT_helper import DropPath, to_2tuple, trunc_normal_\n",
        "from models_search.diff_aug import DiffAugment\n",
        "\"\"\"\n",
        "class matmul(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=2, keepdim=True) + 1e-8)\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "def leakyrelu(x):\n",
        "    return nn.functional.leaky_relu_(x, 0.2)\n",
        "\n",
        "class CustomAct(nn.Module):\n",
        "    def __init__(self, act_layer):\n",
        "        super().__init__()\n",
        "        if act_layer == \"gelu\":\n",
        "            self.act_layer = gelu\n",
        "        elif act_layer == \"leakyrelu\":\n",
        "            self.act_layer = leakyrelu\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act_layer(x)\n",
        "        \n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = CustomAct(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "        self.window_size = window_size\n",
        "        if self.window_size != 0:\n",
        "            self.relative_position_bias_table = nn.Parameter(\n",
        "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "            # get pair-wise relative position index for each token inside the window\n",
        "            coords_h = torch.arange(window_size)\n",
        "            coords_w = torch.arange(window_size)\n",
        "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "            relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "            relative_coords[:, :, 1] += window_size - 1\n",
        "            relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        if self.window_size != 0:\n",
        "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1).clone()].view(\n",
        "                self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "            attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class CustomNorm(nn.Module):\n",
        "    def __init__(self, norm_layer, dim):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_layer\n",
        "        if norm_layer == \"ln\":\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        elif norm_layer == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(dim)\n",
        "        elif norm_layer == \"in\":\n",
        "            self.norm = nn.InstanceNorm1d(dim)\n",
        "        elif norm_layer == \"pn\":\n",
        "            self.norm = PixelNorm(dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.norm_type == \"bn\" or self.norm_type == \"in\":\n",
        "            x = self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
        "            return x\n",
        "        elif self.norm_type == \"none\":\n",
        "            return x\n",
        "        else:\n",
        "            return self.norm(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=8):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "    \n",
        "class StageBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, depth, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=8):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.block = nn.ModuleList([\n",
        "                        Block(\n",
        "                        dim=dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop, \n",
        "                        attn_drop=attn_drop, \n",
        "                        drop_path=drop_path, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=window_size\n",
        "                        ) for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.block:\n",
        "            x = blk(x)\n",
        "        return x\n",
        "\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, args, img_size=224, patch_size=16, in_chans=3, num_classes=10, embed_dim=384, depth=5,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super(Generator, self).__init__()\n",
        "        self.args = args\n",
        "        self.ch = embed_dim\n",
        "        self.bottom_width = args.bottom_width\n",
        "        self.embed_dim = embed_dim = args.gf_dim\n",
        "        norm_layer = args.g_norm\n",
        "        mlp_ratio = args.g_mlp\n",
        "        depth = [int(i) for i in args.g_depth.split(\",\")]\n",
        "        act_layer = args.g_act\n",
        "        \n",
        "        self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.embed_dim)\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, self.bottom_width**2, embed_dim))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, (self.bottom_width*2)**2, embed_dim//4))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, (self.bottom_width*4)**2, embed_dim//16))\n",
        "        self.pos_embed = [\n",
        "            self.pos_embed_1,\n",
        "            self.pos_embed_2,\n",
        "            self.pos_embed_3\n",
        "        ]\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  # stochastic depth decay rule\n",
        "        self.blocks = StageBlock(\n",
        "                        depth=depth[0],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=args.bottom_width,\n",
        "                        )\n",
        "        self.upsample_blocks = nn.ModuleList([\n",
        "                    StageBlock(\n",
        "                        depth=depth[1],\n",
        "                        dim=embed_dim//4, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=args.bottom_width*2,\n",
        "                        ),\n",
        "                    StageBlock(\n",
        "                        depth=depth[2],\n",
        "                        dim=embed_dim//16, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=args.bottom_width*4,\n",
        "                        )\n",
        "                    ])\n",
        "        for i in range(len(self.pos_embed)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//16, 3, 1, 1, 0)\n",
        "        )\n",
        "#         self.apply(self._init_weights)\n",
        "    \n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.BatchNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.InstanceNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "            \n",
        "    def set_arch(self, x, cur_stage):\n",
        "        pass\n",
        "\n",
        "    def forward(self, z, epoch):\n",
        "        if self.args.latent_norm:\n",
        "            latent_size = z.size(-1)\n",
        "            z = (z/z.norm(dim=-1, keepdim=True) * (latent_size ** 0.5))\n",
        "        x = self.l1(z).view(-1, self.bottom_width ** 2, self.embed_dim)\n",
        "        x = x + self.pos_embed[0].to(x.get_device())\n",
        "        B = x.size()\n",
        "        H, W = self.bottom_width, self.bottom_width\n",
        "        x = self.blocks(x)\n",
        "        for index, blk in enumerate(self.upsample_blocks):\n",
        "            # x = x.permute(0,2,1)\n",
        "            # x = x.view(-1, self.embed_dim, H, W)\n",
        "            x, H, W = pixel_upsample(x, H, W)\n",
        "            x = x + self.pos_embed[index+1].to(x.get_device())\n",
        "            x = blk(x)\n",
        "            # _, _, H, W = x.size()\n",
        "            # x = x.view(-1, self.embed_dim, H*W)\n",
        "            # x = x.permute(0,2,1)\n",
        "        output = self.deconv(x.permute(0, 2, 1).view(-1, self.embed_dim//16, H, W))\n",
        "        return output\n",
        "\n",
        "\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=leakyrelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gain = np.sqrt(0.5) if norm_layer == \"none\" else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x*self.gain + self.drop_path(self.attn(self.norm1(x)))*self.gain\n",
        "        x = x*self.gain + self.drop_path(self.mlp(self.norm2(x)))*self.gain\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, args, img_size=32, patch_size=None, in_chans=3, num_classes=1, embed_dim=None, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = args.df_dim  \n",
        "        \n",
        "        depth = args.d_depth\n",
        "        self.args = args\n",
        "        patch_size = args.patch_size\n",
        "        norm_layer = args.d_norm\n",
        "        act_layer = args.d_act\n",
        "        mlp_ratio = args.d_mlp\n",
        "        if hybrid_backbone is not None:\n",
        "            self.patch_embed = HybridEmbed(\n",
        "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        else:\n",
        "            self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        num_patches = (args.img_size // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim, \n",
        "                num_heads=num_heads, \n",
        "                mlp_ratio=mlp_ratio, \n",
        "                qkv_bias=qkv_bias, \n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate, \n",
        "                attn_drop=attn_drop_rate, \n",
        "                drop_path=dpr[i],\n",
        "                act_layer=act_layer,\n",
        "                norm_layer=norm_layer\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "        \n",
        "        self.norm = CustomNorm(norm_layer, embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        if \"None\" not in self.args.diff_aug:\n",
        "            x = DiffAugment(x, self.args.diff_aug, True)\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x).flatten(2).permute(0,2,1)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cihNMpuQq52i"
      },
      "source": [
        "##ViT_custom_scale2_rp_noise.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAK2sQuUq2IA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "#This portion is not needed as everything is in one place!\n",
        "\"\"\"\n",
        "from models_search.ViT_helper import DropPath, to_2tuple, trunc_normal_\n",
        "from models_search.diff_aug import DiffAugment\n",
        "from utils.utils import make_grid, save_image\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from models_search.ada import *\n",
        "\"\"\"\n",
        "import scipy.signal\n",
        "\n",
        "\n",
        "wavelets = {\n",
        "    'haar': [0.7071067811865476, 0.7071067811865476],\n",
        "    'db1':  [0.7071067811865476, 0.7071067811865476],\n",
        "    'db2':  [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],\n",
        "    'db5':  [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125],\n",
        "    'db6':  [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017],\n",
        "    'db7':  [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236],\n",
        "    'db8':  [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161],\n",
        "    'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n",
        "    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],\n",
        "    'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148],\n",
        "    'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255],\n",
        "    'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609],\n",
        "}\n",
        "\n",
        "class matmul(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=2, keepdim=True) + 1e-8)\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "def leakyrelu(x):\n",
        "    return nn.functional.leaky_relu_(x, 0.2)\n",
        "\n",
        "class CustomAct(nn.Module):\n",
        "    def __init__(self, act_layer):\n",
        "        super().__init__()\n",
        "        if act_layer == \"gelu\":\n",
        "            self.act_layer = gelu\n",
        "        elif act_layer == \"leakyrelu\":\n",
        "            self.act_layer = leakyrelu\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act_layer(x)\n",
        "        \n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = CustomAct(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "        self.window_size = window_size\n",
        "        self.noise_strength_1 = torch.nn.Parameter(torch.zeros([]))\n",
        "        if self.window_size != 0:\n",
        "            self.relative_position_bias_table = nn.Parameter(\n",
        "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "            # get pair-wise relative position index for each token inside the window\n",
        "            coords_h = torch.arange(window_size)\n",
        "            coords_w = torch.arange(window_size)\n",
        "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "            relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "            relative_coords[:, :, 1] += window_size - 1\n",
        "            relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "            \n",
        "\n",
        "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + torch.randn([x.size(0), x.size(1), 1], device=x.device) * self.noise_strength_1\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        if self.window_size != 0:\n",
        "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1).clone()].view(\n",
        "                self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "            attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class CustomNorm(nn.Module):\n",
        "    def __init__(self, norm_layer, dim):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_layer\n",
        "        if norm_layer == \"ln\":\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        elif norm_layer == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(dim)\n",
        "        elif norm_layer == \"in\":\n",
        "            self.norm = nn.InstanceNorm1d(dim)\n",
        "        elif norm_layer == \"pn\":\n",
        "            self.norm = PixelNorm(dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.norm_type == \"bn\" or self.norm_type == \"in\":\n",
        "            x = self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
        "            return x\n",
        "        elif self.norm_type == \"none\":\n",
        "            return x\n",
        "        else:\n",
        "            return self.norm(x)\n",
        "        \n",
        "        \n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "    \n",
        "class StageBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, depth, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.block = nn.ModuleList([\n",
        "                        Block(\n",
        "                        dim=dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop, \n",
        "                        attn_drop=attn_drop, \n",
        "                        drop_path=drop_path, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=window_size,\n",
        "                        ) for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.block:\n",
        "            x = blk(x)\n",
        "        return x\n",
        "\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, args, img_size=224, patch_size=16, in_chans=3, num_classes=10, embed_dim=384, depth=5,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super(Generator, self).__init__()\n",
        "        self.args = args\n",
        "        self.ch = embed_dim\n",
        "        self.bottom_width = args.bottom_width\n",
        "        self.embed_dim = embed_dim = args.gf_dim\n",
        "        norm_layer = args.g_norm\n",
        "        depth = [int(i) for i in args.g_depth.split(\",\")]\n",
        "        act_layer = args.g_act\n",
        "        \n",
        "        self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.embed_dim)\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, self.bottom_width**2, embed_dim))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, (self.bottom_width*2)**2, embed_dim//4))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, (self.bottom_width*4)**2, embed_dim//16))\n",
        "        self.pos_embed = [\n",
        "            self.pos_embed_1,\n",
        "            self.pos_embed_2,\n",
        "            self.pos_embed_3\n",
        "        ]\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  # stochastic depth decay rule\n",
        "        self.blocks = StageBlock(\n",
        "                        depth=depth[0],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=8,\n",
        "                        )\n",
        "        self.upsample_blocks = nn.ModuleList([\n",
        "                    StageBlock(\n",
        "                        depth=depth[1],\n",
        "                        dim=embed_dim//4, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=16,\n",
        "                        ),\n",
        "                    StageBlock(\n",
        "                        depth=depth[2],\n",
        "                        dim=embed_dim//16, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer,\n",
        "                        window_size=32,\n",
        "                        )\n",
        "                    ])\n",
        "        for i in range(len(self.pos_embed)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "\n",
        "        self.tRGB_1 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim, 3, 1, 1, 0)\n",
        "        )\n",
        "        self.tRGB_2 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//4, 3, 1, 1, 0)\n",
        "        )\n",
        "        self.tRGB_3 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//16, 3, 1, 1, 0)\n",
        "        )\n",
        "    \n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.BatchNorm1d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.InstanceNorm1d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "            \n",
        "    def set_arch(self, x, cur_stage):\n",
        "        pass\n",
        "\n",
        "    def forward(self, z, epoch):\n",
        "        x = self.l1(z).view(-1, self.bottom_width ** 2, self.embed_dim)\n",
        "        x = x + self.pos_embed[0].to(x.get_device())\n",
        "        B = x.size(0)\n",
        "        H, W = self.bottom_width, self.bottom_width\n",
        "        x = self.blocks(x)\n",
        "        \n",
        "        x_1 = self.tRGB_1(x.permute(0,2,1).view(B,self.embed_dim,H,W))\n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[1].to(x.get_device())\n",
        "        x = self.upsample_blocks[0](x)\n",
        "        \n",
        "        x_2 = self.tRGB_2(x.permute(0,2,1).view(B,self.embed_dim//4,H,W))\n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[2].to(x.get_device())\n",
        "        x = self.upsample_blocks[1](x)\n",
        "\n",
        "        x_3 = self.tRGB_3(x.permute(0,2,1).view(B,self.embed_dim//16,H,W))\n",
        "        output = F.interpolate(x_1, scale_factor=4) + F.interpolate(x_2, scale_factor=2) + x_3\n",
        "        return output\n",
        "\n",
        "\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "\n",
        "class SpaceToDepth(nn.Module):\n",
        "    def __init__(self, block_size=2):\n",
        "        super().__init__()\n",
        "        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        S = self.block_size\n",
        "        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\n",
        "        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n",
        "        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\n",
        "        return x\n",
        "    \n",
        "class DisBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=leakyrelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gain = np.sqrt(0.5) if norm_layer == \"none\" else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x*self.gain + self.drop_path(self.attn(self.norm1(x)))*self.gain\n",
        "        x = x*self.gain + self.drop_path(self.mlp(self.norm2(x)))*self.gain\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, img_size=32, patch_size=None, in_chans=3, num_classes=1, embed_dim=None, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = args.df_dim  \n",
        "        \n",
        "        depth = args.d_depth\n",
        "        self.args = args\n",
        "        self.patch_size = patch_size = args.patch_size\n",
        "        norm_layer = args.d_norm\n",
        "        self.window_size = args.d_window_size\n",
        "        \n",
        "        act_layer = args.d_act\n",
        "        self.fRGB_1 = nn.Conv2d(3, embed_dim//4*3, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_2 = nn.Conv2d(3, embed_dim//4, kernel_size=patch_size*2, stride=patch_size*2, padding=0)\n",
        "#         self.fRGB_4 = nn.Conv2d(3, embed_dim//2, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        \n",
        "        num_patches_1 = (args.img_size // patch_size)**2\n",
        "        num_patches_2 = ((args.img_size//2) // patch_size)**2\n",
        "#         num_patches_4 = ((args.img_size//8) // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, num_patches_1, embed_dim//4*3))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, num_patches_2, embed_dim))\n",
        "#         self.pos_embed_4 = nn.Parameter(torch.zeros(1, num_patches_4, embed_dim))\n",
        "        \n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks_1 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//4*3, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=args.bottom_width*4//2)\n",
        "            for i in range(depth)])\n",
        "        self.blocks_2 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=args.bottom_width*4//4)\n",
        "            for i in range(depth)])\n",
        "        \n",
        "        self.last_block = nn.Sequential(\n",
        "#             Block(\n",
        "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer),\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], act_layer=act_layer, norm_layer=norm_layer, window_size=0)\n",
        "            )\n",
        "        \n",
        "        self.norm = CustomNorm(norm_layer, embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed_1, std=.02)\n",
        "        trunc_normal_(self.pos_embed_2, std=.02)\n",
        "#         trunc_normal_(self.pos_embed_4, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "        if 'filter' in self.args.diff_aug:\n",
        "            Hz_lo = np.asarray(wavelets['sym2'])            # H(z)\n",
        "            Hz_hi = Hz_lo * ((-1) ** np.arange(Hz_lo.size)) # H(-z)\n",
        "            Hz_lo2 = np.convolve(Hz_lo, Hz_lo[::-1]) / 2    # H(z) * H(z^-1) / 2\n",
        "            Hz_hi2 = np.convolve(Hz_hi, Hz_hi[::-1]) / 2    # H(-z) * H(-z^-1) / 2\n",
        "            Hz_fbank = np.eye(4, 1)                         # Bandpass(H(z), b_i)\n",
        "            for i in range(1, Hz_fbank.shape[0]):\n",
        "                Hz_fbank = np.dstack([Hz_fbank, np.zeros_like(Hz_fbank)]).reshape(Hz_fbank.shape[0], -1)[:, :-1]\n",
        "                Hz_fbank = scipy.signal.convolve(Hz_fbank, [Hz_lo2])\n",
        "                Hz_fbank[i, (Hz_fbank.shape[1] - Hz_hi2.size) // 2 : (Hz_fbank.shape[1] + Hz_hi2.size) // 2] += Hz_hi2\n",
        "            Hz_fbank = torch.as_tensor(Hz_fbank, dtype=torch.float32)\n",
        "            self.register_buffer('Hz_fbank', torch.as_tensor(Hz_fbank, dtype=torch.float32))\n",
        "        else:\n",
        "            self.Hz_fbank = None\n",
        "        if 'geo' in self.args.diff_aug:\n",
        "            self.register_buffer('Hz_geom', upfirdn2d.setup_filter(wavelets['sym6']))\n",
        "        else:\n",
        "            self.Hz_geom = None\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "            \n",
        "    def forward_features(self, x, aug=True, epoch=400):\n",
        "        if \"None\" not in self.args.diff_aug and aug:\n",
        "            x = DiffAugment(x, self.args.diff_aug, True, [self.Hz_geom, self.Hz_fbank])\n",
        "#         with torch.no_grad():\n",
        "#             save_image(x.clone(), f'in_{self.args.rank}.png', nrow=4, padding=1, normalize=True, scale_each=True)\n",
        "#         import time\n",
        "#         time.sleep(5)\n",
        "        B, _, H, W = x.size()\n",
        "        H = W = H//self.patch_size\n",
        "        \n",
        "        x_1 = self.fRGB_1(x).flatten(2).permute(0,2,1)\n",
        "        x_2 = self.fRGB_2(x).flatten(2).permute(0,2,1)\n",
        "#         x_4 = self.fRGB_4(nn.AvgPool2d(8)(x)).flatten(2).permute(0,2,1)\n",
        "        B = x.shape[0]\n",
        "        \n",
        "\n",
        "        x = x_1 + self.pos_embed_1\n",
        "        B, _, C = x.size()\n",
        "        for blk in self.blocks_1:\n",
        "            x = blk(x)\n",
        "            \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_2], dim=-1)\n",
        "        x = x + self.pos_embed_2\n",
        "        \n",
        "        for blk in self.blocks_2:\n",
        "            x = blk(x)\n",
        "        \n",
        "            \n",
        "            \n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.last_block(x)\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x, aug=True, epoch=400):\n",
        "        x = self.forward_features(x, aug=aug, epoch=epoch)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NecW75Klv6dM"
      },
      "source": [
        "##ViT_custom_scale2.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGU6Ppnjv-4T"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from models_search.ViT_helper import DropPath, to_2tuple, trunc_normal_\n",
        "from models_search.diff_aug import DiffAugment\n",
        "from utils.utils import make_grid, save_image\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from models_search.ada import *\n",
        "'''\n",
        "import scipy.signal\n",
        "\n",
        "\n",
        "wavelets = {\n",
        "    'haar': [0.7071067811865476, 0.7071067811865476],\n",
        "    'db1':  [0.7071067811865476, 0.7071067811865476],\n",
        "    'db2':  [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],\n",
        "    'db5':  [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125],\n",
        "    'db6':  [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017],\n",
        "    'db7':  [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236],\n",
        "    'db8':  [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161],\n",
        "    'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n",
        "    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],\n",
        "    'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148],\n",
        "    'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255],\n",
        "    'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609],\n",
        "}\n",
        "\n",
        "class matmul(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=2, keepdim=True) + 1e-8)\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "def leakyrelu(x):\n",
        "    return nn.functional.leaky_relu_(x, 0.2)\n",
        "\n",
        "class CustomAct(nn.Module):\n",
        "    def __init__(self, act_layer):\n",
        "        super().__init__()\n",
        "        if act_layer == \"gelu\":\n",
        "            self.act_layer = gelu\n",
        "        elif act_layer == \"leakyrelu\":\n",
        "            self.act_layer = leakyrelu\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act_layer(x)\n",
        "        \n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = CustomAct(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "        self.window_size = window_size\n",
        "        if self.window_size != 0:\n",
        "            self.relative_position_bias_table = nn.Parameter(\n",
        "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "            # get pair-wise relative position index for each token inside the window\n",
        "            coords_h = torch.arange(window_size)\n",
        "            coords_w = torch.arange(window_size)\n",
        "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "            relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "            relative_coords[:, :, 1] += window_size - 1\n",
        "            relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        if self.window_size != 0:\n",
        "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1).clone()].view(\n",
        "                self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "            attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class CustomNorm(nn.Module):\n",
        "    def __init__(self, norm_layer, dim):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_layer\n",
        "        if norm_layer == \"ln\":\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        elif norm_layer == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(dim)\n",
        "        elif norm_layer == \"in\":\n",
        "            self.norm = nn.InstanceNorm1d(dim)\n",
        "        elif norm_layer == \"pn\":\n",
        "            self.norm = PixelNorm(dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.norm_type == \"bn\" or self.norm_type == \"in\":\n",
        "            x = self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
        "            return x\n",
        "        elif self.norm_type == \"none\":\n",
        "            return x\n",
        "        else:\n",
        "            return self.norm(x)\n",
        "        \n",
        "        \n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "    \n",
        "class StageBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, depth, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.block = nn.ModuleList([\n",
        "                        Block(\n",
        "                        dim=dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop, \n",
        "                        attn_drop=attn_drop, \n",
        "                        drop_path=drop_path, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        ) for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.block:\n",
        "            x = blk(x)\n",
        "        return x\n",
        "\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, args, img_size=224, patch_size=16, in_chans=3, num_classes=10, embed_dim=384, depth=5,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super(Generator, self).__init__()\n",
        "        self.args = args\n",
        "        self.ch = embed_dim\n",
        "        self.bottom_width = args.bottom_width\n",
        "        self.embed_dim = embed_dim = args.gf_dim\n",
        "        norm_layer = args.g_norm\n",
        "        depth = [int(i) for i in args.g_depth.split(\",\")]\n",
        "        act_layer = args.g_act\n",
        "        \n",
        "        self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.embed_dim)\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, self.bottom_width**2, embed_dim))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, (self.bottom_width*2)**2, embed_dim//4))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, (self.bottom_width*4)**2, embed_dim//16))\n",
        "        self.pos_embed = [\n",
        "            self.pos_embed_1,\n",
        "            self.pos_embed_2,\n",
        "            self.pos_embed_3\n",
        "        ]\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  # stochastic depth decay rule\n",
        "        self.blocks = StageBlock(\n",
        "                        depth=depth[0],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        )\n",
        "        self.upsample_blocks = nn.ModuleList([\n",
        "                    StageBlock(\n",
        "                        depth=depth[1],\n",
        "                        dim=embed_dim//4, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        ),\n",
        "                    StageBlock(\n",
        "                        depth=depth[2],\n",
        "                        dim=embed_dim//16, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        )\n",
        "                    ])\n",
        "        for i in range(len(self.pos_embed)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "\n",
        "        self.tRGB_1 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim, 3, 1, 1, 0)\n",
        "        )\n",
        "        self.tRGB_2 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//4, 3, 1, 1, 0)\n",
        "        )\n",
        "        self.tRGB_3 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//16, 3, 1, 1, 0)\n",
        "        )\n",
        "    \n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.BatchNorm1d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.InstanceNorm1d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "            \n",
        "    def set_arch(self, x, cur_stage):\n",
        "        pass\n",
        "\n",
        "    def forward(self, z, epoch):\n",
        "        x = self.l1(z).view(-1, self.bottom_width ** 2, self.embed_dim)\n",
        "        x = x + self.pos_embed[0].to(x.get_device())\n",
        "        B = x.size(0)\n",
        "        H, W = self.bottom_width, self.bottom_width\n",
        "        x = self.blocks(x)\n",
        "        \n",
        "        x_1 = self.tRGB_1(x.permute(0,2,1).view(B,self.embed_dim,H,W))\n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[1].to(x.get_device())\n",
        "        x = self.upsample_blocks[0](x)\n",
        "        \n",
        "        x_2 = self.tRGB_2(x.permute(0,2,1).view(B,self.embed_dim//4,H,W))\n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[2].to(x.get_device())\n",
        "        x = self.upsample_blocks[1](x)\n",
        "\n",
        "        x_3 = self.tRGB_3(x.permute(0,2,1).view(B,self.embed_dim//16,H,W))\n",
        "        output = F.interpolate(x_1, scale_factor=4) + F.interpolate(x_2, scale_factor=2) + x_3\n",
        "        return output\n",
        "\n",
        "\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "\n",
        "class SpaceToDepth(nn.Module):\n",
        "    def __init__(self, block_size=2):\n",
        "        super().__init__()\n",
        "        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        S = self.block_size\n",
        "        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\n",
        "        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n",
        "        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\n",
        "        return x\n",
        "    \n",
        "class DisBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=leakyrelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gain = np.sqrt(0.5) if norm_layer == \"none\" else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x*self.gain + self.drop_path(self.attn(self.norm1(x)))*self.gain\n",
        "        x = x*self.gain + self.drop_path(self.mlp(self.norm2(x)))*self.gain\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, img_size=32, patch_size=None, in_chans=3, num_classes=1, embed_dim=None, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = args.df_dim  \n",
        "        \n",
        "        depth = args.d_depth\n",
        "        self.args = args\n",
        "        self.patch_size = patch_size = args.patch_size\n",
        "        norm_layer = args.d_norm\n",
        "        self.window_size = args.d_window_size\n",
        "        \n",
        "        act_layer = args.d_act\n",
        "        self.fRGB_1 = nn.Conv2d(3, embed_dim//4*3, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_2 = nn.Conv2d(3, embed_dim//4, kernel_size=patch_size*2, stride=patch_size*2, padding=0)\n",
        "#         self.fRGB_4 = nn.Conv2d(3, embed_dim//2, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        \n",
        "        num_patches_1 = (args.img_size // patch_size)**2\n",
        "        num_patches_2 = ((args.img_size//2) // patch_size)**2\n",
        "#         num_patches_4 = ((args.img_size//8) // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, num_patches_1, embed_dim//4*3))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, num_patches_2, embed_dim))\n",
        "#         self.pos_embed_4 = nn.Parameter(torch.zeros(1, num_patches_4, embed_dim))\n",
        "        \n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks_1 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//4*3, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=args.bottom_width*4//2)\n",
        "            for i in range(depth)])\n",
        "        self.blocks_2 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=args.bottom_width*4//4)\n",
        "            for i in range(depth)])\n",
        "        \n",
        "        self.last_block = nn.Sequential(\n",
        "#             Block(\n",
        "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer),\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], act_layer=act_layer, norm_layer=norm_layer, window_size=0)\n",
        "            )\n",
        "        \n",
        "        self.norm = CustomNorm(norm_layer, embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed_1, std=.02)\n",
        "        trunc_normal_(self.pos_embed_2, std=.02)\n",
        "#         trunc_normal_(self.pos_embed_4, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "        if 'filter' in self.args.diff_aug:\n",
        "            Hz_lo = np.asarray(wavelets['sym2'])            # H(z)\n",
        "            Hz_hi = Hz_lo * ((-1) ** np.arange(Hz_lo.size)) # H(-z)\n",
        "            Hz_lo2 = np.convolve(Hz_lo, Hz_lo[::-1]) / 2    # H(z) * H(z^-1) / 2\n",
        "            Hz_hi2 = np.convolve(Hz_hi, Hz_hi[::-1]) / 2    # H(-z) * H(-z^-1) / 2\n",
        "            Hz_fbank = np.eye(4, 1)                         # Bandpass(H(z), b_i)\n",
        "            for i in range(1, Hz_fbank.shape[0]):\n",
        "                Hz_fbank = np.dstack([Hz_fbank, np.zeros_like(Hz_fbank)]).reshape(Hz_fbank.shape[0], -1)[:, :-1]\n",
        "                Hz_fbank = scipy.signal.convolve(Hz_fbank, [Hz_lo2])\n",
        "                Hz_fbank[i, (Hz_fbank.shape[1] - Hz_hi2.size) // 2 : (Hz_fbank.shape[1] + Hz_hi2.size) // 2] += Hz_hi2\n",
        "            Hz_fbank = torch.as_tensor(Hz_fbank, dtype=torch.float32)\n",
        "            self.register_buffer('Hz_fbank', torch.as_tensor(Hz_fbank, dtype=torch.float32))\n",
        "        else:\n",
        "            self.Hz_fbank = None\n",
        "        if 'geo' in self.args.diff_aug:\n",
        "            self.register_buffer('Hz_geom', upfirdn2d.setup_filter(wavelets['sym6']))\n",
        "        else:\n",
        "            self.Hz_geom = None\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "            \n",
        "    def forward_features(self, x, aug=True, epoch=400):\n",
        "        if \"None\" not in self.args.diff_aug and aug:\n",
        "            x = DiffAugment(x, self.args.diff_aug, True, [self.Hz_geom, self.Hz_fbank])\n",
        "#         with torch.no_grad():\n",
        "#             save_image(x.clone(), f'in_{self.args.rank}.png', nrow=4, padding=1, normalize=True, scale_each=True)\n",
        "#         import time\n",
        "#         time.sleep(5)\n",
        "        B, _, H, W = x.size()\n",
        "        H = W = H//self.patch_size\n",
        "        \n",
        "        x_1 = self.fRGB_1(x).flatten(2).permute(0,2,1)\n",
        "        x_2 = self.fRGB_2(x).flatten(2).permute(0,2,1)\n",
        "#         x_4 = self.fRGB_4(nn.AvgPool2d(8)(x)).flatten(2).permute(0,2,1)\n",
        "        B = x.shape[0]\n",
        "        \n",
        "\n",
        "        x = x_1 + self.pos_embed_1\n",
        "        B, _, C = x.size()\n",
        "        for blk in self.blocks_1:\n",
        "            x = blk(x)\n",
        "            \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_2], dim=-1)\n",
        "        x = x + self.pos_embed_2\n",
        "        \n",
        "        for blk in self.blocks_2:\n",
        "            x = blk(x)\n",
        "        \n",
        "            \n",
        "            \n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.last_block(x)\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x, aug=True, epoch=400):\n",
        "        x = self.forward_features(x, aug=aug, epoch=epoch)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK7OH1kNwBjz"
      },
      "source": [
        "##ViT_custom.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc5r30VSwEl7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from models_search.ViT_helper import DropPath, to_2tuple, trunc_normal_\n",
        "from models_search.diff_aug import DiffAugment\n",
        "'''\n",
        "class matmul(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=2, keepdim=True) + 1e-8)\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "def leakyrelu(x):\n",
        "    return nn.functional.leaky_relu_(x, 0.2)\n",
        "\n",
        "class CustomAct(nn.Module):\n",
        "    def __init__(self, act_layer):\n",
        "        super().__init__()\n",
        "        if act_layer == \"gelu\":\n",
        "            self.act_layer = gelu\n",
        "        elif act_layer == \"leakyrelu\":\n",
        "            self.act_layer = leakyrelu\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act_layer(x)\n",
        "        \n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = CustomAct(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "        self.noise_strength_1 = torch.nn.Parameter(torch.zeros([]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + torch.randn([x.size(0), x.size(1), 1], device=x.device) * self.noise_strength_1\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class CustomNorm(nn.Module):\n",
        "    def __init__(self, norm_layer, dim):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_layer\n",
        "        if norm_layer == \"ln\":\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        elif norm_layer == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(dim)\n",
        "        elif norm_layer == \"in\":\n",
        "            self.norm = nn.InstanceNorm1d(dim)\n",
        "        elif norm_layer == \"pn\":\n",
        "            self.norm = PixelNorm(dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.norm_type == \"bn\" or self.norm_type == \"in\":\n",
        "            x = self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
        "            return x\n",
        "        elif self.norm_type == \"none\":\n",
        "            return x\n",
        "        else:\n",
        "            return self.norm(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "    \n",
        "class StageBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, depth, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.block = nn.ModuleList([\n",
        "                        Block(\n",
        "                        dim=dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop, \n",
        "                        attn_drop=attn_drop, \n",
        "                        drop_path=drop_path, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        ) for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.block:\n",
        "            x = blk(x)\n",
        "        return x\n",
        "\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, args, img_size=224, patch_size=16, in_chans=3, num_classes=10, embed_dim=384, depth=5,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super(Generator, self).__init__()\n",
        "        self.args = args\n",
        "        self.ch = embed_dim\n",
        "        self.bottom_width = args.bottom_width\n",
        "        self.embed_dim = embed_dim = args.gf_dim\n",
        "        norm_layer = args.g_norm\n",
        "        mlp_ratio = args.g_mlp\n",
        "        depth = [int(i) for i in args.g_depth.split(\",\")]\n",
        "        act_layer = args.g_act\n",
        "        \n",
        "        self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.embed_dim)\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, self.bottom_width**2, embed_dim))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, (self.bottom_width*2)**2, embed_dim//4))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, (self.bottom_width*4)**2, embed_dim//16))\n",
        "        self.pos_embed = [\n",
        "            self.pos_embed_1,\n",
        "            self.pos_embed_2,\n",
        "            self.pos_embed_3\n",
        "        ]\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  # stochastic depth decay rule\n",
        "        self.blocks = StageBlock(\n",
        "                        depth=depth[0],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        )\n",
        "        self.upsample_blocks = nn.ModuleList([\n",
        "                    StageBlock(\n",
        "                        depth=depth[1],\n",
        "                        dim=embed_dim//4, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        ),\n",
        "                    StageBlock(\n",
        "                        depth=depth[2],\n",
        "                        dim=embed_dim//16, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        )\n",
        "                    ])\n",
        "        for i in range(len(self.pos_embed)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//16, 3, 1, 1, 0)\n",
        "        )\n",
        "#         self.apply(self._init_weights)\n",
        "    \n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.BatchNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.InstanceNorm1d):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "            \n",
        "    def set_arch(self, x, cur_stage):\n",
        "        pass\n",
        "\n",
        "    def forward(self, z, epoch):\n",
        "        if self.args.latent_norm:\n",
        "            latent_size = z.size(-1)\n",
        "            z = (z/z.norm(dim=-1, keepdim=True) * (latent_size ** 0.5))\n",
        "        x = self.l1(z).view(-1, self.bottom_width ** 2, self.embed_dim)\n",
        "        x = x + self.pos_embed[0].to(x.get_device())\n",
        "        B = x.size()\n",
        "        H, W = self.bottom_width, self.bottom_width\n",
        "        x = self.blocks(x)\n",
        "        for index, blk in enumerate(self.upsample_blocks):\n",
        "            # x = x.permute(0,2,1)\n",
        "            # x = x.view(-1, self.embed_dim, H, W)\n",
        "            x, H, W = pixel_upsample(x, H, W)\n",
        "            x = x + self.pos_embed[index+1].to(x.get_device())\n",
        "            x = blk(x)\n",
        "            # _, _, H, W = x.size()\n",
        "            # x = x.view(-1, self.embed_dim, H*W)\n",
        "            # x = x.permute(0,2,1)\n",
        "        output = self.deconv(x.permute(0, 2, 1).view(-1, self.embed_dim//16, H, W))\n",
        "        return output\n",
        "\n",
        "\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=leakyrelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gain = np.sqrt(0.5) if norm_layer == \"none\" else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x*self.gain + self.drop_path(self.attn(self.norm1(x)))*self.gain\n",
        "        x = x*self.gain + self.drop_path(self.mlp(self.norm2(x)))*self.gain\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, args, img_size=32, patch_size=None, in_chans=3, num_classes=1, embed_dim=None, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = args.df_dim  \n",
        "        \n",
        "        depth = args.d_depth\n",
        "        self.args = args\n",
        "        patch_size = args.patch_size\n",
        "        norm_layer = args.d_norm\n",
        "        act_layer = args.d_act\n",
        "        mlp_ratio = args.d_mlp\n",
        "        if hybrid_backbone is not None:\n",
        "            self.patch_embed = HybridEmbed(\n",
        "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        else:\n",
        "            self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        num_patches = (args.img_size // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim, \n",
        "                num_heads=num_heads, \n",
        "                mlp_ratio=mlp_ratio, \n",
        "                qkv_bias=qkv_bias, \n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate, \n",
        "                attn_drop=attn_drop_rate, \n",
        "                drop_path=dpr[i],\n",
        "                act_layer=act_layer,\n",
        "                norm_layer=norm_layer\n",
        "            )\n",
        "            for i in range(depth)])\n",
        "        \n",
        "        self.norm = CustomNorm(norm_layer, embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Conv2d) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        if \"None\" not in self.args.diff_aug:\n",
        "            x = DiffAugment(x, self.args.diff_aug, True)\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x).flatten(2).permute(0,2,1)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LYKbqk4wImT"
      },
      "source": [
        "##ViT_helper.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAiahFbgwMFk"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from itertools import repeat\n",
        "from torch._six import container_abcs\n",
        "'''\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "# From PyTorch internals\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, container_abcs.Iterable):\n",
        "            return x\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse\n",
        "\n",
        "\n",
        "to_1tuple = _ntuple(1)\n",
        "to_2tuple = _ntuple(2)\n",
        "to_3tuple = _ntuple(3)\n",
        "to_4tuple = _ntuple(4)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwTdRk4uwPNs"
      },
      "source": [
        "##ViT_scale3_local_new_rp.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SexNemcfwS9k"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from models_search.ViT_helper import DropPath, to_2tuple, trunc_normal_\n",
        "from models_search.diff_aug import DiffAugment\n",
        "from utils.utils import make_grid, save_image\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from models_search.ada import *\n",
        "'''\n",
        "import scipy.signal\n",
        "\n",
        "\n",
        "wavelets = {\n",
        "    'haar': [0.7071067811865476, 0.7071067811865476],\n",
        "    'db1':  [0.7071067811865476, 0.7071067811865476],\n",
        "    'db2':  [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],\n",
        "    'db5':  [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125],\n",
        "    'db6':  [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017],\n",
        "    'db7':  [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236],\n",
        "    'db8':  [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161],\n",
        "    'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n",
        "    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],\n",
        "    'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148],\n",
        "    'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255],\n",
        "    'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609],\n",
        "}\n",
        "\n",
        "class matmul(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        x = x1@x2\n",
        "        return x\n",
        "\n",
        "def count_matmul(m, x, y):\n",
        "    num_mul = x[0].numel() * x[1].size(-1)\n",
        "    # m.total_ops += torch.DoubleTensor([int(num_mul)])\n",
        "    m.total_ops += torch.DoubleTensor([int(0)])\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=2, keepdim=True) + 1e-8)\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\" Original Implementation of the gelu activation function in Google Bert repo when initialy created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "def leakyrelu(x):\n",
        "    return nn.functional.leaky_relu_(x, 0.2)\n",
        "\n",
        "class CustomAct(nn.Module):\n",
        "    def __init__(self, act_layer):\n",
        "        super().__init__()\n",
        "        if act_layer == \"gelu\":\n",
        "            self.act_layer = gelu\n",
        "        elif act_layer == \"leakyrelu\":\n",
        "            self.act_layer = leakyrelu\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.act_layer(x)\n",
        "        \n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=gelu, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = CustomAct(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., window_size=16):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.mat = matmul()\n",
        "        self.window_size = window_size\n",
        "        if self.window_size != 0:\n",
        "            self.relative_position_bias_table = nn.Parameter(\n",
        "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "            # get pair-wise relative position index for each token inside the window\n",
        "            coords_h = torch.arange(window_size)\n",
        "            coords_w = torch.arange(window_size)\n",
        "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "            relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "            relative_coords[:, :, 1] += window_size - 1\n",
        "            relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (self.mat(q, k.transpose(-2, -1))) * self.scale\n",
        "        if self.window_size != 0:\n",
        "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1).clone()].view(\n",
        "                self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "            attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = self.mat(attn, v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class CustomNorm(nn.Module):\n",
        "    def __init__(self, norm_layer, dim):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_layer\n",
        "        if norm_layer == \"ln\":\n",
        "            self.norm = nn.LayerNorm(dim)\n",
        "        elif norm_layer == \"bn\":\n",
        "            self.norm = nn.BatchNorm1d(dim)\n",
        "        elif norm_layer == \"in\":\n",
        "            self.norm = nn.InstanceNorm1d(dim)\n",
        "        elif norm_layer == \"pn\":\n",
        "            self.norm = PixelNorm(dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.norm_type == \"bn\" or self.norm_type == \"in\":\n",
        "            x = self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
        "            return x\n",
        "        elif self.norm_type == \"none\":\n",
        "            return x\n",
        "        else:\n",
        "            return self.norm(x)\n",
        "        \n",
        "        \n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "    \n",
        "class StageBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, depth, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.depth = depth\n",
        "        self.block = nn.ModuleList([\n",
        "                        Block(\n",
        "                        dim=dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop, \n",
        "                        attn_drop=attn_drop, \n",
        "                        drop_path=drop_path, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        ) for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.block:\n",
        "            x = blk(x)\n",
        "        return x\n",
        "\n",
        "def pixel_upsample(x, H, W):\n",
        "    B, N, C = x.size()\n",
        "    assert N == H*W\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = x.view(-1, C, H, W)\n",
        "    x = nn.PixelShuffle(2)(x)\n",
        "    B, C, H, W = x.size()\n",
        "    x = x.view(-1, C, H*W)\n",
        "    x = x.permute(0,2,1)\n",
        "    return x, H, W\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, args, img_size=224, patch_size=16, in_chans=3, num_classes=10, embed_dim=384, depth=5,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super(Generator, self).__init__()\n",
        "        self.args = args\n",
        "        self.ch = embed_dim\n",
        "        self.bottom_width = args.bottom_width\n",
        "        self.embed_dim = embed_dim = args.gf_dim\n",
        "        norm_layer = args.g_norm\n",
        "        depth = [int(i) for i in args.g_depth.split(\",\")]\n",
        "        act_layer = args.g_act\n",
        "        \n",
        "        self.l1 = nn.Linear(args.latent_dim, (self.bottom_width ** 2) * self.embed_dim)\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, self.bottom_width**2, embed_dim))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, (self.bottom_width*2)**2, embed_dim//4))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, (self.bottom_width*4)**2, embed_dim//16))\n",
        "        self.pos_embed = [\n",
        "            self.pos_embed_1,\n",
        "            self.pos_embed_2,\n",
        "            self.pos_embed_3\n",
        "        ]\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  # stochastic depth decay rule\n",
        "        self.blocks = StageBlock(\n",
        "                        depth=depth[0],\n",
        "                        dim=embed_dim, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        )\n",
        "        self.upsample_blocks = nn.ModuleList([\n",
        "                    StageBlock(\n",
        "                        depth=depth[1],\n",
        "                        dim=embed_dim//4, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0, \n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        ),\n",
        "                    StageBlock(\n",
        "                        depth=depth[2],\n",
        "                        dim=embed_dim//16, \n",
        "                        num_heads=num_heads, \n",
        "                        mlp_ratio=mlp_ratio, \n",
        "                        qkv_bias=qkv_bias, \n",
        "                        qk_scale=qk_scale,\n",
        "                        drop=drop_rate, \n",
        "                        attn_drop=attn_drop_rate, \n",
        "                        drop_path=0,\n",
        "                        act_layer=act_layer,\n",
        "                        norm_layer=norm_layer\n",
        "                        )\n",
        "                    ])\n",
        "        for i in range(len(self.pos_embed)):\n",
        "            trunc_normal_(self.pos_embed[i], std=.02)\n",
        "\n",
        "        self.tRGB_1 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim, 3, 1, 1, 0)\n",
        "        )\n",
        "        self.tRGB_2 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//4, 3, 1, 1, 0)\n",
        "        )\n",
        "        self.tRGB_3 = nn.Sequential(\n",
        "            nn.Conv2d(self.embed_dim//16, 3, 1, 1, 0)\n",
        "        )\n",
        "    \n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.BatchNorm1d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.InstanceNorm1d):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "            \n",
        "    def set_arch(self, x, cur_stage):\n",
        "        pass\n",
        "\n",
        "    def forward(self, z, epoch):\n",
        "        x = self.l1(z).view(-1, self.bottom_width ** 2, self.embed_dim)\n",
        "        x = x + self.pos_embed[0].to(x.get_device())\n",
        "        B = x.size(0)\n",
        "        H, W = self.bottom_width, self.bottom_width\n",
        "        x = self.blocks(x)\n",
        "        \n",
        "        x_1 = self.tRGB_1(x.permute(0,2,1).view(B,self.embed_dim,H,W))\n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[1].to(x.get_device())\n",
        "        x = self.upsample_blocks[0](x)\n",
        "        \n",
        "        x_2 = self.tRGB_2(x.permute(0,2,1).view(B,self.embed_dim//4,H,W))\n",
        "        x, H, W = pixel_upsample(x, H, W)\n",
        "        x = x + self.pos_embed[2].to(x.get_device())\n",
        "        x = self.upsample_blocks[1](x)\n",
        "\n",
        "        x_3 = self.tRGB_3(x.permute(0,2,1).view(B,self.embed_dim//16,H,W))\n",
        "        output = F.interpolate(x_1, scale_factor=4) + F.interpolate(x_2, scale_factor=2) + x_3\n",
        "        return output\n",
        "\n",
        "\n",
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return nn.AvgPool2d(kernel_size=2)(x)\n",
        "\n",
        "class SpaceToDepth(nn.Module):\n",
        "    def __init__(self, block_size=2):\n",
        "        super().__init__()\n",
        "        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        S = self.block_size\n",
        "        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\n",
        "        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n",
        "        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\n",
        "        return x\n",
        "    \n",
        "class DisBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=leakyrelu, norm_layer=nn.LayerNorm, window_size=16):\n",
        "        super().__init__()\n",
        "        self.norm1 = CustomNorm(norm_layer, dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = CustomNorm(norm_layer, dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "        self.gain = np.sqrt(0.5) if norm_layer == \"none\" else 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x*self.gain + self.drop_path(self.attn(self.norm1(x)))*self.gain\n",
        "        x = x*self.gain + self.drop_path(self.mlp(self.norm2(x)))*self.gain\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, img_size=32, patch_size=None, in_chans=3, num_classes=1, embed_dim=None, depth=7,\n",
        "                 num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim = self.embed_dim = args.df_dim  \n",
        "        \n",
        "        depth = args.d_depth\n",
        "        self.args = args\n",
        "        self.patch_size = patch_size = args.patch_size\n",
        "        norm_layer = args.d_norm\n",
        "        self.window_size = args.d_window_size\n",
        "        \n",
        "        act_layer = args.d_act\n",
        "        self.fRGB_1 = nn.Conv2d(3, embed_dim//4*3, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        self.fRGB_2 = nn.Conv2d(3, embed_dim//8, kernel_size=patch_size*2, stride=patch_size*2, padding=0)\n",
        "        self.fRGB_3 = nn.Conv2d(3, embed_dim//8, kernel_size=patch_size*4, stride=patch_size*4, padding=0)\n",
        "#         self.fRGB_4 = nn.Conv2d(3, embed_dim//2, kernel_size=patch_size, stride=patch_size, padding=0)\n",
        "        \n",
        "        num_patches_1 = (args.img_size // patch_size)**2\n",
        "        num_patches_2 = ((args.img_size//2) // patch_size)**2\n",
        "        num_patches_3 = ((args.img_size//4) // patch_size)**2\n",
        "#         num_patches_4 = ((args.img_size//8) // patch_size)**2\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed_1 = nn.Parameter(torch.zeros(1, num_patches_1, embed_dim//4*3))\n",
        "        self.pos_embed_2 = nn.Parameter(torch.zeros(1, num_patches_2, embed_dim//8*7))\n",
        "        self.pos_embed_3 = nn.Parameter(torch.zeros(1, num_patches_3, embed_dim))\n",
        "#         self.pos_embed_4 = nn.Parameter(torch.zeros(1, num_patches_4, embed_dim))\n",
        "        \n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        \n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks_1 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//4*3, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=self.window_size)\n",
        "            for i in range(depth-1)])\n",
        "        self.blocks_11 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//4*3, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=64)\n",
        "            for i in range(1)])\n",
        "        self.blocks_2 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim//8*7, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=32)\n",
        "            for i in range(depth)])\n",
        "        self.blocks_3 = nn.ModuleList([\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer, window_size=16)\n",
        "            for i in range(depth+1)])\n",
        "#         self.blocks_4 = nn.ModuleList([\n",
        "#             DisBlock(\n",
        "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0, act_layer=act_layer, norm_layer=norm_layer)\n",
        "#             for i in range(depth)])\n",
        "        self.last_block = nn.Sequential(\n",
        "#             Block(\n",
        "#                 dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#                 drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], norm_layer=norm_layer),\n",
        "            DisBlock(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[0], act_layer=act_layer, norm_layer=norm_layer, window_size=0)\n",
        "            )\n",
        "        \n",
        "        self.norm = CustomNorm(norm_layer, embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed_1, std=.02)\n",
        "        trunc_normal_(self.pos_embed_2, std=.02)\n",
        "        trunc_normal_(self.pos_embed_3, std=.02)\n",
        "#         trunc_normal_(self.pos_embed_4, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "        if 'filter' in self.args.diff_aug:\n",
        "            Hz_lo = np.asarray(wavelets['sym2'])            # H(z)\n",
        "            Hz_hi = Hz_lo * ((-1) ** np.arange(Hz_lo.size)) # H(-z)\n",
        "            Hz_lo2 = np.convolve(Hz_lo, Hz_lo[::-1]) / 2    # H(z) * H(z^-1) / 2\n",
        "            Hz_hi2 = np.convolve(Hz_hi, Hz_hi[::-1]) / 2    # H(-z) * H(-z^-1) / 2\n",
        "            Hz_fbank = np.eye(4, 1)                         # Bandpass(H(z), b_i)\n",
        "            for i in range(1, Hz_fbank.shape[0]):\n",
        "                Hz_fbank = np.dstack([Hz_fbank, np.zeros_like(Hz_fbank)]).reshape(Hz_fbank.shape[0], -1)[:, :-1]\n",
        "                Hz_fbank = scipy.signal.convolve(Hz_fbank, [Hz_lo2])\n",
        "                Hz_fbank[i, (Hz_fbank.shape[1] - Hz_hi2.size) // 2 : (Hz_fbank.shape[1] + Hz_hi2.size) // 2] += Hz_hi2\n",
        "            Hz_fbank = torch.as_tensor(Hz_fbank, dtype=torch.float32)\n",
        "            self.register_buffer('Hz_fbank', torch.as_tensor(Hz_fbank, dtype=torch.float32))\n",
        "        else:\n",
        "            self.Hz_fbank = None\n",
        "        if 'geo' in self.args.diff_aug:\n",
        "            self.register_buffer('Hz_geom', upfirdn2d.setup_filter(wavelets['sym6']))\n",
        "        else:\n",
        "            self.Hz_geom = None\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "            \n",
        "    def forward_features(self, x):\n",
        "        if \"None\" not in self.args.diff_aug:\n",
        "            x = DiffAugment(x, self.args.diff_aug, True, [self.Hz_geom, self.Hz_fbank])\n",
        "#         with torch.no_grad():\n",
        "#             save_image(x.clone(), f'in_{self.args.rank}.png', nrow=4, padding=1, normalize=True, scale_each=True)\n",
        "#         import time\n",
        "#         time.sleep(10)\n",
        "        B, _, H, W = x.size()\n",
        "        H = W = H//self.patch_size\n",
        "        \n",
        "        x_1 = self.fRGB_1(x).flatten(2).permute(0,2,1)\n",
        "        x_2 = self.fRGB_2(x).flatten(2).permute(0,2,1)\n",
        "        x_3 = self.fRGB_3(x).flatten(2).permute(0,2,1)\n",
        "#         x_4 = self.fRGB_4(nn.AvgPool2d(8)(x)).flatten(2).permute(0,2,1)\n",
        "        B = x.shape[0]\n",
        "        \n",
        "\n",
        "        x = x_1 + self.pos_embed_1\n",
        "        B, _, C = x.size()\n",
        "        x = x.view(B, H, W, C)\n",
        "        x = window_partition(x, self.window_size)\n",
        "        x = x.view(-1, self.window_size*self.window_size, C)\n",
        "        for blk in self.blocks_1:\n",
        "            x = blk(x)\n",
        "        x = x.view(-1, self.window_size, self.window_size, C)\n",
        "        x = window_reverse(x, self.window_size, H, W).view(B,H*W,C)\n",
        "        for blk in self.blocks_11:\n",
        "            x = blk(x)\n",
        "            \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_2], dim=-1)\n",
        "        x = x + self.pos_embed_2\n",
        "        \n",
        "        for blk in self.blocks_2:\n",
        "            x = blk(x)\n",
        "        \n",
        "        _, _, C = x.shape\n",
        "        x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "#         x = SpaceToDepth(2)(x)\n",
        "        x = nn.AvgPool2d(2)(x)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.flatten(2).permute(0, 2, 1)\n",
        "        x = torch.cat([x, x_3], dim=-1)\n",
        "        x = x + self.pos_embed_3\n",
        "        \n",
        "        for blk in self.blocks_3:\n",
        "            x = blk(x)\n",
        "            \n",
        "#         _, _, C = x.shape\n",
        "#         x = x.permute(0, 2, 1).view(B, C, H, W)\n",
        "# #         x = SpaceToDepth(2)(x)\n",
        "#         x = nn.AvgPool2d(2)(x)\n",
        "#         _, _, H, W = x.shape\n",
        "#         x = x.flatten(2).permute(0, 2, 1)\n",
        "#         x = torch.cat([x, x_4], dim=-1)\n",
        "#         x = x + self.pos_embed_4\n",
        "        \n",
        "#         for blk in self.blocks_4:\n",
        "#             x = blk(x)\n",
        "            \n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.last_block(x)\n",
        "        x = self.norm(x)\n",
        "        return x[:,0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqSyN_bswbBs"
      },
      "source": [
        "#torch_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BM1Y8bAwkyU"
      },
      "source": [
        "##custom_ops.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn23KJOswi4E",
        "outputId": "ec1d9400-d090-423f-cc3a-3e0bea3e04c4"
      },
      "source": [
        "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
        "# and proprietary rights in and to this software, related documentation\n",
        "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
        "# distribution of this software and related documentation without an express\n",
        "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.utils.cpp_extension\n",
        "import importlib\n",
        "import hashlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.file_baton import FileBaton\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Global options.\n",
        "\n",
        "verbosity = 'brief' # Verbosity level: 'none', 'brief', 'full'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Internal helper funcs.\n",
        "\n",
        "def _find_compiler_bindir():\n",
        "    patterns = [\n",
        "        'C:/Program Files (x86)/Microsoft Visual Studio/*/Professional/VC/Tools/MSVC/*/bin/Hostx64/x64',\n",
        "        'C:/Program Files (x86)/Microsoft Visual Studio/*/BuildTools/VC/Tools/MSVC/*/bin/Hostx64/x64',\n",
        "        'C:/Program Files (x86)/Microsoft Visual Studio/*/Community/VC/Tools/MSVC/*/bin/Hostx64/x64',\n",
        "        'C:/Program Files (x86)/Microsoft Visual Studio */vc/bin',\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        matches = sorted(glob.glob(pattern))\n",
        "        if len(matches):\n",
        "            return matches[-1]\n",
        "    return None\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Main entry point for compiling and loading C++/CUDA plugins.\n",
        "\n",
        "_cached_plugins = dict()\n",
        "\n",
        "def get_plugin(module_name, sources, **build_kwargs):\n",
        "    assert verbosity in ['none', 'brief', 'full']\n",
        "\n",
        "    # Already cached?\n",
        "    if module_name in _cached_plugins:\n",
        "        return _cached_plugins[module_name]\n",
        "\n",
        "    # Print status.\n",
        "    if verbosity == 'full':\n",
        "        print(f'Setting up PyTorch plugin \"{module_name}\"...')\n",
        "    elif verbosity == 'brief':\n",
        "        print(f'Setting up PyTorch plugin \"{module_name}\"... ', end='', flush=True)\n",
        "\n",
        "    try: # pylint: disable=too-many-nested-blocks\n",
        "        # Make sure we can find the necessary compiler binaries.\n",
        "        if os.name == 'nt' and os.system(\"where cl.exe >nul 2>nul\") != 0:\n",
        "            compiler_bindir = _find_compiler_bindir()\n",
        "            if compiler_bindir is None:\n",
        "                raise RuntimeError(f'Could not find MSVC/GCC/CLANG installation on this computer. Check _find_compiler_bindir() in \"{__file__}\".')\n",
        "            os.environ['PATH'] += ';' + compiler_bindir\n",
        "\n",
        "        # Compile and load.\n",
        "        verbose_build = (verbosity == 'full')\n",
        "\n",
        "        # Incremental build md5sum trickery.  Copies all the input source files\n",
        "        # into a cached build directory under a combined md5 digest of the input\n",
        "        # source files.  Copying is done only if the combined digest has changed.\n",
        "        # This keeps input file timestamps and filenames the same as in previous\n",
        "        # extension builds, allowing for fast incremental rebuilds.\n",
        "        #\n",
        "        # This optimization is done only in case all the source files reside in\n",
        "        # a single directory (just for simplicity) and if the TORCH_EXTENSIONS_DIR\n",
        "        # environment variable is set (we take this as a signal that the user\n",
        "        # actually cares about this.)\n",
        "        source_dirs_set = set(os.path.dirname(source) for source in sources)\n",
        "        if len(source_dirs_set) == 1 and ('TORCH_EXTENSIONS_DIR' in os.environ):\n",
        "            all_source_files = sorted(list(x for x in Path(list(source_dirs_set)[0]).iterdir() if x.is_file()))\n",
        "\n",
        "            # Compute a combined hash digest for all source files in the same\n",
        "            # custom op directory (usually .cu, .cpp, .py and .h files).\n",
        "            hash_md5 = hashlib.md5()\n",
        "            for src in all_source_files:\n",
        "                with open(src, 'rb') as f:\n",
        "                    hash_md5.update(f.read())\n",
        "            build_dir = torch.utils.cpp_extension._get_build_directory(module_name, verbose=verbose_build) # pylint: disable=protected-access\n",
        "            digest_build_dir = os.path.join(build_dir, hash_md5.hexdigest())\n",
        "\n",
        "            if not os.path.isdir(digest_build_dir):\n",
        "                os.makedirs(digest_build_dir, exist_ok=True)\n",
        "                baton = FileBaton(os.path.join(digest_build_dir, 'lock'))\n",
        "                if baton.try_acquire():\n",
        "                    try:\n",
        "                        for src in all_source_files:\n",
        "                            shutil.copyfile(src, os.path.join(digest_build_dir, os.path.basename(src)))\n",
        "                    finally:\n",
        "                        baton.release()\n",
        "                else:\n",
        "                    # Someone else is copying source files under the digest dir,\n",
        "                    # wait until done and continue.\n",
        "                    baton.wait()\n",
        "            digest_sources = [os.path.join(digest_build_dir, os.path.basename(x)) for x in sources]\n",
        "            torch.utils.cpp_extension.load(name=module_name, build_directory=build_dir,\n",
        "                verbose=verbose_build, sources=digest_sources, **build_kwargs)\n",
        "        else:\n",
        "            torch.utils.cpp_extension.load(name=module_name, verbose=verbose_build, sources=sources, **build_kwargs)\n",
        "        module = importlib.import_module(module_name)\n",
        "\n",
        "    except:\n",
        "        if verbosity == 'brief':\n",
        "            print('Failed!')\n",
        "        raise\n",
        "\n",
        "    # Print status and add to cache.\n",
        "    if verbosity == 'full':\n",
        "        print(f'Done setting up PyTorch plugin \"{module_name}\".')\n",
        "    elif verbosity == 'brief':\n",
        "        print('Done.')\n",
        "    _cached_plugins[module_name] = module\n",
        "    return module\n",
        "\n",
        "#----------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJF0QKRMw6o8"
      },
      "source": [
        "##misc.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBl5dtM2w_Vk"
      },
      "source": [
        "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
        "# and proprietary rights in and to this software, related documentation\n",
        "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
        "# distribution of this software and related documentation without an express\n",
        "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
        "\n",
        "import re\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "import dnnlib\n",
        "'''\n",
        "#----------------------------------------------------------------------------\n",
        "# Cached construction of constant tensors. Avoids CPU=>GPU copy when the\n",
        "# same constant is used multiple times.\n",
        "\n",
        "_constant_cache = dict()\n",
        "\n",
        "def constant(value, shape=None, dtype=None, device=None, memory_format=None):\n",
        "    value = np.asarray(value)\n",
        "    if shape is not None:\n",
        "        shape = tuple(shape)\n",
        "    if dtype is None:\n",
        "        dtype = torch.get_default_dtype()\n",
        "    if device is None:\n",
        "        device = torch.device('cpu')\n",
        "    if memory_format is None:\n",
        "        memory_format = torch.contiguous_format\n",
        "\n",
        "    key = (value.shape, value.dtype, value.tobytes(), shape, dtype, device, memory_format)\n",
        "    tensor = _constant_cache.get(key, None)\n",
        "    if tensor is None:\n",
        "        tensor = torch.as_tensor(value.copy(), dtype=dtype, device=device)\n",
        "        if shape is not None:\n",
        "            tensor, _ = torch.broadcast_tensors(tensor, torch.empty(shape))\n",
        "        tensor = tensor.contiguous(memory_format=memory_format)\n",
        "        _constant_cache[key] = tensor\n",
        "    return tensor\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Replace NaN/Inf with specified numerical values.\n",
        "\n",
        "try:\n",
        "    nan_to_num = torch.nan_to_num # 1.8.0a0\n",
        "except AttributeError:\n",
        "    def nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None): # pylint: disable=redefined-builtin\n",
        "        assert isinstance(input, torch.Tensor)\n",
        "        if posinf is None:\n",
        "            posinf = torch.finfo(input.dtype).max\n",
        "        if neginf is None:\n",
        "            neginf = torch.finfo(input.dtype).min\n",
        "        assert nan == 0\n",
        "        return torch.clamp(input.unsqueeze(0).nansum(0), min=neginf, max=posinf, out=out)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Symbolic assert.\n",
        "\n",
        "try:\n",
        "    symbolic_assert = torch._assert # 1.8.0a0 # pylint: disable=protected-access\n",
        "except AttributeError:\n",
        "    symbolic_assert = torch.Assert # 1.7.0\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Context manager to suppress known warnings in torch.jit.trace().\n",
        "\n",
        "class suppress_tracer_warnings(warnings.catch_warnings):\n",
        "    def __enter__(self):\n",
        "        super().__enter__()\n",
        "        warnings.simplefilter('ignore', category=torch.jit.TracerWarning)\n",
        "        return self\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Assert that the shape of a tensor matches the given list of integers.\n",
        "# None indicates that the size of a dimension is allowed to vary.\n",
        "# Performs symbolic assertion when used in torch.jit.trace().\n",
        "\n",
        "def assert_shape(tensor, ref_shape):\n",
        "    if tensor.ndim != len(ref_shape):\n",
        "        raise AssertionError(f'Wrong number of dimensions: got {tensor.ndim}, expected {len(ref_shape)}')\n",
        "    for idx, (size, ref_size) in enumerate(zip(tensor.shape, ref_shape)):\n",
        "        if ref_size is None:\n",
        "            pass\n",
        "        elif isinstance(ref_size, torch.Tensor):\n",
        "            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n",
        "                symbolic_assert(torch.equal(torch.as_tensor(size), ref_size), f'Wrong size for dimension {idx}')\n",
        "        elif isinstance(size, torch.Tensor):\n",
        "            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n",
        "                symbolic_assert(torch.equal(size, torch.as_tensor(ref_size)), f'Wrong size for dimension {idx}: expected {ref_size}')\n",
        "        elif size != ref_size:\n",
        "            raise AssertionError(f'Wrong size for dimension {idx}: got {size}, expected {ref_size}')\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Function decorator that calls torch.autograd.profiler.record_function().\n",
        "\n",
        "def profiled_function(fn):\n",
        "    def decorator(*args, **kwargs):\n",
        "        with torch.autograd.profiler.record_function(fn.__name__):\n",
        "            return fn(*args, **kwargs)\n",
        "    decorator.__name__ = fn.__name__\n",
        "    return decorator\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Sampler for torch.utils.data.DataLoader that loops over the dataset\n",
        "# indefinitely, shuffling items as it goes.\n",
        "\n",
        "class InfiniteSampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, dataset, rank=0, num_replicas=1, shuffle=True, seed=0, window_size=0.5):\n",
        "        assert len(dataset) > 0\n",
        "        assert num_replicas > 0\n",
        "        assert 0 <= rank < num_replicas\n",
        "        assert 0 <= window_size <= 1\n",
        "        super().__init__(dataset)\n",
        "        self.dataset = dataset\n",
        "        self.rank = rank\n",
        "        self.num_replicas = num_replicas\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        order = np.arange(len(self.dataset))\n",
        "        rnd = None\n",
        "        window = 0\n",
        "        if self.shuffle:\n",
        "            rnd = np.random.RandomState(self.seed)\n",
        "            rnd.shuffle(order)\n",
        "            window = int(np.rint(order.size * self.window_size))\n",
        "\n",
        "        idx = 0\n",
        "        while True:\n",
        "            i = idx % order.size\n",
        "            if idx % self.num_replicas == self.rank:\n",
        "                yield order[i]\n",
        "            if window >= 2:\n",
        "                j = (i - rnd.randint(window)) % order.size\n",
        "                order[i], order[j] = order[j], order[i]\n",
        "            idx += 1\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Utilities for operating with torch.nn.Module parameters and buffers.\n",
        "\n",
        "def params_and_buffers(module):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    return list(module.parameters()) + list(module.buffers())\n",
        "\n",
        "def named_params_and_buffers(module):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    return list(module.named_parameters()) + list(module.named_buffers())\n",
        "\n",
        "def copy_params_and_buffers(src_module, dst_module, require_all=False):\n",
        "    assert isinstance(src_module, torch.nn.Module)\n",
        "    assert isinstance(dst_module, torch.nn.Module)\n",
        "    src_tensors = {name: tensor for name, tensor in named_params_and_buffers(src_module)}\n",
        "    for name, tensor in named_params_and_buffers(dst_module):\n",
        "        assert (name in src_tensors) or (not require_all)\n",
        "        if name in src_tensors:\n",
        "            tensor.copy_(src_tensors[name].detach()).requires_grad_(tensor.requires_grad)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Context manager for easily enabling/disabling DistributedDataParallel\n",
        "# synchronization.\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def ddp_sync(module, sync):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    if sync or not isinstance(module, torch.nn.parallel.DistributedDataParallel):\n",
        "        yield\n",
        "    else:\n",
        "        with module.no_sync():\n",
        "            yield\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Check DistributedDataParallel consistency across processes.\n",
        "\n",
        "def check_ddp_consistency(module, ignore_regex=None):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    for name, tensor in named_params_and_buffers(module):\n",
        "        fullname = type(module).__name__ + '.' + name\n",
        "        if ignore_regex is not None and re.fullmatch(ignore_regex, fullname):\n",
        "            continue\n",
        "        tensor = tensor.detach()\n",
        "        other = tensor.clone()\n",
        "        torch.distributed.broadcast(tensor=other, src=0)\n",
        "        assert (nan_to_num(tensor) == nan_to_num(other)).all(), fullname\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Print summary table of module hierarchy.\n",
        "\n",
        "def print_module_summary(module, inputs, max_nesting=3, skip_redundant=True):\n",
        "    assert isinstance(module, torch.nn.Module)\n",
        "    assert not isinstance(module, torch.jit.ScriptModule)\n",
        "    assert isinstance(inputs, (tuple, list))\n",
        "\n",
        "    # Register hooks.\n",
        "    entries = []\n",
        "    nesting = [0]\n",
        "    def pre_hook(_mod, _inputs):\n",
        "        nesting[0] += 1\n",
        "    def post_hook(mod, _inputs, outputs):\n",
        "        nesting[0] -= 1\n",
        "        if nesting[0] <= max_nesting:\n",
        "            outputs = list(outputs) if isinstance(outputs, (tuple, list)) else [outputs]\n",
        "            outputs = [t for t in outputs if isinstance(t, torch.Tensor)]\n",
        "            entries.append(dnnlib.EasyDict(mod=mod, outputs=outputs))\n",
        "    hooks = [mod.register_forward_pre_hook(pre_hook) for mod in module.modules()]\n",
        "    hooks += [mod.register_forward_hook(post_hook) for mod in module.modules()]\n",
        "\n",
        "    # Run module.\n",
        "    outputs = module(*inputs)\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    # Identify unique outputs, parameters, and buffers.\n",
        "    tensors_seen = set()\n",
        "    for e in entries:\n",
        "        e.unique_params = [t for t in e.mod.parameters() if id(t) not in tensors_seen]\n",
        "        e.unique_buffers = [t for t in e.mod.buffers() if id(t) not in tensors_seen]\n",
        "        e.unique_outputs = [t for t in e.outputs if id(t) not in tensors_seen]\n",
        "        tensors_seen |= {id(t) for t in e.unique_params + e.unique_buffers + e.unique_outputs}\n",
        "\n",
        "    # Filter out redundant entries.\n",
        "    if skip_redundant:\n",
        "        entries = [e for e in entries if len(e.unique_params) or len(e.unique_buffers) or len(e.unique_outputs)]\n",
        "\n",
        "    # Construct table.\n",
        "    rows = [[type(module).__name__, 'Parameters', 'Buffers', 'Output shape', 'Datatype']]\n",
        "    rows += [['---'] * len(rows[0])]\n",
        "    param_total = 0\n",
        "    buffer_total = 0\n",
        "    submodule_names = {mod: name for name, mod in module.named_modules()}\n",
        "    for e in entries:\n",
        "        name = '<top-level>' if e.mod is module else submodule_names[e.mod]\n",
        "        param_size = sum(t.numel() for t in e.unique_params)\n",
        "        buffer_size = sum(t.numel() for t in e.unique_buffers)\n",
        "        output_shapes = [str(list(e.outputs[0].shape)) for t in e.outputs]\n",
        "        output_dtypes = [str(t.dtype).split('.')[-1] for t in e.outputs]\n",
        "        rows += [[\n",
        "            name + (':0' if len(e.outputs) >= 2 else ''),\n",
        "            str(param_size) if param_size else '-',\n",
        "            str(buffer_size) if buffer_size else '-',\n",
        "            (output_shapes + ['-'])[0],\n",
        "            (output_dtypes + ['-'])[0],\n",
        "        ]]\n",
        "        for idx in range(1, len(e.outputs)):\n",
        "            rows += [[name + f':{idx}', '-', '-', output_shapes[idx], output_dtypes[idx]]]\n",
        "        param_total += param_size\n",
        "        buffer_total += buffer_size\n",
        "    rows += [['---'] * len(rows[0])]\n",
        "    rows += [['Total', str(param_total), str(buffer_total), '-', '-']]\n",
        "\n",
        "    # Print table.\n",
        "    widths = [max(len(cell) for cell in column) for column in zip(*rows)]\n",
        "    print()\n",
        "    for row in rows:\n",
        "        print('  '.join(cell + ' ' * (width - len(cell)) for cell, width in zip(row, widths)))\n",
        "    print()\n",
        "    return outputs\n",
        "\n",
        "#----------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV7CvH5vxAKc"
      },
      "source": [
        "##persistence.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC5kBy8bxDhc"
      },
      "source": [
        "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
        "# and proprietary rights in and to this software, related documentation\n",
        "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
        "# distribution of this software and related documentation without an express\n",
        "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
        "\n",
        "\"\"\"Facilities for pickling Python code alongside other data.\n",
        "\n",
        "The pickled code is automatically imported into a separate Python module\n",
        "during unpickling. This way, any previously exported pickles will remain\n",
        "usable even if the original code is no longer available, or if the current\n",
        "version of the code is not consistent with what was originally pickled.\"\"\"\n",
        "\n",
        "import sys\n",
        "import pickle\n",
        "import io\n",
        "import inspect\n",
        "import copy\n",
        "import uuid\n",
        "import types\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "import dnnlib\n",
        "'''\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "_version            = 6         # internal version number\n",
        "_decorators         = set()     # {decorator_class, ...}\n",
        "_import_hooks       = []        # [hook_function, ...]\n",
        "_module_to_src_dict = dict()    # {module: src, ...}\n",
        "_src_to_module_dict = dict()    # {src: module, ...}\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def persistent_class(orig_class):\n",
        "    r\"\"\"Class decorator that extends a given class to save its source code\n",
        "    when pickled.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        from torch_utils import persistence\n",
        "\n",
        "        @persistence.persistent_class\n",
        "        class MyNetwork(torch.nn.Module):\n",
        "            def __init__(self, num_inputs, num_outputs):\n",
        "                super().__init__()\n",
        "                self.fc = MyLayer(num_inputs, num_outputs)\n",
        "                ...\n",
        "\n",
        "        @persistence.persistent_class\n",
        "        class MyLayer(torch.nn.Module):\n",
        "            ...\n",
        "\n",
        "    When pickled, any instance of `MyNetwork` and `MyLayer` will save its\n",
        "    source code alongside other internal state (e.g., parameters, buffers,\n",
        "    and submodules). This way, any previously exported pickle will remain\n",
        "    usable even if the class definitions have been modified or are no\n",
        "    longer available.\n",
        "\n",
        "    The decorator saves the source code of the entire Python module\n",
        "    containing the decorated class. It does *not* save the source code of\n",
        "    any imported modules. Thus, the imported modules must be available\n",
        "    during unpickling, also including `torch_utils.persistence` itself.\n",
        "\n",
        "    It is ok to call functions defined in the same module from the\n",
        "    decorated class. However, if the decorated class depends on other\n",
        "    classes defined in the same module, they must be decorated as well.\n",
        "    This is illustrated in the above example in the case of `MyLayer`.\n",
        "\n",
        "    It is also possible to employ the decorator just-in-time before\n",
        "    calling the constructor. For example:\n",
        "\n",
        "        cls = MyLayer\n",
        "        if want_to_make_it_persistent:\n",
        "            cls = persistence.persistent_class(cls)\n",
        "        layer = cls(num_inputs, num_outputs)\n",
        "\n",
        "    As an additional feature, the decorator also keeps track of the\n",
        "    arguments that were used to construct each instance of the decorated\n",
        "    class. The arguments can be queried via `obj.init_args` and\n",
        "    `obj.init_kwargs`, and they are automatically pickled alongside other\n",
        "    object state. A typical use case is to first unpickle a previous\n",
        "    instance of a persistent class, and then upgrade it to use the latest\n",
        "    version of the source code:\n",
        "\n",
        "        with open('old_pickle.pkl', 'rb') as f:\n",
        "            old_net = pickle.load(f)\n",
        "        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)\n",
        "        misc.copy_params_and_buffers(old_net, new_net, require_all=True)\n",
        "    \"\"\"\n",
        "    assert isinstance(orig_class, type)\n",
        "    if is_persistent(orig_class):\n",
        "        return orig_class\n",
        "\n",
        "    assert orig_class.__module__ in sys.modules\n",
        "    orig_module = sys.modules[orig_class.__module__]\n",
        "    orig_module_src = _module_to_src(orig_module)\n",
        "\n",
        "    class Decorator(orig_class):\n",
        "        _orig_module_src = orig_module_src\n",
        "        _orig_class_name = orig_class.__name__\n",
        "\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self._init_args = copy.deepcopy(args)\n",
        "            self._init_kwargs = copy.deepcopy(kwargs)\n",
        "            assert orig_class.__name__ in orig_module.__dict__\n",
        "            _check_pickleable(self.__reduce__())\n",
        "\n",
        "        @property\n",
        "        def init_args(self):\n",
        "            return copy.deepcopy(self._init_args)\n",
        "\n",
        "        @property\n",
        "        def init_kwargs(self):\n",
        "            return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))\n",
        "\n",
        "        def __reduce__(self):\n",
        "            fields = list(super().__reduce__())\n",
        "            fields += [None] * max(3 - len(fields), 0)\n",
        "            if fields[0] is not _reconstruct_persistent_obj:\n",
        "                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])\n",
        "                fields[0] = _reconstruct_persistent_obj # reconstruct func\n",
        "                fields[1] = (meta,) # reconstruct args\n",
        "                fields[2] = None # state dict\n",
        "            return tuple(fields)\n",
        "\n",
        "    Decorator.__name__ = orig_class.__name__\n",
        "    _decorators.add(Decorator)\n",
        "    return Decorator\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def is_persistent(obj):\n",
        "    r\"\"\"Test whether the given object or class is persistent, i.e.,\n",
        "    whether it will save its source code when pickled.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if obj in _decorators:\n",
        "            return True\n",
        "    except TypeError:\n",
        "        pass\n",
        "    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def import_hook(hook):\n",
        "    r\"\"\"Register an import hook that is called whenever a persistent object\n",
        "    is being unpickled. A typical use case is to patch the pickled source\n",
        "    code to avoid errors and inconsistencies when the API of some imported\n",
        "    module has changed.\n",
        "\n",
        "    The hook should have the following signature:\n",
        "\n",
        "        hook(meta) -> modified meta\n",
        "\n",
        "    `meta` is an instance of `dnnlib.EasyDict` with the following fields:\n",
        "\n",
        "        type:       Type of the persistent object, e.g. `'class'`.\n",
        "        version:    Internal version number of `torch_utils.persistence`.\n",
        "        module_src  Original source code of the Python module.\n",
        "        class_name: Class name in the original Python module.\n",
        "        state:      Internal state of the object.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        @persistence.import_hook\n",
        "        def wreck_my_network(meta):\n",
        "            if meta.class_name == 'MyNetwork':\n",
        "                print('MyNetwork is being imported. I will wreck it!')\n",
        "                meta.module_src = meta.module_src.replace(\"True\", \"False\")\n",
        "            return meta\n",
        "    \"\"\"\n",
        "    assert callable(hook)\n",
        "    _import_hooks.append(hook)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _reconstruct_persistent_obj(meta):\n",
        "    r\"\"\"Hook that is called internally by the `pickle` module to unpickle\n",
        "    a persistent object.\n",
        "    \"\"\"\n",
        "    meta = dnnlib.EasyDict(meta)\n",
        "    meta.state = dnnlib.EasyDict(meta.state)\n",
        "    for hook in _import_hooks:\n",
        "        meta = hook(meta)\n",
        "        assert meta is not None\n",
        "\n",
        "    assert meta.version == _version\n",
        "    module = _src_to_module(meta.module_src)\n",
        "\n",
        "    assert meta.type == 'class'\n",
        "    orig_class = module.__dict__[meta.class_name]\n",
        "    decorator_class = persistent_class(orig_class)\n",
        "    obj = decorator_class.__new__(decorator_class)\n",
        "\n",
        "    setstate = getattr(obj, '__setstate__', None)\n",
        "    if callable(setstate):\n",
        "        setstate(meta.state) # pylint: disable=not-callable\n",
        "    else:\n",
        "        obj.__dict__.update(meta.state)\n",
        "    return obj\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _module_to_src(module):\n",
        "    r\"\"\"Query the source code of a given Python module.\n",
        "    \"\"\"\n",
        "    src = _module_to_src_dict.get(module, None)\n",
        "    if src is None:\n",
        "        src = inspect.getsource(module)\n",
        "        _module_to_src_dict[module] = src\n",
        "        _src_to_module_dict[src] = module\n",
        "    return src\n",
        "\n",
        "def _src_to_module(src):\n",
        "    r\"\"\"Get or create a Python module for the given source code.\n",
        "    \"\"\"\n",
        "    module = _src_to_module_dict.get(src, None)\n",
        "    if module is None:\n",
        "        module_name = \"_imported_module_\" + uuid.uuid4().hex\n",
        "        module = types.ModuleType(module_name)\n",
        "        sys.modules[module_name] = module\n",
        "        _module_to_src_dict[module] = src\n",
        "        _src_to_module_dict[src] = module\n",
        "        exec(src, module.__dict__) # pylint: disable=exec-used\n",
        "    return module\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _check_pickleable(obj):\n",
        "    r\"\"\"Check that the given object is pickleable, raising an exception if\n",
        "    it is not. This function is expected to be considerably more efficient\n",
        "    than actually pickling the object.\n",
        "    \"\"\"\n",
        "    def recurse(obj):\n",
        "        if isinstance(obj, (list, tuple, set)):\n",
        "            return [recurse(x) for x in obj]\n",
        "        if isinstance(obj, dict):\n",
        "            return [[recurse(x), recurse(y)] for x, y in obj.items()]\n",
        "        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):\n",
        "            return None # Python primitive types are pickleable.\n",
        "        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor']:\n",
        "            return None # NumPy arrays and PyTorch tensors are pickleable.\n",
        "        if is_persistent(obj):\n",
        "            return None # Persistent objects are pickleable, by virtue of the constructor check.\n",
        "        return obj\n",
        "    with io.BytesIO() as f:\n",
        "        pickle.dump(recurse(obj), f)\n",
        "\n",
        "#----------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_PM6cxyxL3c"
      },
      "source": [
        "##training_stats.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkmHWKyAxP7E"
      },
      "source": [
        "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
        "# and proprietary rights in and to this software, related documentation\n",
        "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
        "# distribution of this software and related documentation without an express\n",
        "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
        "\n",
        "\"\"\"Facilities for reporting and collecting training statistics across\n",
        "multiple processes and devices. The interface is designed to minimize\n",
        "synchronization overhead as well as the amount of boilerplate in user\n",
        "code.\"\"\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "import dnnlib\n",
        "from . import misc\n",
        "'''\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "_num_moments    = 3             # [num_scalars, sum_of_scalars, sum_of_squares]\n",
        "_reduce_dtype   = torch.float32 # Data type to use for initial per-tensor reduction.\n",
        "_counter_dtype  = torch.float64 # Data type to use for the internal counters.\n",
        "_rank           = 0             # Rank of the current process.\n",
        "_sync_device    = None          # Device to use for multiprocess communication. None = single-process.\n",
        "_sync_called    = False         # Has _sync() been called yet?\n",
        "_counters       = dict()        # Running counters on each device, updated by report(): name => device => torch.Tensor\n",
        "_cumulative     = dict()        # Cumulative counters on the CPU, updated by _sync(): name => torch.Tensor\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def init_multiprocessing(rank, sync_device):\n",
        "    r\"\"\"Initializes `torch_utils.training_stats` for collecting statistics\n",
        "    across multiple processes.\n",
        "\n",
        "    This function must be called after\n",
        "    `torch.distributed.init_process_group()` and before `Collector.update()`.\n",
        "    The call is not necessary if multi-process collection is not needed.\n",
        "\n",
        "    Args:\n",
        "        rank:           Rank of the current process.\n",
        "        sync_device:    PyTorch device to use for inter-process\n",
        "                        communication, or None to disable multi-process\n",
        "                        collection. Typically `torch.device('cuda', rank)`.\n",
        "    \"\"\"\n",
        "    global _rank, _sync_device\n",
        "    assert not _sync_called\n",
        "    _rank = rank\n",
        "    _sync_device = sync_device\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@profiled_function\n",
        "def report(name, value):\n",
        "    r\"\"\"Broadcasts the given set of scalars to all interested instances of\n",
        "    `Collector`, across device and process boundaries.\n",
        "\n",
        "    This function is expected to be extremely cheap and can be safely\n",
        "    called from anywhere in the training loop, loss function, or inside a\n",
        "    `torch.nn.Module`.\n",
        "\n",
        "    Warning: The current implementation expects the set of unique names to\n",
        "    be consistent across processes. Please make sure that `report()` is\n",
        "    called at least once for each unique name by each process, and in the\n",
        "    same order. If a given process has no scalars to broadcast, it can do\n",
        "    `report(name, [])` (empty list).\n",
        "\n",
        "    Args:\n",
        "        name:   Arbitrary string specifying the name of the statistic.\n",
        "                Averages are accumulated separately for each unique name.\n",
        "        value:  Arbitrary set of scalars. Can be a list, tuple,\n",
        "                NumPy array, PyTorch tensor, or Python scalar.\n",
        "\n",
        "    Returns:\n",
        "        The same `value` that was passed in.\n",
        "    \"\"\"\n",
        "    if name not in _counters:\n",
        "        _counters[name] = dict()\n",
        "\n",
        "    elems = torch.as_tensor(value)\n",
        "    if elems.numel() == 0:\n",
        "        return value\n",
        "\n",
        "    elems = elems.detach().flatten().to(_reduce_dtype)\n",
        "    moments = torch.stack([\n",
        "        torch.ones_like(elems).sum(),\n",
        "        elems.sum(),\n",
        "        elems.square().sum(),\n",
        "    ])\n",
        "    assert moments.ndim == 1 and moments.shape[0] == _num_moments\n",
        "    moments = moments.to(_counter_dtype)\n",
        "\n",
        "    device = moments.device\n",
        "    if device not in _counters[name]:\n",
        "        _counters[name][device] = torch.zeros_like(moments)\n",
        "    _counters[name][device].add_(moments)\n",
        "    return value\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def report0(name, value):\n",
        "    r\"\"\"Broadcasts the given set of scalars by the first process (`rank = 0`),\n",
        "    but ignores any scalars provided by the other processes.\n",
        "    See `report()` for further details.\n",
        "    \"\"\"\n",
        "    report(name, value if _rank == 0 else [])\n",
        "    return value\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class Collector:\n",
        "    r\"\"\"Collects the scalars broadcasted by `report()` and `report0()` and\n",
        "    computes their long-term averages (mean and standard deviation) over\n",
        "    user-defined periods of time.\n",
        "\n",
        "    The averages are first collected into internal counters that are not\n",
        "    directly visible to the user. They are then copied to the user-visible\n",
        "    state as a result of calling `update()` and can then be queried using\n",
        "    `mean()`, `std()`, `as_dict()`, etc. Calling `update()` also resets the\n",
        "    internal counters for the next round, so that the user-visible state\n",
        "    effectively reflects averages collected between the last two calls to\n",
        "    `update()`.\n",
        "\n",
        "    Args:\n",
        "        regex:          Regular expression defining which statistics to\n",
        "                        collect. The default is to collect everything.\n",
        "        keep_previous:  Whether to retain the previous averages if no\n",
        "                        scalars were collected on a given round\n",
        "                        (default: True).\n",
        "    \"\"\"\n",
        "    def __init__(self, regex='.*', keep_previous=True):\n",
        "        self._regex = re.compile(regex)\n",
        "        self._keep_previous = keep_previous\n",
        "        self._cumulative = dict()\n",
        "        self._moments = dict()\n",
        "        self.update()\n",
        "        self._moments.clear()\n",
        "\n",
        "    def names(self):\n",
        "        r\"\"\"Returns the names of all statistics broadcasted so far that\n",
        "        match the regular expression specified at construction time.\n",
        "        \"\"\"\n",
        "        return [name for name in _counters if self._regex.fullmatch(name)]\n",
        "\n",
        "    def update(self):\n",
        "        r\"\"\"Copies current values of the internal counters to the\n",
        "        user-visible state and resets them for the next round.\n",
        "\n",
        "        If `keep_previous=True` was specified at construction time, the\n",
        "        operation is skipped for statistics that have received no scalars\n",
        "        since the last update, retaining their previous averages.\n",
        "\n",
        "        This method performs a number of GPU-to-CPU transfers and one\n",
        "        `torch.distributed.all_reduce()`. It is intended to be called\n",
        "        periodically in the main training loop, typically once every\n",
        "        N training steps.\n",
        "        \"\"\"\n",
        "        if not self._keep_previous:\n",
        "            self._moments.clear()\n",
        "        for name, cumulative in _sync(self.names()):\n",
        "            if name not in self._cumulative:\n",
        "                self._cumulative[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n",
        "            delta = cumulative - self._cumulative[name]\n",
        "            self._cumulative[name].copy_(cumulative)\n",
        "            if float(delta[0]) != 0:\n",
        "                self._moments[name] = delta\n",
        "\n",
        "    def _get_delta(self, name):\n",
        "        r\"\"\"Returns the raw moments that were accumulated for the given\n",
        "        statistic between the last two calls to `update()`, or zero if\n",
        "        no scalars were collected.\n",
        "        \"\"\"\n",
        "        assert self._regex.fullmatch(name)\n",
        "        if name not in self._moments:\n",
        "            self._moments[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n",
        "        return self._moments[name]\n",
        "\n",
        "    def num(self, name):\n",
        "        r\"\"\"Returns the number of scalars that were accumulated for the given\n",
        "        statistic between the last two calls to `update()`, or zero if\n",
        "        no scalars were collected.\n",
        "        \"\"\"\n",
        "        delta = self._get_delta(name)\n",
        "        return int(delta[0])\n",
        "\n",
        "    def mean(self, name):\n",
        "        r\"\"\"Returns the mean of the scalars that were accumulated for the\n",
        "        given statistic between the last two calls to `update()`, or NaN if\n",
        "        no scalars were collected.\n",
        "        \"\"\"\n",
        "        delta = self._get_delta(name)\n",
        "        if int(delta[0]) == 0:\n",
        "            return float('nan')\n",
        "        return float(delta[1] / delta[0])\n",
        "\n",
        "    def std(self, name):\n",
        "        r\"\"\"Returns the standard deviation of the scalars that were\n",
        "        accumulated for the given statistic between the last two calls to\n",
        "        `update()`, or NaN if no scalars were collected.\n",
        "        \"\"\"\n",
        "        delta = self._get_delta(name)\n",
        "        if int(delta[0]) == 0 or not np.isfinite(float(delta[1])):\n",
        "            return float('nan')\n",
        "        if int(delta[0]) == 1:\n",
        "            return float(0)\n",
        "        mean = float(delta[1] / delta[0])\n",
        "        raw_var = float(delta[2] / delta[0])\n",
        "        return np.sqrt(max(raw_var - np.square(mean), 0))\n",
        "\n",
        "    def as_dict(self):\n",
        "        r\"\"\"Returns the averages accumulated between the last two calls to\n",
        "        `update()` as an `dnnlib.EasyDict`. The contents are as follows:\n",
        "\n",
        "            dnnlib.EasyDict(\n",
        "                NAME = dnnlib.EasyDict(num=FLOAT, mean=FLOAT, std=FLOAT),\n",
        "                ...\n",
        "            )\n",
        "        \"\"\"\n",
        "        stats = dnnlib.EasyDict()\n",
        "        for name in self.names():\n",
        "            stats[name] = dnnlib.EasyDict(num=self.num(name), mean=self.mean(name), std=self.std(name))\n",
        "        return stats\n",
        "\n",
        "    def __getitem__(self, name):\n",
        "        r\"\"\"Convenience getter.\n",
        "        `collector[name]` is a synonym for `collector.mean(name)`.\n",
        "        \"\"\"\n",
        "        return self.mean(name)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _sync(names):\n",
        "    r\"\"\"Synchronize the global cumulative counters across devices and\n",
        "    processes. Called internally by `Collector.update()`.\n",
        "    \"\"\"\n",
        "    if len(names) == 0:\n",
        "        return []\n",
        "    global _sync_called\n",
        "    _sync_called = True\n",
        "\n",
        "    # Collect deltas within current rank.\n",
        "    deltas = []\n",
        "    device = _sync_device if _sync_device is not None else torch.device('cpu')\n",
        "    for name in names:\n",
        "        delta = torch.zeros([_num_moments], dtype=_counter_dtype, device=device)\n",
        "        for counter in _counters[name].values():\n",
        "            delta.add_(counter.to(device))\n",
        "            counter.copy_(torch.zeros_like(counter))\n",
        "        deltas.append(delta)\n",
        "    deltas = torch.stack(deltas)\n",
        "\n",
        "    # Sum deltas across ranks.\n",
        "    if _sync_device is not None:\n",
        "        torch.distributed.all_reduce(deltas)\n",
        "\n",
        "    # Update cumulative values.\n",
        "    deltas = deltas.cpu()\n",
        "    for idx, name in enumerate(names):\n",
        "        if name not in _cumulative:\n",
        "            _cumulative[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n",
        "        _cumulative[name].add_(deltas[idx])\n",
        "\n",
        "    # Return name-value pairs.\n",
        "    return [(name, _cumulative[name]) for name in names]\n",
        "\n",
        "#----------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB-y6GqHxUqt"
      },
      "source": [
        "#adamw.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ4oSkImxZK0"
      },
      "source": [
        "\"\"\" AdamW Optimizer\n",
        "Impl copied from PyTorch master\n",
        "\"\"\"\n",
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "    r\"\"\"Implements AdamW algorithm.\n",
        "\n",
        "    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=1e-2, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AdamW, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                # Perform stepweight decay\n",
        "                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "                # Perform optimization step\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "                else:\n",
        "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr'] / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjEhp13vxgXk"
      },
      "source": [
        "#celeba.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dvDJmcdxh8c"
      },
      "source": [
        "\n",
        "from functools import partial\n",
        "import torch\n",
        "import os\n",
        "import PIL\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.datasets.utils import download_file_from_google_drive, check_integrity, verify_str_arg\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "\n",
        "\n",
        "\n",
        "class CelebA(Dataset):\n",
        "    \"\"\" pyTorch Dataset wrapper for the generic flat directory images dataset \"\"\"\n",
        "\n",
        "    def __setup_files(self):\n",
        "        \"\"\"\n",
        "        private helper for setting up the files_list\n",
        "        :return: files => list of paths of files\n",
        "        \"\"\"\n",
        "        file_names = os.listdir(self.data_dir)\n",
        "        files = []  # initialize to empty list\n",
        "\n",
        "        for file_name in file_names:\n",
        "            possible_file = os.path.join(self.data_dir, file_name)\n",
        "            if os.path.isfile(possible_file):\n",
        "                files.append(possible_file)\n",
        "                \n",
        "        # return the files list\n",
        "        return files\n",
        "\n",
        "    def __init__(self, root, transform=None):\n",
        "        \"\"\"\n",
        "        constructor for the class\n",
        "        :param data_dir: path to the directory containing the data\n",
        "        :param transform: transforms to be applied to the images\n",
        "        \"\"\"\n",
        "        # define the state of the object\n",
        "        self.data_dir = root\n",
        "        self.transform = transform\n",
        "\n",
        "        # setup the files for reading\n",
        "        self.files = self.__setup_files()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        compute the length of the dataset\n",
        "        :return: len => length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        obtain the image (read and transform)\n",
        "        :param idx: index of the file required\n",
        "        :return: img => image array\n",
        "        \"\"\"\n",
        "        from PIL import Image\n",
        "\n",
        "        # read the image:\n",
        "        img_name = self.files[idx]\n",
        "        if img_name[-4:] == \".npy\":\n",
        "            img = np.load(img_name)\n",
        "            img = Image.fromarray(img.squeeze(0).transpose(1, 2, 0))\n",
        "        else:\n",
        "            img = Image.open(img_name)\n",
        "\n",
        "        # apply the transforms on the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # return the image:\n",
        "        return img, img\n",
        "    \n",
        "    \n",
        "class FFHQ(Dataset):\n",
        "    \"\"\" pyTorch Dataset wrapper for the generic flat directory images dataset \"\"\"\n",
        "\n",
        "    def __setup_files(self):\n",
        "        \"\"\"\n",
        "        private helper for setting up the files_list\n",
        "        :return: files => list of paths of files\n",
        "        \"\"\"\n",
        "        file_names = glob.glob(os.path.join(self.data_dir, \"./*/*.png\")) + \\\n",
        "                     glob.glob(os.path.join(self.data_dir, \"./*.jpg\")) + \\\n",
        "                    [y for x in os.walk(self.data_dir) for y in glob.glob(os.path.join(x[0], \"*.webp\"))]\n",
        "        files = []  # initialize to empty list\n",
        "\n",
        "        for file_name in file_names:\n",
        "            possible_file = os.path.join(self.data_dir, file_name)\n",
        "            if os.path.isfile(possible_file):\n",
        "                files.append(possible_file)\n",
        "\n",
        "        # return the files list\n",
        "        return files\n",
        "\n",
        "    def __init__(self, root, transform=None):\n",
        "        \"\"\"\n",
        "        constructor for the class\n",
        "        :param data_dir: path to the directory containing the data\n",
        "        :param transform: transforms to be applied to the images\n",
        "        \"\"\"\n",
        "        # define the state of the object\n",
        "        self.data_dir = root\n",
        "        self.transform = transform\n",
        "\n",
        "        # setup the files for reading\n",
        "        self.files = self.__setup_files()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        compute the length of the dataset\n",
        "        :return: len => length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        obtain the image (read and transform)\n",
        "        :param idx: index of the file required\n",
        "        :return: img => image array\n",
        "        \"\"\"\n",
        "        from PIL import Image\n",
        "\n",
        "        # read the image:\n",
        "        img_name = self.files[idx]\n",
        "        if img_name[-4:] == \".npy\":\n",
        "            img = np.load(img_name)\n",
        "            img = Image.fromarray(img.squeeze(0).transpose(1, 2, 0))\n",
        "        else:\n",
        "            img = Image.open(img_name)\n",
        "\n",
        "        # apply the transforms on the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # return the image:\n",
        "        return img, img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6weNos1xkm8"
      },
      "source": [
        "#cfg.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NSIowzpxovN"
      },
      "source": [
        "\n",
        "import argparse\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--world-size', default=-1, type=int,\n",
        "                    help='number of nodes for distributed training')\n",
        "    parser.add_argument('--rank', default=-1, type=int,\n",
        "                        help='node rank for distributed training')\n",
        "    parser.add_argument('--loca_rank', default=-1, type=int,\n",
        "                        help='node rank for distributed training')\n",
        "    parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,\n",
        "                        help='url used to set up distributed training')\n",
        "    parser.add_argument('--dist-backend', default='nccl', type=str,\n",
        "                        help='distributed backend')\n",
        "    parser.add_argument('--seed', default=12345, type=int,\n",
        "                        help='seed for initializing training. ')\n",
        "    parser.add_argument('--gpu', default=None, type=int,\n",
        "                        help='GPU id to use.')\n",
        "    parser.add_argument('--multiprocessing-distributed', action='store_true',\n",
        "                    help='Use multi-processing distributed training to launch '\n",
        "                         'N processes per node, which has N GPUs. This is the '\n",
        "                         'fastest way to use PyTorch for either single node or '\n",
        "                         'multi node data parallel training')\n",
        "    parser.add_argument(\n",
        "        '--max_epoch',\n",
        "        type=int,\n",
        "        default=200,\n",
        "        help='number of epochs of training')\n",
        "    parser.add_argument(\n",
        "        '--max_iter',\n",
        "        type=int,\n",
        "        default=None,\n",
        "        help='set the max iteration number')\n",
        "    parser.add_argument(\n",
        "        '-gen_bs',\n",
        "        '--gen_batch_size',\n",
        "        type=int,\n",
        "        default=64,\n",
        "        help='size of the batches')\n",
        "    parser.add_argument(\n",
        "        '-dis_bs',\n",
        "        '--dis_batch_size',\n",
        "        type=int,\n",
        "        default=64,\n",
        "        help='size of the batches')\n",
        "    parser.add_argument(\n",
        "        '--g_lr',\n",
        "        type=float,\n",
        "        default=0.0002,\n",
        "        help='adam: gen learning rate')\n",
        "    parser.add_argument(\n",
        "        '--wd',\n",
        "        type=float,\n",
        "        default=0,\n",
        "        help='adamw: gen weight decay')\n",
        "    parser.add_argument(\n",
        "        '--d_lr',\n",
        "        type=float,\n",
        "        default=0.0002,\n",
        "        help='adam: disc learning rate')\n",
        "    parser.add_argument(\n",
        "        '--ctrl_lr',\n",
        "        type=float,\n",
        "        default=3.5e-4,\n",
        "        help='adam: ctrl learning rate')\n",
        "    parser.add_argument(\n",
        "        '--lr_decay',\n",
        "        action='store_true',\n",
        "        help='learning rate decay or not')\n",
        "    parser.add_argument(\n",
        "        '--beta1',\n",
        "        type=float,\n",
        "        default=0.0,\n",
        "        help='adam: decay of first order momentum of gradient')\n",
        "    parser.add_argument(\n",
        "        '--beta2',\n",
        "        type=float,\n",
        "        default=0.9,\n",
        "        help='adam: decay of first order momentum of gradient')\n",
        "    parser.add_argument(\n",
        "        '--num_workers',\n",
        "        type=int,\n",
        "        default=8,\n",
        "        help='number of cpu threads to use during batch generation')\n",
        "    parser.add_argument(\n",
        "        '--latent_dim',\n",
        "        type=int,\n",
        "        default=128,\n",
        "        help='dimensionality of the latent space')\n",
        "    parser.add_argument(\n",
        "        '--img_size',\n",
        "        type=int,\n",
        "        default=32,\n",
        "        help='size of each image dimension')\n",
        "    parser.add_argument(\n",
        "        '--channels',\n",
        "        type=int,\n",
        "        default=3,\n",
        "        help='number of image channels')\n",
        "    parser.add_argument(\n",
        "        '--n_critic',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='number of training steps for discriminator per iter')\n",
        "    parser.add_argument(\n",
        "        '--val_freq',\n",
        "        type=int,\n",
        "        default=20,\n",
        "        help='interval between each validation')\n",
        "    parser.add_argument(\n",
        "        '--print_freq',\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help='interval between each verbose')\n",
        "    parser.add_argument(\n",
        "        '--load_path',\n",
        "        type=str,\n",
        "        help='The reload model path')\n",
        "    parser.add_argument(\n",
        "        '--exp_name',\n",
        "        type=str,\n",
        "        help='The name of exp')\n",
        "    parser.add_argument(\n",
        "        '--d_spectral_norm',\n",
        "        type=str2bool,\n",
        "        default=False,\n",
        "        help='add spectral_norm on discriminator?')\n",
        "    parser.add_argument(\n",
        "        '--g_spectral_norm',\n",
        "        type=str2bool,\n",
        "        default=False,\n",
        "        help='add spectral_norm on generator?')\n",
        "    parser.add_argument(\n",
        "        '--dataset',\n",
        "        type=str,\n",
        "        default='cifar10',\n",
        "        help='dataset type')\n",
        "    parser.add_argument(\n",
        "        '--data_path',\n",
        "        type=str,\n",
        "        default='./data',\n",
        "        help='The path of data set')\n",
        "    parser.add_argument('--init_type', type=str, default='normal',\n",
        "                        choices=['normal', 'orth', 'xavier_uniform', 'false'],\n",
        "                        help='The init type')\n",
        "    parser.add_argument('--gf_dim', type=int, default=64,\n",
        "                        help='The base channel num of gen')\n",
        "    parser.add_argument('--df_dim', type=int, default=64,\n",
        "                        help='The base channel num of disc')\n",
        "    parser.add_argument(\n",
        "        '--gen_model',\n",
        "        type=str,\n",
        "        help='path of gen model')\n",
        "    parser.add_argument(\n",
        "        '--dis_model',\n",
        "        type=str,\n",
        "        help='path of dis model')\n",
        "    parser.add_argument(\n",
        "        '--controller',\n",
        "        type=str,\n",
        "        default='controller',\n",
        "        help='path of controller')\n",
        "    parser.add_argument('--eval_batch_size', type=int, default=100)\n",
        "    parser.add_argument('--num_eval_imgs', type=int, default=50000)\n",
        "    parser.add_argument(\n",
        "        '--bottom_width',\n",
        "        type=int,\n",
        "        default=4,\n",
        "        help=\"the base resolution of the GAN\")\n",
        "    parser.add_argument('--random_seed', type=int, default=12345)\n",
        "\n",
        "    # search\n",
        "    parser.add_argument('--shared_epoch', type=int, default=15,\n",
        "                        help='the number of epoch to train the shared gan at each search iteration')\n",
        "    parser.add_argument('--grow_step1', type=int, default=25,\n",
        "                        help='which iteration to grow the image size from 8 to 16')\n",
        "    parser.add_argument('--grow_step2', type=int, default=55,\n",
        "                        help='which iteration to grow the image size from 16 to 32')\n",
        "    parser.add_argument('--max_search_iter', type=int, default=90,\n",
        "                        help='max search iterations of this algorithm')\n",
        "    parser.add_argument('--ctrl_step', type=int, default=30,\n",
        "                        help='number of steps to train the controller at each search iteration')\n",
        "    parser.add_argument('--ctrl_sample_batch', type=int, default=1,\n",
        "                        help='sample size of controller of each step')\n",
        "    parser.add_argument('--hid_size', type=int, default=100,\n",
        "                        help='the size of hidden vector')\n",
        "    parser.add_argument('--baseline_decay', type=float, default=0.9,\n",
        "                        help='baseline decay rate in RL')\n",
        "    parser.add_argument('--rl_num_eval_img', type=int, default=5000,\n",
        "                        help='number of images to be sampled in order to get the reward')\n",
        "    parser.add_argument('--num_candidate', type=int, default=10,\n",
        "                        help='number of candidate architectures to be sampled')\n",
        "    parser.add_argument('--topk', type=int, default=5,\n",
        "                        help='preserve topk models architectures after each stage' )\n",
        "    parser.add_argument('--entropy_coeff', type=float, default=1e-3,\n",
        "                        help='to encourage the exploration')\n",
        "    parser.add_argument('--dynamic_reset_threshold', type=float, default=1e-3,\n",
        "                        help='var threshold')\n",
        "    parser.add_argument('--dynamic_reset_window', type=int, default=500,\n",
        "                        help='the window size')\n",
        "    parser.add_argument('--arch', nargs='+', type=int,\n",
        "                        help='the vector of a discovered architecture')\n",
        "    parser.add_argument('--optimizer', type=str, default=\"adam\",\n",
        "                        help='optimizer')\n",
        "    parser.add_argument('--loss', type=str, default=\"hinge\",\n",
        "                        help='loss function')\n",
        "    parser.add_argument('--n_classes', type=int, default=0,\n",
        "                        help='classes')\n",
        "    parser.add_argument('--phi', type=float, default=1,\n",
        "                        help='wgan-gp phi')\n",
        "    parser.add_argument('--grow_steps', nargs='+', type=int,\n",
        "                        help='the vector of a discovered architecture')\n",
        "    parser.add_argument('--D_downsample', type=str, default=\"avg\",\n",
        "                        help='downsampling type')\n",
        "    parser.add_argument('--fade_in', type=float, default=1,\n",
        "                        help='fade in step')\n",
        "    parser.add_argument('--d_depth', type=int, default=7,\n",
        "                        help='Discriminator Depth')\n",
        "    parser.add_argument('--g_depth', type=str, default=\"5,4,2\",\n",
        "                        help='Generator Depth')\n",
        "    parser.add_argument('--g_norm', type=str, default=\"ln\",\n",
        "                        help='Generator Normalization')\n",
        "    parser.add_argument('--d_norm', type=str, default=\"ln\",\n",
        "                        help='Discriminator Normalization')\n",
        "    parser.add_argument('--g_act', type=str, default=\"gelu\",\n",
        "                        help='Generator activation Layer')\n",
        "    parser.add_argument('--d_act', type=str, default=\"gelu\",\n",
        "                        help='Discriminator activation layer')\n",
        "    parser.add_argument('--patch_size', type=int, default=4,\n",
        "                        help='Discriminator Depth')\n",
        "    parser.add_argument('--fid_stat', type=str, default=\"None\",\n",
        "                        help='Discriminator Depth')\n",
        "    parser.add_argument('--diff_aug', type=str, default=\"None\",\n",
        "                        help='differentiable augmentation type')\n",
        "    parser.add_argument('--accumulated_times', type=int, default=1,\n",
        "                        help='gradient accumulation')\n",
        "    parser.add_argument('--g_accumulated_times', type=int, default=1,\n",
        "                        help='gradient accumulation')\n",
        "    parser.add_argument('--num_landmarks', type=int, default=64,\n",
        "                        help='number of landmarks')\n",
        "    parser.add_argument('--d_heads', type=int, default=4,\n",
        "                        help='number of heads')\n",
        "    parser.add_argument('--dropout', type=float, default=0.,\n",
        "                        help='dropout ratio')\n",
        "    parser.add_argument('--ema', type=float, default=0.995,\n",
        "                        help='ema')\n",
        "    parser.add_argument('--ema_warmup', type=float, default=0.,\n",
        "                        help='ema warm up')\n",
        "    parser.add_argument('--ema_kimg', type=int, default=500,\n",
        "                        help='ema thousand images')\n",
        "    parser.add_argument('--latent_norm',action='store_true',\n",
        "        help='latent vector normalization')\n",
        "    parser.add_argument('--ministd',action='store_true',\n",
        "        help='mini batch std')\n",
        "    parser.add_argument('--g_mlp', type=int, default=4,\n",
        "                        help='generator mlp ratio')\n",
        "    parser.add_argument('--d_mlp', type=int, default=4,\n",
        "                        help='discriminator mlp ratio')\n",
        "    parser.add_argument('--g_window_size', type=int, default=8,\n",
        "                        help='generator mlp ratio')\n",
        "    parser.add_argument('--d_window_size', type=int, default=8,\n",
        "                        help='discriminator mlp ratio')\n",
        "    parser.add_argument('--show', action='store_true',\n",
        "                    help='show')\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    return opt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KAvZfIqxpaM"
      },
      "source": [
        "#datasets.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJNIMfqcxthM"
      },
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from celeba import CelebA, FFHQ\n",
        "'''\n",
        "class ImageDataset(object):\n",
        "    def __init__(self, args, cur_img_size=None, bs=None):\n",
        "        bs = args.dis_batch_size if bs == None else bs\n",
        "        img_size = cur_img_size if args.fade_in > 0 else args.img_size\n",
        "        if args.dataset.lower() == 'cifar10':\n",
        "            Dt = datasets.CIFAR10\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(size=(img_size, img_size)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ])\n",
        "            args.n_classes = 0\n",
        "            train_dataset = Dt(root=args.data_path, train=True, transform=transform, download=True)\n",
        "            val_dataset = Dt(root=args.data_path, train=False, transform=transform)\n",
        "            \n",
        "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "            self.train_sampler = train_sampler\n",
        "            self.train = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "            self.valid = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "\n",
        "            self.test = self.valid\n",
        "        \n",
        "            \n",
        "        elif args.dataset.lower() == 'stl10':\n",
        "            Dt = datasets.STL10\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(img_size),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ])\n",
        "            \n",
        "            train_dataset = Dt(root=args.data_path, split='train+unlabeled', transform=transform, download=True)\n",
        "            val_dataset = Dt(root=args.data_path, split='test', transform=transform)\n",
        "            if args.distributed:\n",
        "                train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "                val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "            else:\n",
        "                train_sampler = None\n",
        "                val_sampler = None\n",
        "            self.train_sampler = train_sampler\n",
        "            self.train = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "            self.valid = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "\n",
        "            self.test = self.valid\n",
        "        elif args.dataset.lower() == 'celeba':\n",
        "            Dt = CelebA\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(size=(img_size, img_size)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ])\n",
        "            \n",
        "            train_dataset = Dt(root=args.data_path, transform=transform)\n",
        "            val_dataset = Dt(root=args.data_path, transform=transform)\n",
        "            \n",
        "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "            self.train_sampler = train_sampler\n",
        "            self.train = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=args.num_workers, pin_memory=True, drop_last=True, sampler=train_sampler)\n",
        "\n",
        "            self.valid = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "\n",
        "            self.test = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "        elif args.dataset.lower() == 'ffhq':\n",
        "            Dt = FFHQ\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(size=(img_size, img_size)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ])\n",
        "            \n",
        "            train_dataset = Dt(root=args.data_path, transform=transform)\n",
        "            val_dataset = Dt(root=args.data_path, transform=transform)\n",
        "            \n",
        "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "            self.train_sampler = train_sampler\n",
        "            \n",
        "            self.train = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=args.num_workers, pin_memory=True, drop_last=True, sampler=train_sampler)\n",
        "\n",
        "            self.valid = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "\n",
        "            self.test = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "        elif args.dataset.lower() == 'bedroom':\n",
        "            Dt = datasets.LSUN\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(size=(img_size, img_size)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ])\n",
        "            \n",
        "            train_dataset = Dt(root=args.data_path, classes=[\"bedroom_train\"], transform=transform)\n",
        "            val_dataset = Dt(root=args.data_path, classes=[\"bedroom_val\"], transform=transform)\n",
        "            \n",
        "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "            self.train_sampler = train_sampler\n",
        "            self.train = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=args.num_workers, pin_memory=True, drop_last=True, sampler=train_sampler)\n",
        "\n",
        "            self.valid = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "\n",
        "            self.test = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "        elif args.dataset.lower() == 'church':\n",
        "            Dt = datasets.LSUN\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize(size=(img_size, img_size)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ])\n",
        "            \n",
        "            train_dataset = Dt(root=args.data_path, classes=[\"church_outdoor_train\"], transform=transform)\n",
        "            val_dataset = Dt(root=args.data_path, classes=[\"church_outdoor_val\"], transform=transform)\n",
        "            \n",
        "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "            val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "            self.train_sampler = train_sampler\n",
        "            self.train = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=(train_sampler is None),\n",
        "                num_workers=args.num_workers, pin_memory=True, drop_last=True, sampler=train_sampler)\n",
        "\n",
        "            self.valid = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "\n",
        "            self.test = torch.utils.data.DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=args.dis_batch_size, shuffle=False,\n",
        "                num_workers=args.num_workers, pin_memory=True, sampler=val_sampler)\n",
        "        else:\n",
        "            raise NotImplementedError('Unknown dataset: {}'.format(args.dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnRoaSV3xuZs"
      },
      "source": [
        "#flops.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8EMSdK0xxol"
      },
      "source": [
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "import cfg\n",
        "import models_search\n",
        "import datasets\n",
        "from functions import train, validate, LinearLrDecay, load_params, copy_params, cur_stages\n",
        "from utils.utils import set_log_dir, save_checkpoint, create_logger\n",
        "from utils.inception_score import _init_inception\n",
        "from utils.fid_score import create_inception_graph, check_or_download_inception\n",
        "'''\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "!pip install tensorboardX\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from adamw import AdamW\n",
        "'''\n",
        "import random \n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from models_search.ViT_8_8 import matmul, count_matmul\n",
        "'''\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    torch.cuda.manual_seed(args.random_seed)\n",
        "    torch.cuda.manual_seed_all(args.random_seed)\n",
        "    np.random.seed(args.random_seed)\n",
        "    random.seed(args.random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "    # set tf env\n",
        "    # _init_inception()\n",
        "    # inception_path = check_or_download_inception(None)\n",
        "    # create_inception_graph(inception_path)\n",
        "\n",
        "    # # import network\n",
        "    gen_net = eval('models_search.'+args.gen_model+'.Generator')(args=args).cuda()\n",
        "    dis_net = eval('models_search.'+args.dis_model+'.Discriminator')(args=args).cuda()\n",
        "    gen_net.set_arch(args.arch, cur_stage=2)\n",
        "\n",
        "    import thop, math\n",
        "    dummy_data = (1, 1024)\n",
        "    macs, params = thop.profile(gen_net, inputs=(torch.randn(dummy_data).cuda(), ),\n",
        "                        custom_ops={matmul: count_matmul})\n",
        "    flops, params = thop.clever_format([macs, params], \"%.3f\")\n",
        "    print('Flops (GB):\\t', flops)\n",
        "    print('Params Size (MB):\\t', params)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBBSKBp5xydP"
      },
      "source": [
        "#functions.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGNNpS7Ox06c"
      },
      "source": [
        "\n",
        "\n",
        "import logging\n",
        "import operator\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from imageio import imsave\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "from utils.utils import make_grid, save_image\n",
        "from utils.torch_fid_score import get_fid\n",
        "'''\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# from utils.fid_score import calculate_fid_given_paths\n",
        "\n",
        "# from utils.inception_score import get_inception_scorepython exps/dist1_new_church256.py --node 0022 --rank 0sample\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def cur_stages(iter, args):\n",
        "        \"\"\"\n",
        "        Return current stage.\n",
        "        :param epoch: current epoch.\n",
        "        :return: current stage\n",
        "        \"\"\"\n",
        "        # if search_iter < self.grow_step1:\n",
        "        #     return 0\n",
        "        # elif self.grow_step1 <= search_iter < self.grow_step2:\n",
        "        #     return 1\n",
        "        # else:\n",
        "        #     return 2\n",
        "        # for idx, grow_step in enumerate(args.grow_steps):\n",
        "        #     if iter < grow_step:\n",
        "        #         return idx\n",
        "        # return len(args.grow_steps)\n",
        "        idx = 0\n",
        "        for i in range(len(args.grow_steps)):\n",
        "            if iter >= args.grow_steps[i]:\n",
        "                idx = i+1\n",
        "        return idx\n",
        "\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples, phi):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).to(real_samples.get_device())\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = torch.ones([real_samples.shape[0], 1], requires_grad=False).to(real_samples.get_device())\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.reshape(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - phi) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def train(args, gen_net: nn.Module, dis_net: nn.Module, gen_optimizer, dis_optimizer, gen_avg_param, train_loader,\n",
        "          epoch, writer_dict, fixed_z, schedulers=None):\n",
        "    writer = writer_dict['writer']\n",
        "    gen_step = 0\n",
        "    # train mode\n",
        "    gen_net.train()\n",
        "    dis_net.train()\n",
        "    \n",
        "    dis_optimizer.zero_grad()\n",
        "    gen_optimizer.zero_grad()\n",
        "    for iter_idx, (imgs, _) in enumerate(tqdm(train_loader)):\n",
        "        global_steps = writer_dict['train_global_steps']\n",
        "        \n",
        "\n",
        "        # Adversarial ground truths\n",
        "        real_imgs = imgs.type(torch.cuda.FloatTensor).cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = torch.cuda.FloatTensor(np.random.normal(0, 1, (imgs.shape[0], args.latent_dim))).cuda(args.gpu, non_blocking=True)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        \n",
        "\n",
        "        real_validity = dis_net(real_imgs)\n",
        "        fake_imgs = gen_net(z, epoch).detach()\n",
        "        assert fake_imgs.size() == real_imgs.size(), f\"fake_imgs.size(): {fake_imgs.size()} real_imgs.size(): {real_imgs.size()}\"\n",
        "\n",
        "        fake_validity = dis_net(fake_imgs)\n",
        "\n",
        "        # cal loss\n",
        "        if args.loss == 'hinge':\n",
        "            d_loss = 0\n",
        "            d_loss = torch.mean(nn.ReLU(inplace=True)(1.0 - real_validity)) + \\\n",
        "                    torch.mean(nn.ReLU(inplace=True)(1 + fake_validity))\n",
        "        elif args.loss == 'standard':\n",
        "            real_label = torch.full((imgs.shape[0],), 1., dtype=torch.float, device=real_imgs.get_device())\n",
        "            fake_label = torch.full((imgs.shape[0],), 0., dtype=torch.float, device=real_imgs.get_device())\n",
        "            real_validity = nn.Sigmoid()(real_validity.view(-1))\n",
        "            fake_validity = nn.Sigmoid()(fake_validity.view(-1))\n",
        "            d_real_loss = nn.BCELoss()(real_validity, real_label)\n",
        "            d_fake_loss = nn.BCELoss()(fake_validity, fake_label)\n",
        "        elif args.loss == 'lsgan':\n",
        "            if isinstance(fake_validity, list):\n",
        "                d_loss = 0\n",
        "                for real_validity_item, fake_validity_item in zip(real_validity, fake_validity):\n",
        "                    real_label = torch.full((real_validity_item.shape[0],real_validity_item.shape[1]), 1., dtype=torch.float, device=real_imgs.get_device())\n",
        "                    fake_label = torch.full((real_validity_item.shape[0],real_validity_item.shape[1]), 0., dtype=torch.float, device=real_imgs.get_device())\n",
        "                    d_real_loss = nn.MSELoss()(real_validity_item, real_label)\n",
        "                    d_fake_loss = nn.MSELoss()(fake_validity_item, fake_label)\n",
        "                    d_loss += d_real_loss + d_fake_loss\n",
        "            else:\n",
        "                real_label = torch.full((real_validity.shape[0],real_validity.shape[1]), 1., dtype=torch.float, device=real_imgs.get_device())\n",
        "                fake_label = torch.full((real_validity.shape[0],real_validity.shape[1]), 0., dtype=torch.float, device=real_imgs.get_device())\n",
        "                d_real_loss = nn.MSELoss()(real_validity, real_label)\n",
        "                d_fake_loss = nn.MSELoss()(fake_validity, fake_label)\n",
        "                d_loss = d_real_loss + d_fake_loss\n",
        "        elif args.loss == 'wgangp':\n",
        "            gradient_penalty = compute_gradient_penalty(dis_net, real_imgs, fake_imgs.detach(), args.phi)\n",
        "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gradient_penalty * 10 / (\n",
        "                    args.phi ** 2)\n",
        "        elif args.loss == 'wgangp-mode':\n",
        "            gradient_penalty = compute_gradient_penalty(dis_net, real_imgs, fake_imgs.detach(), args.phi)\n",
        "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gradient_penalty * 10 / (\n",
        "                    args.phi ** 2)\n",
        "        elif args.loss == 'wgangp-eps':\n",
        "            gradient_penalty = compute_gradient_penalty(dis_net, real_imgs, fake_imgs.detach(), args.phi)\n",
        "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gradient_penalty * 10 / (\n",
        "                    args.phi ** 2)\n",
        "            d_loss += (torch.mean(real_validity) ** 2) * 1e-3\n",
        "        else:\n",
        "            raise NotImplementedError(args.loss)\n",
        "        d_loss = d_loss/float(args.accumulated_times)\n",
        "        d_loss.backward()\n",
        "        \n",
        "        if (iter_idx + 1) % args.accumulated_times == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(dis_net.parameters(), 5.)\n",
        "            dis_optimizer.step()\n",
        "            dis_optimizer.zero_grad()\n",
        "\n",
        "            writer.add_scalar('d_loss', d_loss.item(), global_steps) if args.rank == 0 else 0\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        if global_steps % (args.n_critic * args.accumulated_times) == 0:\n",
        "            \n",
        "            for accumulated_idx in range(args.g_accumulated_times):\n",
        "                gen_z = torch.cuda.FloatTensor(np.random.normal(0, 1, (args.gen_batch_size, args.latent_dim)))\n",
        "                gen_imgs = gen_net(gen_z, epoch)\n",
        "                fake_validity = dis_net(gen_imgs)\n",
        "\n",
        "                # cal loss\n",
        "                loss_lz = torch.tensor(0)\n",
        "                if args.loss == \"standard\":\n",
        "                    real_label = torch.full((args.gen_batch_size,), 1., dtype=torch.float, device=real_imgs.get_device())\n",
        "                    fake_validity = nn.Sigmoid()(fake_validity.view(-1))\n",
        "                    g_loss = nn.BCELoss()(fake_validity.view(-1), real_label)\n",
        "                if args.loss == \"lsgan\":\n",
        "                    if isinstance(fake_validity, list):\n",
        "                        g_loss = 0\n",
        "                        for fake_validity_item in fake_validity:\n",
        "                            real_label = torch.full((fake_validity_item.shape[0],fake_validity_item.shape[1]), 1., dtype=torch.float, device=real_imgs.get_device())\n",
        "                            g_loss += nn.MSELoss()(fake_validity_item, real_label)\n",
        "                    else:\n",
        "                        real_label = torch.full((fake_validity.shape[0],fake_validity.shape[1]), 1., dtype=torch.float, device=real_imgs.get_device())\n",
        "                        # fake_validity = nn.Sigmoid()(fake_validity.view(-1))\n",
        "                        g_loss = nn.MSELoss()(fake_validity, real_label)\n",
        "                elif args.loss == 'wgangp-mode':\n",
        "                    fake_image1, fake_image2 = gen_imgs[:args.gen_batch_size//2], gen_imgs[args.gen_batch_size//2:]\n",
        "                    z_random1, z_random2 = gen_z[:args.gen_batch_size//2], gen_z[args.gen_batch_size//2:]\n",
        "                    lz = torch.mean(torch.abs(fake_image2 - fake_image1)) / torch.mean(\n",
        "                    torch.abs(z_random2 - z_random1))\n",
        "                    eps = 1 * 1e-5\n",
        "                    loss_lz = 1 / (lz + eps)\n",
        "\n",
        "                    g_loss = -torch.mean(fake_validity) + loss_lz\n",
        "                else:\n",
        "                    g_loss = -torch.mean(fake_validity)\n",
        "                g_loss = g_loss/float(args.g_accumulated_times)\n",
        "                g_loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(gen_net.parameters(), 5.)\n",
        "            gen_optimizer.step()\n",
        "            gen_optimizer.zero_grad()\n",
        "\n",
        "            # adjust learning rate\n",
        "            if schedulers:\n",
        "                gen_scheduler, dis_scheduler = schedulers\n",
        "                g_lr = gen_scheduler.step(global_steps)\n",
        "                d_lr = dis_scheduler.step(global_steps)\n",
        "                writer.add_scalar('LR/g_lr', g_lr, global_steps)\n",
        "                writer.add_scalar('LR/d_lr', d_lr, global_steps)\n",
        "\n",
        "            # moving average weight\n",
        "            ema_nimg = args.ema_kimg * 1000\n",
        "            cur_nimg = args.dis_batch_size * args.world_size * global_steps\n",
        "            if args.ema_warmup != 0:\n",
        "                ema_nimg = min(ema_nimg, cur_nimg * args.ema_warmup)\n",
        "                ema_beta = 0.5 ** (float(args.dis_batch_size * args.world_size) / max(ema_nimg, 1e-8))\n",
        "            else:\n",
        "                ema_beta = args.ema\n",
        "                \n",
        "            # moving average weight\n",
        "            for p, avg_p in zip(gen_net.parameters(), gen_avg_param):\n",
        "                cpu_p = deepcopy(p)\n",
        "                avg_p.mul_(ema_beta).add_(1. - ema_beta, cpu_p.cpu().data)\n",
        "                del cpu_p\n",
        "\n",
        "            writer.add_scalar('g_loss', g_loss.item(), global_steps) if args.rank == 0 else 0\n",
        "            gen_step += 1\n",
        "\n",
        "        # verbose\n",
        "        if gen_step and iter_idx % args.print_freq == 0 and args.rank == 0:\n",
        "            sample_imgs = torch.cat((gen_imgs[:16], real_imgs[:16]), dim=0)\n",
        "#             scale_factor = args.img_size // int(sample_imgs.size(3))\n",
        "#             sample_imgs = torch.nn.functional.interpolate(sample_imgs, scale_factor=2)\n",
        "#             img_grid = make_grid(sample_imgs, nrow=4, normalize=True, scale_each=True)\n",
        "#             save_image(sample_imgs, f'sampled_images_{args.exp_name}.jpg', nrow=4, normalize=True, scale_each=True)\n",
        "            # writer.add_image(f'sampled_images_{args.exp_name}', img_grid, global_steps)\n",
        "            tqdm.write(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [ema: %f] \" %\n",
        "                (epoch, args.max_epoch, iter_idx % len(train_loader), len(train_loader), d_loss.item(), g_loss.item(), ema_beta))\n",
        "            del gen_imgs\n",
        "            del real_imgs\n",
        "            del fake_validity\n",
        "            del real_validity\n",
        "            del g_loss\n",
        "            del d_loss\n",
        "\n",
        "        writer_dict['train_global_steps'] = global_steps + 1 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_is(args, gen_net: nn.Module, num_img):\n",
        "    \"\"\"\n",
        "    Get inception score.\n",
        "    :param args:\n",
        "    :param gen_net:\n",
        "    :param num_img:\n",
        "    :return: Inception score\n",
        "    \"\"\"\n",
        "\n",
        "    # eval mode\n",
        "    gen_net = gen_net.eval()\n",
        "\n",
        "    eval_iter = num_img // args.eval_batch_size\n",
        "    img_list = list()\n",
        "    for _ in range(eval_iter):\n",
        "        z = torch.cuda.FloatTensor(np.random.normal(0, 1, (args.eval_batch_size, args.latent_dim)))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = gen_net(z).mul_(127.5).add_(127.5).clamp_(0.0, 255.0).permute(0, 2, 3, 1).to('cpu',\n",
        "                                                                                                torch.uint8).numpy()\n",
        "        img_list.extend(list(gen_imgs))\n",
        "\n",
        "    # get inception score\n",
        "    logger.info('calculate Inception score...')\n",
        "    mean, std = get_inception_score(img_list)\n",
        "\n",
        "    return mean\n",
        "\n",
        "\n",
        "def validate(args, fixed_z, fid_stat, epoch, gen_net: nn.Module, writer_dict, clean_dir=True):\n",
        "    writer = writer_dict['writer']\n",
        "    global_steps = writer_dict['valid_global_steps']\n",
        "\n",
        "    # eval mode\n",
        "    gen_net.eval()\n",
        "\n",
        "#     generate images\n",
        "#     with torch.no_grad():\n",
        "#         sample_imgs = gen_net(fixed_z, epoch)\n",
        "#     img_grid = make_grid(sample_imgs, nrow=5, normalize=True, scale_each=True)\n",
        "\n",
        "#     get fid and inception score\n",
        "#     if args.gpu == 0:\n",
        "#         fid_buffer_dir = os.path.join(args.path_helper['sample_path'], 'fid_buffer')\n",
        "#         os.makedirs(fid_buffer_dir, exist_ok=True) if args.gpu == 0 else 0\n",
        "\n",
        "#     eval_iter = args.num_eval_imgs // args.eval_batch_size\n",
        "#     img_list = list()\n",
        "#     for iter_idx in tqdm(range(eval_iter), desc='sample images'):\n",
        "#         z = torch.cuda.FloatTensor(np.random.normal(0, 1, (args.eval_batch_size, args.latent_dim)))\n",
        "    \n",
        "#         # Generate a batch of images\n",
        "#         gen_imgs = gen_net(z, epoch).mul_(127.5).add_(127.5).clamp_(0.0, 255.0).permute(0, 2, 3, 1).to('cpu',\n",
        "#                                                                                                 torch.uint8).numpy()\n",
        "#         for img_idx, img in enumerate(gen_imgs):\n",
        "#             file_name = os.path.join(fid_buffer_dir, f'iter{iter_idx}_b{img_idx}.png')\n",
        "#             imsave(file_name, img)\n",
        "#         img_list.extend(list(gen_imgs))\n",
        "\n",
        "#     get inception score\n",
        "    logger.info('=> calculate inception score') if args.rank == 0 else 0\n",
        "    if args.rank == 0:\n",
        "#         mean, std = get_inception_score(img_list)\n",
        "        mean, std = 0, 0\n",
        "    else:\n",
        "        mean, std = 0, 0\n",
        "    print(f\"Inception score: {mean}\") if args.rank == 0 else 0\n",
        "#     mean, std = 0, 0\n",
        "    # get fid score\n",
        "    print('=> calculate fid score') if args.rank == 0 else 0\n",
        "    if args.rank == 0:\n",
        "        fid_score = get_fid(args, fid_stat, epoch, gen_net, args.num_eval_imgs, args.gen_batch_size, args.eval_batch_size, writer_dict=writer_dict, cls_idx=None)\n",
        "    else:\n",
        "        fid_score = 10000\n",
        "    # fid_score = 10000\n",
        "    print(f\"FID score: {fid_score}\") if args.rank == 0 else 0\n",
        "    \n",
        "#     if args.gpu == 0:\n",
        "#         if clean_dir:\n",
        "#             os.system('rm -r {}'.format(fid_buffer_dir))\n",
        "#         else:\n",
        "#             logger.info(f'=> sampled images are saved to {fid_buffer_dir}')\n",
        "\n",
        "#     writer.add_image('sampled_images', img_grid, global_steps)\n",
        "    if args.rank == 0:\n",
        "        writer.add_scalar('Inception_score/mean', mean, global_steps)\n",
        "        writer.add_scalar('Inception_score/std', std, global_steps)\n",
        "        writer.add_scalar('FID_score', fid_score, global_steps)\n",
        "\n",
        "        writer_dict['valid_global_steps'] = global_steps + 1\n",
        "\n",
        "    return mean, fid_score\n",
        "\n",
        "\n",
        "def save_samples(args, fixed_z, fid_stat, epoch, gen_net: nn.Module, writer_dict, clean_dir=True):\n",
        "\n",
        "    # eval mode\n",
        "    gen_net.eval()\n",
        "    with torch.no_grad():\n",
        "        # generate images\n",
        "        batch_size = fixed_z.size(0)\n",
        "        sample_imgs = []\n",
        "        for i in range(fixed_z.size(0)):\n",
        "            sample_img = gen_net(fixed_z[i:(i+1)], epoch)\n",
        "            sample_imgs.append(sample_img)\n",
        "        sample_imgs = torch.cat(sample_imgs, dim=0)\n",
        "        os.makedirs(f\"./samples/{args.exp_name}\", exist_ok=True)\n",
        "        save_image(sample_imgs, f'./samples/{args.exp_name}/sampled_images_{epoch}.png', nrow=10, normalize=True, scale_each=True)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_topk_arch_hidden(args, controller, gen_net, prev_archs, prev_hiddens):\n",
        "    \"\"\"\n",
        "    ~\n",
        "    :param args:\n",
        "    :param controller:\n",
        "    :param gen_net:\n",
        "    :param prev_archs: previous architecture\n",
        "    :param prev_hiddens: previous hidden vector\n",
        "    :return: a list of topk archs and hiddens.\n",
        "    \"\"\"\n",
        "    logger.info(f'=> get top{args.topk} archs out of {args.num_candidate} candidate archs...')\n",
        "    assert args.num_candidate >= args.topk\n",
        "    controller.eval()\n",
        "    cur_stage = controller.cur_stage\n",
        "    archs, _, _, hiddens = controller.sample(args.num_candidate, with_hidden=True, prev_archs=prev_archs,\n",
        "                                             prev_hiddens=prev_hiddens)\n",
        "    hxs, cxs = hiddens\n",
        "    arch_idx_perf_table = {}\n",
        "    for arch_idx in range(len(archs)):\n",
        "        logger.info(f'arch: {archs[arch_idx]}')\n",
        "        gen_net.set_arch(archs[arch_idx], cur_stage)\n",
        "        is_score = get_is(args, gen_net, args.rl_num_eval_img)\n",
        "        logger.info(f'get Inception score of {is_score}')\n",
        "        arch_idx_perf_table[arch_idx] = is_score\n",
        "    topk_arch_idx_perf = sorted(arch_idx_perf_table.items(), key=operator.itemgetter(1))[::-1][:args.topk]\n",
        "    topk_archs = []\n",
        "    topk_hxs = []\n",
        "    topk_cxs = []\n",
        "    logger.info(f'top{args.topk} archs:')\n",
        "    for arch_idx_perf in topk_arch_idx_perf:\n",
        "        logger.info(arch_idx_perf)\n",
        "        arch_idx = arch_idx_perf[0]\n",
        "        topk_archs.append(archs[arch_idx])\n",
        "        topk_hxs.append(hxs[arch_idx].detach().requires_grad_(False))\n",
        "        topk_cxs.append(cxs[arch_idx].detach().requires_grad_(False))\n",
        "\n",
        "    return topk_archs, (topk_hxs, topk_cxs)\n",
        "\n",
        "\n",
        "class LinearLrDecay(object):\n",
        "    def __init__(self, optimizer, start_lr, end_lr, decay_start_step, decay_end_step):\n",
        "\n",
        "        assert start_lr > end_lr\n",
        "        self.optimizer = optimizer\n",
        "        self.delta = (start_lr - end_lr) / (decay_end_step - decay_start_step)\n",
        "        self.decay_start_step = decay_start_step\n",
        "        self.decay_end_step = decay_end_step\n",
        "        self.start_lr = start_lr\n",
        "        self.end_lr = end_lr\n",
        "\n",
        "    def step(self, current_step):\n",
        "        if current_step <= self.decay_start_step:\n",
        "            lr = self.start_lr\n",
        "        elif current_step >= self.decay_end_step:\n",
        "            lr = self.end_lr\n",
        "        else:\n",
        "            lr = self.start_lr - self.delta * (current_step - self.decay_start_step)\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "def load_params(model, new_param, args, mode=\"gpu\"):\n",
        "    if mode == \"cpu\":\n",
        "        for p, new_p in zip(model.parameters(), new_param):\n",
        "            cpu_p = deepcopy(new_p)\n",
        "            p.data.copy_(cpu_p.cuda().to(f\"cuda:{args.gpu}\"))\n",
        "            del cpu_p\n",
        "    \n",
        "    else:\n",
        "        for p, new_p in zip(model.parameters(), new_param):\n",
        "            p.data.copy_(new_p)\n",
        "\n",
        "\n",
        "def copy_params(model, mode='cpu'):\n",
        "    if mode == 'gpu':\n",
        "        flatten = []\n",
        "        for p in model.parameters():\n",
        "            cpu_p = deepcopy(p).cpu()\n",
        "            flatten.append(cpu_p.data)\n",
        "    else:\n",
        "        flatten = deepcopy(list(p.data for p in model.parameters()))\n",
        "    return flatten\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7T_aB3Dx64N"
      },
      "source": [
        "#test.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDydol11x749"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import logging\n",
        "import operator\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from imageio import imsave\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "#This portion is not needed as everything is in one place!\n",
        "'''\n",
        "import cfg\n",
        "import models_search\n",
        "from functions import validate\n",
        "from utils.fid_score import calculate_fid_given_paths\n",
        "from utils.utils import set_log_dir, create_logger\n",
        "from utils.inception_score import _init_inception\n",
        "from utils.fid_score import create_inception_graph, check_or_download_inception\n",
        "from tensorboardX import SummaryWriter\n",
        "from utils.inception_score import get_inception_score\n",
        "'''\n",
        "# from utils.torch_fid_score import get_fid\n",
        "# from utils.inception_score import get_inception_score\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def validate(args, fixed_z, fid_stat, epoch, gen_net: nn.Module, writer_dict, clean_dir=True):\n",
        "    writer = writer_dict['writer']\n",
        "    global_steps = writer_dict['valid_global_steps']\n",
        "\n",
        "    # eval mode\n",
        "    gen_net.eval()\n",
        "\n",
        "#     generate images\n",
        "    with torch.no_grad():\n",
        "#         sample_imgs = gen_net(fixed_z, epoch)\n",
        "#         img_grid = make_grid(sample_imgs, nrow=5, normalize=True, scale_each=True)\n",
        "\n",
        "\n",
        "        eval_iter = args.num_eval_imgs // args.eval_batch_size\n",
        "        img_list = list()\n",
        "        for iter_idx in tqdm(range(eval_iter), desc='sample images'):\n",
        "            z = torch.cuda.FloatTensor(np.random.normal(0, 1, (args.eval_batch_size, args.latent_dim)))\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_imgs = gen_net(z, epoch).mul_(127.5).add_(127.5).clamp_(0.0, 255.0).permute(0, 2, 3, 1).to('cpu', torch.uint8).numpy()\n",
        "            img_list.extend(list(gen_imgs))\n",
        "\n",
        "#     mean, std = 0, 0\n",
        "    # get fid score\n",
        "#     mean, std = get_inception_score(img_list)\n",
        "#     print(f\"IS score: {mean}\")\n",
        "    print('=> calculate fid score') if args.rank == 0 else 0\n",
        "    fid_score = calculate_fid_given_paths([img_list, fid_stat], inception_path=None)\n",
        "    # fid_score = 10000\n",
        "    print(f\"FID score: {fid_score}\") if args.rank == 0 else 0\n",
        "    with open(f'output/{args.exp_name}.txt', 'a') as f:\n",
        "        print('fid:' + str(fid_score) + 'epoch' + str(epoch), file=f)\n",
        "    \n",
        "    if args.rank == 0:\n",
        "#         writer.add_scalar('Inception_score/mean', mean, global_steps)\n",
        "#         writer.add_scalar('Inception_score/std', std, global_steps)\n",
        "        writer.add_scalar('FID_score', fid_score, global_steps)\n",
        "\n",
        "#         writer_dict['valid_global_steps'] = global_steps + 1\n",
        "\n",
        "    return 0, fid_score\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    torch.cuda.manual_seed(args.random_seed)\n",
        "    assert args.exp_name\n",
        "#     assert args.load_path.endswith('.pth')\n",
        "    assert os.path.exists(args.load_path)\n",
        "    args.path_helper = set_log_dir('logs_eval', args.exp_name)\n",
        "    logger = create_logger(args.path_helper['log_path'], phase='test')\n",
        "\n",
        "    # set tf env\n",
        "    _init_inception()\n",
        "    inception_path = check_or_download_inception(None)\n",
        "    create_inception_graph(inception_path)\n",
        "\n",
        "    # import network\n",
        "    gen_net = eval('models_search.'+args.gen_model+'.Generator')(args=args).cuda()\n",
        "    gen_net = torch.nn.DataParallel(gen_net.to(\"cuda:0\"), device_ids=[0])\n",
        "\n",
        "    # fid stat\n",
        "    if args.dataset.lower() == 'cifar10':\n",
        "        fid_stat = 'fid_stat/fid_stats_cifar10_train.npz'\n",
        "    elif args.dataset.lower() == 'cifar10_flip':\n",
        "        fid_stat = 'fid_stat/fid_stats_cifar10_train.npz'\n",
        "    elif args.dataset.lower() == 'stl10':\n",
        "        fid_stat = 'fid_stat/stl10_train_unlabeled_fid_stats_48.npz'\n",
        "    elif args.fid_stat is not None:\n",
        "        fid_stat = args.fid_stat\n",
        "    else:\n",
        "        raise NotImplementedError(f'no fid stat for {args.dataset.lower()}')\n",
        "    assert os.path.exists(fid_stat)\n",
        "\n",
        "    # initial\n",
        "    fixed_z = torch.cuda.FloatTensor(np.random.normal(0, 1, (4, args.latent_dim)))\n",
        "\n",
        "    # set writer\n",
        "    logger.info(f'=> resuming from {args.load_path}')\n",
        "    checkpoint_file = args.load_path\n",
        "    assert os.path.exists(checkpoint_file)\n",
        "    checkpoint = torch.load(checkpoint_file)\n",
        "\n",
        "    if 'avg_gen_state_dict' in checkpoint:\n",
        "        gen_net.load_state_dict(checkpoint['avg_gen_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        logger.info(f'=> loaded checkpoint {checkpoint_file} (epoch {epoch})')\n",
        "    else:\n",
        "        gen_net.load_state_dict(checkpoint)\n",
        "        logger.info(f'=> loaded checkpoint {checkpoint_file}')\n",
        "\n",
        "    logger.info(args)\n",
        "    writer_dict = {\n",
        "        'writer': SummaryWriter(args.path_helper['log_path']),\n",
        "        'valid_global_steps': 0,\n",
        "    }\n",
        "    inception_score, fid_score = validate(args, fixed_z, fid_stat, epoch, gen_net, writer_dict, clean_dir=False)\n",
        "    logger.info(f'Inception score: {inception_score}, FID score: {fid_score}.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2plPLWpx88W"
      },
      "source": [
        "#train_derived.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcv1_4b6x-WV"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import cfg\n",
        "import models_search\n",
        "import datasets\n",
        "from functions import train, validate, save_samples, LinearLrDecay, load_params, copy_params, cur_stages\n",
        "from utils.utils import set_log_dir, save_checkpoint, create_logger\n",
        "# from utils.inception_score import _init_inception\n",
        "# from utils.fid_score import create_inception_graph, check_or_download_inception\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "import torch.utils.data.distributed\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from adamw import AdamW\n",
        "import random \n",
        "\n",
        "# torch.backends.cudnn.enabled = True\n",
        "# torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = cfg.parse_args()\n",
        "    \n",
        "#     _init_inception()\n",
        "#     inception_path = check_or_download_inception(None)\n",
        "#     create_inception_graph(inception_path)\n",
        "    \n",
        "    if args.seed is not None:\n",
        "        torch.cuda.manual_seed(args.random_seed)\n",
        "        torch.cuda.manual_seed_all(args.random_seed)\n",
        "        np.random.seed(args.random_seed)\n",
        "        random.seed(args.random_seed)\n",
        "\n",
        "    if args.gpu is not None:\n",
        "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
        "                      'disable data parallelism.')\n",
        "\n",
        "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "\n",
        "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
        "\n",
        "    ngpus_per_node = torch.cuda.device_count()\n",
        "    if args.multiprocessing_distributed:\n",
        "        # Since we have ngpus_per_node processes per node, the total world_size\n",
        "        # needs to be adjusted accordingly\n",
        "        args.world_size = ngpus_per_node * args.world_size\n",
        "        # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
        "        # main_worker process function\n",
        "        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
        "    else:\n",
        "        # Simply call main_worker function\n",
        "        main_worker(args.gpu, ngpus_per_node, args)\n",
        "        \n",
        "def main_worker(gpu, ngpus_per_node, args):\n",
        "    args.gpu = gpu\n",
        "    \n",
        "    if args.gpu is not None:\n",
        "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
        "\n",
        "    if args.distributed:\n",
        "        if args.dist_url == \"env://\" and args.rank == -1:\n",
        "            args.rank = int(os.environ[\"RANK\"])\n",
        "        if args.multiprocessing_distributed:\n",
        "            # For multiprocessing distributed training, rank needs to be the\n",
        "            # global rank among all the processes\n",
        "            args.rank = args.rank * ngpus_per_node + gpu\n",
        "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                world_size=args.world_size, rank=args.rank)\n",
        "    # weight init\n",
        "    def weights_init(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv2d') != -1:\n",
        "            if args.init_type == 'normal':\n",
        "                nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "            elif args.init_type == 'orth':\n",
        "                nn.init.orthogonal_(m.weight.data)\n",
        "            elif args.init_type == 'xavier_uniform':\n",
        "                nn.init.xavier_uniform(m.weight.data, 1.)\n",
        "            else:\n",
        "                raise NotImplementedError('{} unknown inital type'.format(args.init_type))\n",
        "#         elif classname.find('Linear') != -1:\n",
        "#             if args.init_type == 'normal':\n",
        "#                 nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "#             elif args.init_type == 'orth':\n",
        "#                 nn.init.orthogonal_(m.weight.data)\n",
        "#             elif args.init_type == 'xavier_uniform':\n",
        "#                 nn.init.xavier_uniform(m.weight.data, 1.)\n",
        "#             else:\n",
        "#                 raise NotImplementedError('{} unknown inital type'.format(args.init_type))\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    # import network\n",
        "    \n",
        "    \n",
        "    if not torch.cuda.is_available():\n",
        "        print('using CPU, this will be slow')\n",
        "    elif args.distributed:\n",
        "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
        "        # should always set the single device scope, otherwise,\n",
        "        # DistributedDataParallel will use all available devices.\n",
        "        if args.gpu is not None:\n",
        "            \n",
        "            torch.cuda.set_device(args.gpu)\n",
        "            gen_net = eval('models_search.'+args.gen_model+'.Generator')(args=args)\n",
        "            dis_net = eval('models_search.'+args.dis_model+'.Discriminator')(args=args)\n",
        "\n",
        "            gen_net.apply(weights_init)\n",
        "            dis_net.apply(weights_init)\n",
        "            gen_net.cuda(args.gpu)\n",
        "            dis_net.cuda(args.gpu)\n",
        "            # When using a single GPU per process and per\n",
        "            # DistributedDataParallel, we need to divide the batch size\n",
        "            # ourselves based on the total number of GPUs we have\n",
        "            args.dis_batch_size = int(args.dis_batch_size / ngpus_per_node)\n",
        "            args.gen_batch_size = int(args.gen_batch_size / ngpus_per_node)\n",
        "            args.batch_size = args.dis_batch_size\n",
        "            \n",
        "            args.num_workers = int((args.num_workers + ngpus_per_node - 1) / ngpus_per_node)\n",
        "            gen_net = torch.nn.parallel.DistributedDataParallel(gen_net, device_ids=[args.gpu], find_unused_parameters=True)\n",
        "            dis_net = torch.nn.parallel.DistributedDataParallel(dis_net, device_ids=[args.gpu], find_unused_parameters=True)\n",
        "        else:\n",
        "            gen_net.cuda()\n",
        "            dis_net.cuda()\n",
        "            # DistributedDataParallel will divide and allocate batch_size to all\n",
        "            # available GPUs if device_ids are not set\n",
        "            gen_net = torch.nn.parallel.DistributedDataParallel(gen_net)\n",
        "            dis_net = torch.nn.parallel.DistributedDataParallel(dis_net)\n",
        "    elif args.gpu is not None:\n",
        "        torch.cuda.set_device(args.gpu)\n",
        "        gen_net.cuda(args.gpu)\n",
        "        dis_net.cuda(args.gpu)\n",
        "    else:\n",
        "        gen_net = torch.nn.DataParallel(gen_net).cuda()\n",
        "        dis_net = torch.nn.DataParallel(dis_net).cuda()\n",
        "    print(dis_net) if args.rank == 0 else 0\n",
        "        \n",
        "\n",
        "    # set optimizer\n",
        "    if args.optimizer == \"adam\":\n",
        "        gen_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, gen_net.parameters()),\n",
        "                                        args.g_lr, (args.beta1, args.beta2))\n",
        "        dis_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, dis_net.parameters()),\n",
        "                                        args.d_lr, (args.beta1, args.beta2))\n",
        "    elif args.optimizer == \"adamw\":\n",
        "        gen_optimizer = AdamW(filter(lambda p: p.requires_grad, gen_net.parameters()),\n",
        "                                        args.g_lr, weight_decay=args.wd)\n",
        "        dis_optimizer = AdamW(filter(lambda p: p.requires_grad, dis_net.parameters()),\n",
        "                                         args.g_lr, weight_decay=args.wd)\n",
        "    gen_scheduler = LinearLrDecay(gen_optimizer, args.g_lr, 0.0, 0, args.max_iter * args.n_critic)\n",
        "    dis_scheduler = LinearLrDecay(dis_optimizer, args.d_lr, 0.0, 0, args.max_iter * args.n_critic)\n",
        "\n",
        "    # fid stat\n",
        "    if args.dataset.lower() == 'cifar10':\n",
        "        fid_stat = 'fid_stat/fid_stats_cifar10_train.npz'\n",
        "    elif args.dataset.lower() == 'stl10':\n",
        "        fid_stat = 'fid_stat/stl10_train_unlabeled_fid_stats_48.npz'\n",
        "    elif args.fid_stat is not None:\n",
        "        fid_stat = args.fid_stat\n",
        "    else:\n",
        "        raise NotImplementedError(f'no fid stat for {args.dataset.lower()}')\n",
        "    assert os.path.exists(fid_stat)\n",
        "\n",
        "\n",
        "    # epoch number for dis_net\n",
        "    args.max_epoch = args.max_epoch * args.n_critic\n",
        "    dataset = datasets.ImageDataset(args, cur_img_size=8)\n",
        "    train_loader = dataset.train\n",
        "    train_sampler = dataset.train_sampler\n",
        "    print(len(train_loader))\n",
        "    if args.max_iter:\n",
        "        args.max_epoch = np.ceil(args.max_iter * args.n_critic / len(train_loader))\n",
        "\n",
        "    # initial\n",
        "    fixed_z = torch.cuda.FloatTensor(np.random.normal(0, 1, (100, args.latent_dim)))\n",
        "    avg_gen_net = deepcopy(gen_net).cpu()\n",
        "    gen_avg_param = copy_params(avg_gen_net)\n",
        "    del avg_gen_net\n",
        "    start_epoch = 0\n",
        "    best_fid = 1e4\n",
        "\n",
        "    # set writer\n",
        "    writer = None\n",
        "    if args.load_path:\n",
        "        print(f'=> resuming from {args.load_path}')\n",
        "        assert os.path.exists(args.load_path)\n",
        "        checkpoint_file = os.path.join(args.load_path)\n",
        "        assert os.path.exists(checkpoint_file)\n",
        "        loc = 'cuda:{}'.format(args.gpu)\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=loc)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_fid = checkpoint['best_fid']\n",
        "        \n",
        "        \n",
        "        dis_net.load_state_dict(checkpoint['dis_state_dict'])\n",
        "        gen_optimizer.load_state_dict(checkpoint['gen_optimizer'])\n",
        "        dis_optimizer.load_state_dict(checkpoint['dis_optimizer'])\n",
        "        \n",
        "#         avg_gen_net = deepcopy(gen_net)\n",
        "        gen_net.load_state_dict(checkpoint['avg_gen_state_dict'])\n",
        "        gen_avg_param = copy_params(gen_net, mode='gpu')\n",
        "        gen_net.load_state_dict(checkpoint['gen_state_dict'])\n",
        "        fixed_z = checkpoint['fixed_z']\n",
        "#         del avg_gen_net\n",
        "#         gen_avg_param = list(p.cuda().to(f\"cuda:{args.gpu}\") for p in gen_avg_param)\n",
        "        \n",
        "        \n",
        "\n",
        "        args.path_helper = checkpoint['path_helper']\n",
        "        logger = create_logger(args.path_helper['log_path']) if args.rank == 0 else None\n",
        "        print(f'=> loaded checkpoint {checkpoint_file} (epoch {start_epoch})')\n",
        "        writer = SummaryWriter(args.path_helper['log_path']) if args.rank == 0 else None\n",
        "        del checkpoint\n",
        "    else:\n",
        "    # create new log dir\n",
        "        assert args.exp_name\n",
        "        if args.rank == 0:\n",
        "            args.path_helper = set_log_dir('logs', args.exp_name)\n",
        "            logger = create_logger(args.path_helper['log_path'])\n",
        "            writer = SummaryWriter(args.path_helper['log_path'])\n",
        "    \n",
        "    if args.rank == 0:\n",
        "        logger.info(args)\n",
        "    writer_dict = {\n",
        "        'writer': writer,\n",
        "        'train_global_steps': start_epoch * len(train_loader),\n",
        "        'valid_global_steps': start_epoch // args.val_freq,\n",
        "    }\n",
        "\n",
        "    # train loop\n",
        "    for epoch in range(int(start_epoch), int(args.max_epoch)):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        lr_schedulers = (gen_scheduler, dis_scheduler) if args.lr_decay else None\n",
        "        cur_stage = cur_stages(epoch, args)\n",
        "        print(\"cur_stage \" + str(cur_stage)) if args.rank==0 else 0\n",
        "        print(f\"path: {args.path_helper['prefix']}\") if args.rank==0 else 0\n",
        "        train(args, gen_net, dis_net, gen_optimizer, dis_optimizer, gen_avg_param, train_loader, epoch, writer_dict,fixed_z,\n",
        "               lr_schedulers)\n",
        "        \n",
        "        if args.rank == 0 and args.show:\n",
        "            backup_param = copy_params(gen_net)\n",
        "            load_params(gen_net, gen_avg_param, args, mode=\"cpu\")\n",
        "            save_samples(args, fixed_z, fid_stat, epoch, gen_net, writer_dict)\n",
        "            load_params(gen_net, backup_param, args)\n",
        "        \n",
        "        if epoch and epoch % args.val_freq == 0 or epoch == int(args.max_epoch)-1:\n",
        "            backup_param = copy_params(gen_net)\n",
        "            load_params(gen_net, gen_avg_param, args, mode=\"cpu\")\n",
        "            inception_score, fid_score = validate(args, fixed_z, fid_stat, epoch, gen_net, writer_dict)\n",
        "            if args.rank==0:\n",
        "                logger.info(f'Inception score: {inception_score}, FID score: {fid_score} || @ epoch {epoch}.')\n",
        "            load_params(gen_net, backup_param, args)\n",
        "            if fid_score < best_fid:\n",
        "                best_fid = fid_score\n",
        "                is_best = True\n",
        "            else:\n",
        "                is_best = False\n",
        "        else:\n",
        "            is_best = False\n",
        "\n",
        "        avg_gen_net = deepcopy(gen_net)\n",
        "        load_params(avg_gen_net, gen_avg_param, args)\n",
        "        if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
        "                and args.rank == 0):\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'gen_model': args.gen_model,\n",
        "                'dis_model': args.dis_model,\n",
        "                'gen_state_dict': gen_net.state_dict(),\n",
        "                'dis_state_dict': dis_net.state_dict(),\n",
        "                'avg_gen_state_dict': avg_gen_net.state_dict(),\n",
        "                'gen_optimizer': gen_optimizer.state_dict(),\n",
        "                'dis_optimizer': dis_optimizer.state_dict(),\n",
        "                'best_fid': best_fid,\n",
        "                'path_helper': args.path_helper,\n",
        "                'fixed_z': fixed_z\n",
        "            }, is_best, args.path_helper['ckpt_path'], filename=\"checkpoint\")\n",
        "        del avg_gen_net\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}